import json
import logging
import uuid
from collections.abc import AsyncGenerator
from datetime import datetime
from typing import Annotated, Any

from fastapi import APIRouter, Depends, HTTPException, Request, status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from family_assistant.processing import ProcessingService
from family_assistant.storage.context import DatabaseContext, get_db_context
from family_assistant.web.dependencies import get_db, get_processing_service
from family_assistant.web.models import ChatMessageResponse, ChatPromptRequest

logger = logging.getLogger(__name__)
chat_api_router = APIRouter()


class ConversationSummary(BaseModel):
    """Summary of a conversation for listing."""

    conversation_id: str = Field(..., description="Unique conversation identifier")
    last_message: str = Field(..., description="Preview of the last message")
    last_timestamp: datetime = Field(..., description="Timestamp of the last message")
    message_count: int = Field(..., description="Total number of messages")


class ConversationListResponse(BaseModel):
    """Response containing list of conversations."""

    conversations: list[ConversationSummary] = Field(
        ..., description="List of conversation summaries"
    )
    total: int = Field(..., description="Total number of conversations")


class ConversationMessage(BaseModel):
    """A single message in a conversation."""

    internal_id: int = Field(..., description="Internal database ID")
    role: str = Field(..., description="Message role (user/assistant/system/tool)")
    content: str | None = Field(None, description="Message content")
    timestamp: datetime = Field(..., description="Message timestamp")
    tool_calls: list[dict] | None = Field(None, description="Tool calls if any")
    tool_call_id: str | None = Field(None, description="Tool call ID for tool messages")
    error_traceback: str | None = Field(None, description="Error traceback if any")


class ConversationMessagesResponse(BaseModel):
    """Response containing messages for a specific conversation."""

    conversation_id: str = Field(..., description="Conversation identifier")
    messages: list[ConversationMessage] = Field(..., description="List of messages")
    total: int = Field(..., description="Total number of messages")


@chat_api_router.post("/v1/chat/send_message")  # Path relative to the prefix in api.py
async def api_chat_send_message(
    payload: ChatPromptRequest,
    request: Request,  # To access app.state for config and service registry
    default_processing_service: Annotated[
        ProcessingService, Depends(get_processing_service)
    ],  # Renamed for clarity
    db_context: Annotated[DatabaseContext, Depends(get_db)],
) -> ChatMessageResponse:
    """
    Receives a user prompt via API, processes it using the specified or default
    ProcessingService, and returns the assistant's reply.
    """
    conversation_id = payload.conversation_id or str(uuid.uuid4())
    # turn_id is generated internally by handle_chat_interaction.
    # We will use a placeholder for the response model if needed, or remove it from response.

    # Determine which processing service to use
    selected_processing_service = default_processing_service
    profile_id_requested = payload.profile_id

    if profile_id_requested:
        logger.info(
            f"API chat request for profile_id: '{profile_id_requested}'. Conversation ID: {conversation_id}, Prompt: '{payload.prompt[:100]}...'"
        )
        processing_services_registry = getattr(
            request.app.state, "processing_services", {}
        )
        if profile_id_requested in processing_services_registry:
            selected_processing_service = processing_services_registry[
                profile_id_requested
            ]
            logger.info(
                f"Using ProcessingService for profile_id: '{profile_id_requested}'."
            )
        else:
            logger.warning(
                f"Profile_id '{profile_id_requested}' not found in registry. Falling back to default profile: '{default_processing_service.service_config.id}'."
            )
    else:
        logger.info(
            f"API chat request (no profile_id specified). Using default profile: '{default_processing_service.service_config.id}'. Conversation ID: {conversation_id}, Prompt: '{payload.prompt[:100]}...'"
        )

    # Prepare trigger_content_parts for the new service method
    trigger_content_parts = [{"type": "text", "text": payload.prompt}]

    # Determine interface type - default to "api" if not specified
    interface_type = payload.interface_type or "api"

    # Call the new centralized interaction handler
    # For API, user_name can be generic or derived from auth if implemented
    user_name_for_api = (
        "API User"  # payload.user_name is not available on ChatPromptRequest
    )

    # The `turn_id` will be generated by `handle_chat_interaction`
    # We can retrieve it from the response if needed by the client,
    # but the ChatMessageResponse model currently expects it.
    # Let's assume for now the client might want the turn_id.
    # The `handle_chat_interaction` doesn't return turn_id directly,
    # but it's logged and associated with messages.
    # For the API response, we might need to reconsider if turn_id is essential.
    # The current ChatMessageResponse model includes it.
    # Let's generate it here for the response, though the one used internally will be from handle_chat_interaction.
    # This is a slight divergence; ideally, the one from handle_chat_interaction would be returned.
    # For now, to match the existing response model:
    response_turn_id = (
        str(uuid.uuid4())  # This is for the *response model only*
    )

    (
        final_reply_content,
        _final_assistant_message_internal_id,  # Not used by API response
        _final_reasoning_info,  # Not used by API response
        error_traceback,
    ) = await selected_processing_service.handle_chat_interaction(
        db_context=db_context,
        interface_type=interface_type,  # Use the interface_type from request or default "api"
        conversation_id=conversation_id,
        trigger_content_parts=trigger_content_parts,
        trigger_interface_message_id=None,  # API prompts don't have a prior interface ID
        user_name=user_name_for_api,
        replied_to_interface_id=None,  # payload.replied_to_message_id is not available on ChatPromptRequest
        chat_interface=None,  # API doesn't use interactive chat elements for confirmation (yet)
        request_confirmation_callback=None,  # No confirmation callback for API (yet)
    )

    if error_traceback:
        logger.error(
            f"Error processing API chat request for Conversation ID {conversation_id}: {error_traceback}"
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error processing request: {error_traceback if getattr(request.app.state, 'debug_mode', False) else 'An internal error occurred.'}",
        )

    if final_reply_content is None:
        logger.error(
            f"No final assistant reply content found for API chat. Conversation ID: {conversation_id}"
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Assistant did not provide a textual reply.",
        )

    return ChatMessageResponse(
        reply=final_reply_content,
        conversation_id=conversation_id,  # Return the used/generated conversation_id
        turn_id=response_turn_id,  # Return the turn_id generated for the response model
    )


@chat_api_router.get("/v1/chat/conversations")
async def get_conversations(
    db_context: Annotated[DatabaseContext, Depends(get_db)],
    limit: int = 20,
    offset: int = 0,
) -> ConversationListResponse:
    """
    Get a list of chat conversations for the web interface.

    Args:
        limit: Maximum number of conversations to return
        offset: Number of conversations to skip for pagination

    Returns:
        List of conversation summaries with metadata
    """
    # Use optimized query for conversation summaries
    summaries, total = await db_context.message_history.get_conversation_summaries(
        interface_type="web", limit=limit, offset=offset
    )

    # Convert to response format
    conversations = [
        ConversationSummary(
            conversation_id=summary["conversation_id"],
            last_message=summary["last_message"],
            last_timestamp=summary["last_timestamp"],
            message_count=summary["message_count"],
        )
        for summary in summaries
    ]

    return ConversationListResponse(
        conversations=conversations,
        total=total,
    )


@chat_api_router.get("/v1/chat/conversations/{conversation_id}/messages")
async def get_conversation_messages(
    conversation_id: str,
    db_context: Annotated[DatabaseContext, Depends(get_db)],
) -> ConversationMessagesResponse:
    """
    Get all messages for a specific conversation.

    Args:
        conversation_id: The conversation identifier

    Returns:
        List of messages in the conversation
    """
    # Get messages for this specific conversation
    history_by_chat = await db_context.message_history.get_all_grouped(
        interface_type="web", conversation_id=conversation_id
    )

    # Extract messages for this conversation (already filtered by conversation_id)
    messages = history_by_chat.get(("web", conversation_id), [])

    # Convert to response format
    response_messages = []
    for msg in messages:
        # Skip messages with missing required fields
        if not all(key in msg for key in ["internal_id", "role", "timestamp"]):
            continue

        response_messages.append(
            ConversationMessage(
                internal_id=msg["internal_id"],
                role=msg["role"],
                content=msg.get("content"),
                timestamp=msg["timestamp"],
                tool_calls=msg.get("tool_calls"),
                tool_call_id=msg.get("tool_call_id"),
                error_traceback=msg.get("error_traceback"),
            )
        )

    return ConversationMessagesResponse(
        conversation_id=conversation_id,
        messages=response_messages,
        total=len(response_messages),
    )


@chat_api_router.post("/v1/chat/send_message_stream")
async def api_chat_send_message_stream(
    payload: ChatPromptRequest,
    request: Request,
    default_processing_service: Annotated[
        ProcessingService, Depends(get_processing_service)
    ],
) -> StreamingResponse:
    """
    Stream chat responses using Server-Sent Events format.

    This endpoint accepts the same payload as the non-streaming endpoint but
    returns a stream of events as the response is generated, including:
    - Text chunks as they're generated
    - Tool calls as they're initiated
    - Tool results as they complete
    - Error events if something goes wrong
    """
    conversation_id = payload.conversation_id or str(uuid.uuid4())

    # Determine which processing service to use (same logic as non-streaming endpoint)
    selected_processing_service = default_processing_service
    profile_id_requested = payload.profile_id

    if profile_id_requested:
        logger.info(
            f"API streaming chat request for profile_id: '{profile_id_requested}'. "
            f"Conversation ID: {conversation_id}, Prompt: '{payload.prompt[:100]}...'"
        )
        processing_services_registry = getattr(
            request.app.state, "processing_services", {}
        )
        if profile_id_requested in processing_services_registry:
            selected_processing_service = processing_services_registry[
                profile_id_requested
            ]
            logger.info(
                f"Using ProcessingService for profile_id: '{profile_id_requested}'."
            )
        else:
            logger.warning(
                f"Profile_id '{profile_id_requested}' not found in registry. "
                f"Falling back to default profile: '{default_processing_service.service_config.id}'."
            )
    else:
        logger.info(
            f"API streaming chat request (no profile_id specified). "
            f"Using default profile: '{default_processing_service.service_config.id}'. "
            f"Conversation ID: {conversation_id}, Prompt: '{payload.prompt[:100]}...'"
        )

    # Prepare trigger content for processing
    trigger_content_parts = [{"type": "text", "text": payload.prompt}]
    interface_type = payload.interface_type or "api"
    user_name_for_api = "API User"

    async def event_generator() -> AsyncGenerator[str, None]:
        """Generate SSE formatted events from the processing stream."""
        # Get a fresh database context for the stream
        async with get_db_context() as stream_db_context:
            try:
                # Use the streaming version of handle_chat_interaction
                async for (
                    event
                ) in selected_processing_service.handle_chat_interaction_stream(
                    db_context=stream_db_context,
                    interface_type=interface_type,
                    conversation_id=conversation_id,
                    trigger_content_parts=trigger_content_parts,
                    trigger_interface_message_id=None,
                    user_name=user_name_for_api,
                    replied_to_interface_id=None,
                    chat_interface=None,
                    request_confirmation_callback=None,
                ):
                    # Format event based on type
                    if event.type == "content":
                        # Send text content chunks
                        yield f"event: text\ndata: {json.dumps({'content': event.content})}\n\n"

                    elif event.type == "tool_call":
                        # Convert tool_call to dict for JSON serialization
                        if event.tool_call:
                            tool_call_dict = {
                                "id": event.tool_call.id,
                                "function": {
                                    "name": event.tool_call.function.name,
                                    "arguments": event.tool_call.function.arguments,
                                },
                            }
                            yield f"event: tool_call\ndata: {json.dumps({'tool_call': tool_call_dict})}\n\n"

                    elif event.type == "tool_result":
                        # Include tool_call_id for correlation
                        yield f"event: tool_result\ndata: {json.dumps({'tool_call_id': event.tool_call_id, 'result': event.tool_result})}\n\n"

                    elif event.type == "done":
                        # Send completion event with optional metadata
                        done_data: dict[str, Any] = {}
                        if event.metadata and event.metadata.get("reasoning_info"):
                            done_data["reasoning_info"] = event.metadata[
                                "reasoning_info"
                            ]
                        yield f"event: end\ndata: {json.dumps(done_data)}\n\n"

                    elif event.type == "error":
                        # Send error event
                        error_data = {"error": event.error or "An error occurred"}
                        if event.metadata and event.metadata.get("error_id"):
                            error_data["error_id"] = event.metadata["error_id"]
                        yield f"event: error\ndata: {json.dumps(error_data)}\n\n"

            except Exception as e:
                error_id = str(uuid.uuid4())
                logger.error(f"Streaming error {error_id}: {e}", exc_info=True)

                # Send error event to client
                error_msg = "An error occurred while processing your request"
                if getattr(request.app.state, "debug_mode", False):
                    error_msg = str(e)

                yield f"event: error\ndata: {json.dumps({'error': error_msg, 'error_id': error_id})}\n\n"
            finally:
                # Send a final close event to ensure client knows stream is done
                yield f"event: close\ndata: {json.dumps({})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable Nginx buffering
            "Access-Control-Allow-Origin": "*",  # CORS support
        },
    )
