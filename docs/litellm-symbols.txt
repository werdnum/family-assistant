# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_logging.py Line: 22
class JsonFormatter(Formatter):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_logging.py Line: 45
def _setup_json_exception_handlers(formatter):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_logging.py Line: 112
def _turn_on_json():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_logging.py Line: 135
def _turn_on_debug():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_logging.py Line: 141
def _disable_debugging():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_logging.py Line: 147
def _enable_debugging():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_logging.py Line: 153
def print_verbose(print_statement):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_logging.py Line: 161
def _is_debugging_on() -> bool:
    "Returns True if debugging is on"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 325
def print_verbose(print_statement, logger_only: bool, log_level: Literal[(?, ?, ?)]="DEBUG"):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 345
def custom_llm_setup():
    "Add custom_llm provider to provider list"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 357
def _add_custom_logger_callback_to_specific_event(callback: str, logging_event: Literal[(?, ?)]) -> ?:
    "Add a custom logger callback to the specific event"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 421
def _custom_logger_class_exists_in_success_callbacks(callback_class: CustomLogger) -> bool:
    """Returns True if an instance of the custom logger exists in litellm.success_callback or litellm._async_success_callback

    e.g if `LangfusePromptManagement` is passed in, it will return True if an instance of `LangfusePromptManagement` exists in litellm.success_callback or litellm._async_success_callback

    Prevents double adding a custom logger callback to the litellm callbacks"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 437
def _custom_logger_class_exists_in_failure_callbacks(callback_class: CustomLogger) -> bool:
    """Returns True if an instance of the custom logger exists in litellm.failure_callback or litellm._async_failure_callback

    e.g if `LangfusePromptManagement` is passed in, it will return True if an instance of `LangfusePromptManagement` exists in litellm.failure_callback or litellm._async_failure_callback

    Prevents double adding a custom logger callback to the litellm callbacks"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 453
def get_request_guardrails(kwargs: Dict[(str, Any)]) -> List[str]:
    "Get the request guardrails from the kwargs"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 463
def get_applied_guardrails(kwargs: Dict[(str, Any)]) -> List[str]:
    """- Add 'default_on' guardrails to the list
    - Add request guardrails to the list"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 482
def load_credentials_from_list(kwargs: dict):
    "Updates kwargs with the credentials if credential_name in kwarg"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 494
def get_dynamic_callbacks(dynamic_callbacks: Optional[List[Union[(str, Callable, CustomLogger)]]]) -> List:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 503
def function_setup(original_function: str, rules_obj, start_time, *args, **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 797
async def _client_async_logging_helper(logging_obj: LiteLLMLoggingObject, result, start_time, end_time, is_completion_with_fallbacks: bool):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 821
def _get_wrapper_num_retries(kwargs: Dict[(str, Any)], exception: Exception) -> Tuple[(Optional[int], Dict[(str, Any)])]:
    """Get the number of retries from the kwargs and the retry policy.
    Used for the wrapper functions."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 844
def _get_wrapper_timeout(kwargs: Dict[(str, Any)], exception: Exception) -> Optional[Union[(float, int, ?)]]:
    """Get the timeout from the kwargs
    Used for the wrapper functions."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 859
def client(original_function):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1471
def _is_async_request(kwargs: Optional[dict], is_pass_through: bool) -> bool:
    """Returns True if the call type is an internal async request.

    eg. litellm.acompletion, litellm.aimage_generation, litellm.acreate_batch, litellm._arealtime

    Args:
        kwargs (dict): The kwargs passed to the litellm function
        is_pass_through (bool): Whether the call is a pass-through call. By default all pass through calls are async."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1503
def update_response_metadata(result: Any, logging_obj: LiteLLMLoggingObject, model: Optional[str], kwargs: dict, start_time: ?, end_time: ?) -> ?:
    """Updates response metadata, adds the following:
        - response._hidden_params
        - response._hidden_params["litellm_overhead_time_ms"]
        - response.response_time_ms"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1528
def _select_tokenizer(model: str, custom_tokenizer: Optional[CustomHuggingfaceTokenizer]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1542
def _select_tokenizer_helper(model: str) -> SelectTokenizerResponse:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1557
def _return_openai_tokenizer(model: str) -> SelectTokenizerResponse:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1561
def _return_huggingface_tokenizer(model: str) -> Optional[SelectTokenizerResponse]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1584
def encode(model="", text="", custom_tokenizer: Optional[dict]):
    """Encodes the given text using the specified model.

    Args:
        model (str): The name of the model to use for tokenization.
        custom_tokenizer (Optional[dict]): A custom tokenizer created with the `create_pretrained_tokenizer` or `create_tokenizer` method. Must be a dictionary with a string value for `type` and Tokenizer for `tokenizer`. Default is None.
        text (str): The text to be encoded.

    Returns:
        enc: The encoded text."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1604
def decode(model="", tokens: List[int], custom_tokenizer: Optional[dict]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1610
def openai_token_counter(messages: Optional[list], model="gpt-3.5-turbo-0613", text: Optional[str], is_tool_call: Optional[bool], tools: Optional[List[ChatCompletionToolParam]], tool_choice: Optional[ChatCompletionNamedToolChoiceParam], count_response_tokens: Optional[bool], use_default_image_token_count: Optional[bool], default_token_count: Optional[int]):
    """Return the number of tokens used by a list of messages.

    Borrowed from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1701
def create_pretrained_tokenizer(identifier: str, revision="main", auth_token: Optional[str]):
    """Creates a tokenizer from an existing file on a HuggingFace repository to be used with `token_counter`.

    Args:
    identifier (str): The identifier of a Model on the Hugging Face Hub, that contains a tokenizer.json file
    revision (str, defaults to main): A branch or commit id
    auth_token (str, optional, defaults to None): An optional auth token used to access private repositories on the Hugging Face Hub

    Returns:
    dict: A dictionary with the tokenizer and its type."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1728
def create_tokenizer(json: str):
    """Creates a tokenizer from a valid JSON string for use with `token_counter`.

    Args:
    json (str): A valid JSON string representing a previously serialized tokenizer

    Returns:
    dict: A dictionary with the tokenizer and its type."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1743
def _format_function_definitions(tools):
    """Formats tool definitions in the format that OpenAI appears to use.
    Based on https://github.com/forestwanglin/openai-java/blob/main/jtokkit/src/main/java/xyz/felh/openai/jtokkit/utils/TikTokenUtils.java"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1768
def _format_object_parameters(parameters, indent):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1785
def _format_type(props, indent):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1809
def _get_num_tokens_from_content_list(content_list: List[Dict[(str, Any)]], use_default_image_token_count: Optional[bool], default_token_count: Optional[int]) -> Tuple[(str, int)]:
    """Get the number of tokens from a list of content.

    Returns:
        Tuple[str, int]: A tuple containing the text and the number of tokens."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1855
def token_counter(model="", custom_tokenizer: Optional[Union[(dict, SelectTokenizerResponse)]], text: Optional[Union[(str, List[str])]], messages: Optional[List], count_response_tokens: Optional[bool], tools: Optional[List[ChatCompletionToolParam]], tool_choice: Optional[ChatCompletionNamedToolChoiceParam], use_default_image_token_count: Optional[bool], default_token_count: Optional[int]) -> int:
    """Count the number of tokens in a given text using a specified model.

    Args:
    model (str): The name of the model to use for tokenization. Default is an empty string.
    custom_tokenizer (Optional[dict]): A custom tokenizer created with the `create_pretrained_tokenizer` or `create_tokenizer` method. Must be a dictionary with a string value for `type` and Tokenizer for `tokenizer`. Default is None.
    text (str): The raw text string to be passed to the model. Default is None.
    messages (Optional[List[Dict[str, str]]]): Alternative to passing in text. A list of dictionaries representing messages with "role" and "content" keys. Default is None.
    default_token_count (Optional[int]): The default number of tokens to return for a message block, if an error occurs. Default is None.

    Returns:
    int: The number of tokens in the text."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1963
def supports_httpx_timeout(custom_llm_provider: str) -> bool:
    "Helper function to know if a provider implementation supports httpx timeout"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1975
def supports_system_messages(model: str, custom_llm_provider: Optional[str]) -> bool:
    """Check if the given model supports system messages and return a boolean value.

    Parameters:
    model (str): The model name to be checked.
    custom_llm_provider (str): The provider to be checked.

    Returns:
    bool: True if the model supports system messages, False otherwise.

    Raises:
    Exception: If the given model is not found in model_prices_and_context_window.json."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 1996
def supports_web_search(model: str, custom_llm_provider: Optional[str]) -> bool:
    """Check if the given model supports web search and return a boolean value.

    Parameters:
    model (str): The model name to be checked.
    custom_llm_provider (str): The provider to be checked.

    Returns:
    bool: True if the model supports web search, False otherwise.

    Raises:
    Exception: If the given model is not found in model_prices_and_context_window.json."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2017
def supports_native_streaming(model: str, custom_llm_provider: Optional[str]) -> bool:
    """Check if the given model supports native streaming and return a boolean value.

    Parameters:
    model (str): The model name to be checked.
    custom_llm_provider (str): The provider to be checked.

    Returns:
    bool: True if the model supports native streaming, False otherwise.

    Raises:
    Exception: If the given model is not found in model_prices_and_context_window.json."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2050
def supports_response_schema(model: str, custom_llm_provider: Optional[str]) -> bool:
    """Check if the given model + provider supports 'response_schema' as a param.

    Parameters:
    model (str): The model name to be checked.
    custom_llm_provider (str): The provider to be checked.

    Returns:
    bool: True if the model supports response_schema, False otherwise.

    Does not raise error. Defaults to 'False'. Outputs logging.error."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2091
def supports_parallel_function_calling(model: str, custom_llm_provider: Optional[str]) -> bool:
    "Check if the given model supports parallel tool calls and return a boolean value."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2104
def supports_function_calling(model: str, custom_llm_provider: Optional[str]) -> bool:
    """Check if the given model supports function calling and return a boolean value.

    Parameters:
    model (str): The model name to be checked.
    custom_llm_provider (Optional[str]): The provider to be checked.

    Returns:
    bool: True if the model supports function calling, False otherwise.

    Raises:
    Exception: If the given model is not found or there's an error in retrieval."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2127
def supports_tool_choice(model: str, custom_llm_provider: Optional[str]) -> bool:
    "Check if the given model supports `tool_choice` and return a boolean value."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2136
def _supports_factory(model: str, custom_llm_provider: Optional[str], key: str) -> bool:
    """Check if the given model supports function calling and return a boolean value.

    Parameters:
    model (str): The model name to be checked.
    custom_llm_provider (Optional[str]): The provider to be checked.

    Returns:
    bool: True if the model supports function calling, False otherwise.

    Raises:
    Exception: If the given model is not found or there's an error in retrieval."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2176
def supports_audio_input(model: str, custom_llm_provider: Optional[str]) -> bool:
    "Check if a given model supports audio input in a chat completion call"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2183
def supports_pdf_input(model: str, custom_llm_provider: Optional[str]) -> bool:
    "Check if a given model supports pdf input in a chat completion call"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2190
def supports_audio_output(model: str, custom_llm_provider: Optional[str]) -> bool:
    "Check if a given model supports audio output in a chat completion call"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2199
def supports_prompt_caching(model: str, custom_llm_provider: Optional[str]) -> bool:
    """Check if the given model supports prompt caching and return a boolean value.

    Parameters:
    model (str): The model name to be checked.
    custom_llm_provider (Optional[str]): The provider to be checked.

    Returns:
    bool: True if the model supports prompt caching, False otherwise.

    Raises:
    Exception: If the given model is not found or there's an error in retrieval."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2222
def supports_vision(model: str, custom_llm_provider: Optional[str]) -> bool:
    """Check if the given model supports vision and return a boolean value.

    Parameters:
    model (str): The model name to be checked.
    custom_llm_provider (Optional[str]): The provider to be checked.

    Returns:
    bool: True if the model supports vision, False otherwise."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2240
def supports_reasoning(model: str, custom_llm_provider: Optional[str]) -> bool:
    "Check if the given model supports reasoning and return a boolean value."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2249
def supports_embedding_image_input(model: str, custom_llm_provider: Optional[str]) -> bool:
    "Check if the given model supports embedding image input and return a boolean value."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2263
def _update_dictionary(existing_dict: Dict, new_dict: dict) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2271
def register_model(model_cost: Union[(str, dict)]):
    """Register new / Override existing models (and their pricing) to specific providers.
    Provide EITHER a model cost dictionary or a url to a hosted json blob
    Example usage:
    model_cost_dict = {
        "gpt-4": {
            "max_tokens": 8192,
            "input_cost_per_token": 0.00003,
            "output_cost_per_token": 0.00006,
            "litellm_provider": "openai",
            "mode": "chat"
        },
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2351
def _should_drop_param(k, additional_drop_params) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2362
def _get_non_default_params(passed_params: dict, default_params: dict, additional_drop_params: Optional[bool]) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2378
def get_optional_params_transcription(model: str, language: Optional[str], prompt: Optional[str], response_format: Optional[str], temperature: Optional[int], timestamp_granularities: Optional[List[Literal[(?, ?)]]], custom_llm_provider: Optional[str], drop_params: Optional[bool], **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2460
def get_optional_params_image_gen(model: Optional[str], n: Optional[int], quality: Optional[str], response_format: Optional[str], size: Optional[str], style: Optional[str], user: Optional[str], custom_llm_provider: Optional[str], additional_drop_params: Optional[bool], **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2562
def get_optional_params_embeddings(model: str, user: Optional[str], encoding_format: Optional[str], dimensions: Optional[int], custom_llm_provider="", drop_params: Optional[bool], additional_drop_params: Optional[bool], **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2795
def _remove_additional_properties(schema):
    """clean out 'additionalProperties = False'. Causes vertexai/gemini OpenAI API Schema errors - https://github.com/langchain-ai/langchainjs/issues/5240

    Relevant Issues: https://github.com/BerriAI/litellm/issues/6136, https://github.com/BerriAI/litellm/issues/6088"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2818
def _remove_strict_from_schema(schema):
    "Relevant Issues: https://github.com/BerriAI/litellm/issues/6136, https://github.com/BerriAI/litellm/issues/6088"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2839
def _remove_unsupported_params(non_default_params: dict, supported_openai_params: Optional[List[str]]) -> dict:
    "Remove unsupported params from non_default_params"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 2856
def get_optional_params(model: str, functions, function_call, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, custom_llm_provider="", response_format, seed, tools, tool_choice, max_retries, logprobs, top_logprobs, extra_headers, api_version, parallel_tool_calls, drop_params, allowed_openai_params: Optional[List[str]], reasoning_effort, additional_drop_params, messages: Optional[List[AllMessageValues]], thinking: Optional[AnthropicThinkingParam], **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 3815
def _apply_openai_param_overrides(optional_params: dict, non_default_params: dict, allowed_openai_params: list):
    """If user passes in allowed_openai_params, apply them to optional_params

    These params will get passed as is to the LLM API since the user opted in to passing them in the request"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 3830
def get_non_default_params(passed_params: dict) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 3871
def calculate_max_parallel_requests(max_parallel_requests: Optional[int], rpm: Optional[int], tpm: Optional[int], default_max_parallel_requests: Optional[int]) -> Optional[int]:
    """Returns the max parallel requests to send to a deployment.

    Used in semaphore for async requests on router.

    Parameters:
    - max_parallel_requests - Optional[int] - max_parallel_requests allowed for that deployment
    - rpm - Optional[int] - requests per minute allowed for that deployment
    - tpm - Optional[int] - tokens per minute allowed for that deployment
    - default_max_parallel_requests - Optional[int] - default_max_parallel_requests allowed for any deployment

    Returns:
    - int or None (if all params are None)

    Order:
    max_parallel_requests > rpm > tpm / 6 (azure formula) > default max_parallel_requests

    Azure RPM formula:
    6 rpm per 1000 TPM
    https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 3914
def _get_order_filtered_deployments(healthy_deployments: List[Dict]) -> List:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 3935
def _get_model_region(custom_llm_provider: str, litellm_params: LiteLLM_Params) -> Optional[str]:
    "Return the region for a model, for a given provider"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 3962
def _infer_model_region(litellm_params: LiteLLM_Params) -> Optional[AllowedModelRegion]:
    """Infer if a model is in the EU or US region

    Returns:
    - str (region) - "eu" or "us"
    - None (if region not found)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4009
def _is_region_eu(litellm_params: LiteLLM_Params) -> bool:
    "Return true/false if a deployment is in the EU"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4023
def _is_region_us(litellm_params: LiteLLM_Params) -> bool:
    "Return true/false if a deployment is in the US"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4037
def is_region_allowed(litellm_params: LiteLLM_Params, allowed_model_region: str) -> bool:
    "Return true/false if a deployment is in the EU"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4048
def get_model_region(litellm_params: LiteLLM_Params, mode: Optional[str]) -> Optional[str]:
    "Pass the litellm params for an azure model, and get back the region"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4074
def get_first_chars_messages(kwargs: dict) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4083
def _count_characters(text: str) -> int:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4089
def get_response_string(response_obj: Union[(ModelResponse, ModelResponseStream)]) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4106
def get_api_key(llm_provider: str, dynamic_api_key: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4150
def get_utc_datetime():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4160
def get_max_tokens(model: str) -> Optional[int]:
    """Get the maximum number of output tokens allowed for a given model.

    Parameters:
    model (str): The name of the model.

    Returns:
        int: The maximum number of tokens allowed for the given model.

    Raises:
        Exception: If the model is not mapped yet.

    Example:
        >>> get_max_tokens("gpt-4")
        8192"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4221
def _strip_stable_vertex_version(model_name) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4225
def _get_base_bedrock_model(model_name) -> str:
    """Get the base model from the given model name.

    Handle model names like - "us.meta.llama3-2-11b-instruct-v1:0" -> "meta.llama3-2-11b-instruct-v1"
    AND "meta.llama3-2-11b-instruct-v1:0" -> "meta.llama3-2-11b-instruct-v1""""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4237
def _strip_openai_finetune_model_name(model_name: str) -> str:
    """Strips the organization, custom suffix, and ID from an OpenAI fine-tuned model name.

    input: ft:gpt-3.5-turbo:my-org:custom_suffix:id
    output: ft:gpt-3.5-turbo

    Args:
    model_name (str): The full model name

    Returns:
    str: The stripped model name"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4253
def _strip_model_name(model: str, custom_llm_provider: Optional[str]) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4272
def _get_model_info_from_model_cost(key: str) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4276
def _check_provider_match(model_info: dict, custom_llm_provider: Optional[str]) -> bool:
    "Check if the model info provider matches the custom provider."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4305
class PotentialModelNamesAndCustomLLMProvider(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4313
def _get_potential_model_names(model: str, custom_llm_provider: Optional[str]) -> PotentialModelNamesAndCustomLLMProvider:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4358
def _get_max_position_embeddings(model_name: str) -> Optional[int]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4381
def _cached_get_model_info_helper(model: str, custom_llm_provider: Optional[str]) -> ModelInfoBase:
    """_get_model_info_helper wrapped with lru_cache

    Speed Optimization to hit high RPS"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4392
def get_provider_info(model: str, custom_llm_provider: Optional[str]) -> Optional[ProviderSpecificModelInfo]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4412
def _get_model_info_helper(model: str, custom_llm_provider: Optional[str]) -> ModelInfoBase:
    "Helper for 'get_model_info'. Separated out to avoid infinite loop caused by returning 'supported_openai_param's"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4653
def get_model_info(model: str, custom_llm_provider: Optional[str]) -> ModelInfo:
    """Get a dict for the maximum tokens (context window), input_cost_per_token, output_cost_per_token  for a given model.

    Parameters:
    - model (str): The name of the model.
    - custom_llm_provider (str | null): the provider used for the model. If provided, used to check if the litellm model info is for that provider.

    Returns:
        dict: A dictionary containing the following information:
            key: Required[str] # the key in litellm.model_cost which is returned
            max_tokens: Required[Optional[int]]
            max_input_tokens: Required[Optional[int]]
            max_output_tokens: Required[Optional[int]]
            input_cost_per_token: Required[float]
            input_cost_per_character: Optional[float]  # only for vertex ai models
            input_cost_per_token_above_128k_tokens: Optional[float]  # only for vertex ai models
            input_cost_per_character_above_128k_tokens: Optional[
                float
            ]  # only for vertex ai models
            input_cost_per_query: Optional[float] # only for rerank models
            input_cost_per_image: Optional[float]  # only for vertex ai models
            input_cost_per_audio_token: Optional[float]
            input_cost_per_audio_per_second: Optional[float]  # only for vertex ai models
            input_cost_per_video_per_second: Optional[float]  # only for vertex ai models
            output_cost_per_token: Required[float]
            output_cost_per_audio_token: Optional[float]
            output_cost_per_character: Optional[float]  # only for vertex ai models
            output_cost_per_token_above_128k_tokens: Optional[
                float
            ]  # only for vertex ai models
            output_cost_per_character_above_128k_tokens: Optional[
                float
            ]  # only for vertex ai models
            output_cost_per_image: Optional[float]
            output_vector_size: Optional[int]
            output_cost_per_video_per_second: Optional[float]  # only for vertex ai models
            output_cost_per_audio_per_second: Optional[float]  # only for vertex ai models
            litellm_provider: Required[str]
            mode: Required[
                Literal[
                    "completion", "embedding", "image_generation", "chat", "audio_transcription"
                ]
            ]
            supported_openai_params: Required[Optional[List[str]]]
            supports_system_messages: Optional[bool]
            supports_response_schema: Optional[bool]
            supports_vision: Optional[bool]
            supports_function_calling: Optional[bool]
            supports_tool_choice: Optional[bool]
            supports_prompt_caching: Optional[bool]
            supports_audio_input: Optional[bool]
            supports_audio_output: Optional[bool]
            supports_pdf_input: Optional[bool]
            supports_web_search: Optional[bool]
            supports_reasoning: Optional[bool]
    Raises:
        Exception: If the model is not mapped yet.

    Example:
        >>> get_model_info("gpt-4")
        {
            "max_tokens": 8192,
            "input_cost_per_token": 0.00003,
            "output_cost_per_token": 0.00006,
            "litellm_provider": "openai",
            "mode": "chat",
            "supported_openai_params": ["temperature", "max_tokens", "top_p", "frequency_penalty", "presence_penalty"]
        }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4741
def json_schema_type(python_type_name: str):
    """Converts standard python types to json schema types

    Parameters
    ----------
    python_type_name : str
        __name__ of type

    Returns
    -------
    str
        a standard JSON schema type, "string" if not recognized."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4767
def function_to_dict(input_function):
    """Using type hints and numpy-styled docstring,
    produce a dictionnary usable for OpenAI function calling

    Parameters
    ----------
    input_function : function
        A function with a numpy-style docstring

    Returns
    -------
    dictionnary
        A dictionnary to add to the list passed to `functions` parameter of `litellm.completion`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4858
def modify_url(original_url, new_path):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4864
def load_test_model(model: str, custom_llm_provider: str="", api_base: str="", prompt: str="", num_calls: int, force_timeout: int):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4907
def get_provider_fields(custom_llm_provider: str) -> List[ProviderField]:
    "Return the fields required for each provider"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4923
def create_proxy_transport_and_mounts():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 4951
def validate_environment(model: Optional[str], api_key: Optional[str], api_base: Optional[str]) -> dict:
    """Checks if the environment variables are valid for the given model.

    Args:
        model (Optional[str]): The name of the model. Defaults to None.
        api_key (Optional[str]): If the user passed in an api key, of their own.

    Returns:
        dict: A dictionary containing the following keys:
            - keys_in_environment (bool): True if all the required keys are present in the environment, False otherwise.
            - missing_keys (List[str]): A list of missing keys in the environment."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5273
def acreate(*args, **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5277
def prompt_token_calculator(model, messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5295
def valid_model(model):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5310
def check_valid_key(model: str, api_key: str):
    """Checks if a given API key is valid for a specific model by making a litellm.completion call with max_tokens=10

    Args:
        model (str): The name of the model to check the API key against.
        api_key (str): The API key to be checked.

    Returns:
        bool: True if the API key is valid for the model, False otherwise."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5333
def _should_retry(status_code: int):
    """Retries on 408, 409, 429 and 500 errors.

    Any client error in the 400-499 range that isn't explicitly handled (such as 400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found, etc.) would not trigger a retry.

    Reimplementation of openai's should retry logic, since that one can't be imported.
    https://github.com/openai/openai-python/blob/af67cfab4210d8e497c05390ce14f39105c77519/src/openai/_base_client.py#L639"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5362
def _get_retry_after_from_exception_header(response_headers: Optional[?]):
    """Reimplementation of openai's calculate retry after, since that one can't be imported.
    https://github.com/openai/openai-python/blob/af67cfab4210d8e497c05390ce14f39105c77519/src/openai/_base_client.py#L631"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5396
def _calculate_retry_after(remaining_retries: int, max_retries: int, response_headers: Optional[?], min_timeout: int) -> Union[(float, int)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5422
def register_prompt_template(model: str, roles: dict, initial_prompt_value: str="", final_prompt_value: str="", tokenizer_config: dict):
    """    Register a prompt template to follow your custom format for a given model

        Args:
            model (str): The name of the model.
            roles (dict): A dictionary mapping roles to their respective prompt values.
            initial_prompt_value (str, optional): The initial prompt value. Defaults to "".
            final_prompt_value (str, optional): The final prompt value. Defaults to "".

        Returns:
            dict: The updated custom prompt dictionary.
        Example usage:
        ```
        import litellm
        litellm.register_prompt_template(
                model="llama-2",
            initial_prompt_value="You are a good assistant" # [OPTIONAL]
                roles={
                "system": {
                    "pre_message": "[INST] <<SYS>>
    ", # [OPTIONAL]
                    "post_message": "
    <</SYS>>
     [/INST]
    " # [OPTIONAL]
                },
                "user": {
                    "pre_message": "[INST] ", # [OPTIONAL]
                    "post_message": " [/INST]" # [OPTIONAL]
                },
                "assistant": {
                    "pre_message": "
    " # [OPTIONAL]
                    "post_message": "
    " # [OPTIONAL]
                }
            }
            final_prompt_value="Now answer as best you can:" # [OPTIONAL]
        )
        ```
        """

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5488
class TextCompletionStreamWrapper:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5570
def mock_completion_streaming_obj(model_response, mock_response, model, n: Optional[int]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5593
async def async_mock_completion_streaming_obj(model_response, mock_response, model, n: Optional[int]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5617
def read_config_args(config_path) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5634
def process_system_message(system_message, max_tokens, model):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5651
def process_messages(messages, max_tokens, model):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5672
def attempt_message_addition(final_messages, message, available_tokens, max_tokens, model):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5691
def can_add_message(message, messages, max_tokens, model):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5697
def get_token_count(messages, model):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5701
def shorten_message_to_fit_limit(message, tokens_needed, model: Optional[str]):
    "Shorten a message to fit within a token limit by removing characters from the middle."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5739
def trim_messages(messages, model: Optional[str], trim_ratio: float=DEFAULT_TRIM_RATIO, return_response_tokens: bool, max_tokens):
    """Trim a list of messages to fit within a model's token limit.

    Args:
        messages: Input messages to be trimmed. Each message is a dictionary with 'role' and 'content'.
        model: The LiteLLM model being used (determines the token limit).
        trim_ratio: Target ratio of tokens to use after trimming. Default is 0.75, meaning it will trim messages so they use about 75% of the model's token limit.
        return_response_tokens: If True, also return the number of tokens left available for the response after trimming.
        max_tokens: Instead of specifying a model or trim_ratio, you can specify this directly.

    Returns:
        Trimmed messages and optionally the number of tokens available for response."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5844
class AvailableModelsCache(InMemoryCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5913
def _infer_valid_provider_from_env_vars(custom_llm_provider: Optional[str]) -> List[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5940
def _get_valid_models_from_provider_api(provider_config: BaseLLMModelInfo, custom_llm_provider: str, litellm_params: Optional[LiteLLM_Params]) -> List[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 5964
def get_valid_models(check_provider_endpoint: Optional[bool], custom_llm_provider: Optional[str], litellm_params: Optional[LiteLLM_Params]) -> List[str]:
    """Returns a list of valid LLMs based on the set environment variables

    Args:
        check_provider_endpoint: If True, will check the provider's endpoint for valid models.
        custom_llm_provider: If provided, will only check the provider's endpoint for valid models.
    Returns:
        A list of valid LLMs"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6029
def print_args_passed_to_litellm(original_function, args, kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6081
def get_logging_id(start_time, response_obj):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6091
def _get_base_model_from_metadata(model_call_details):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6105
class ModelResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6136
class ModelResponseListIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6169
class CustomModelResponseIterator(Iterable):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6174
def is_cached_message(message: AllMessageValues) -> bool:
    """Returns true, if message is marked as needing to be cached.

    Used for anthropic/gemini context caching.

    Follows the anthropic format {"cache_control": {"type": "ephemeral"}}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6198
def is_base64_encoded(s: str) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6217
def get_base64_str(s: str) -> str:
    "s: b64str OR data:image/png;base64,b64str"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6226
def has_tool_call_blocks(messages: List[AllMessageValues]) -> bool:
    """Returns true, if messages has tool call blocks.

    Used for anthropic/bedrock message validation."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6238
def add_dummy_tool(custom_llm_provider: str) -> List[ChatCompletionToolParam]:
    """Prevent Anthropic from raising error when tool_use block exists but no tools are provided.

    Relevent Issues: https://github.com/BerriAI/litellm/issues/5388, https://github.com/BerriAI/litellm/issues/5747"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6269
def convert_to_dict(message: Union[(BaseModel, dict)]) -> dict:
    """Converts a message to a dictionary if it's a Pydantic model.

    Args:
        message: The message, which may be a Pydantic model or a dictionary.

    Returns:
        dict: The converted message."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6289
def validate_and_fix_openai_messages(messages: List):
    """Ensures all messages are valid OpenAI chat completion messages.

    Handles missing role for assistant messages."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6308
def cleanup_none_field_in_message(message: AllMessageValues):
    """Cleans up the message by removing the none field.

    remove None fields in the message - e.g. {"function": None} - some providers raise validation errors"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6318
def validate_chat_completion_user_messages(messages: List[AllMessageValues]):
    """Ensures all user messages are valid OpenAI chat completion messages.

    Args:
        messages: List of message dictionaries
        message_content_type: Type to validate content against

    Returns:
        List[dict]: The validated messages

    Raises:
        ValueError: If any message is invalid"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6359
def validate_chat_completion_tool_choice(tool_choice: Optional[Union[(dict, str)]]) -> Optional[Union[(dict, str)]]:
    """Confirm the tool choice is passed in the OpenAI format.

    Prevents user errors like: https://github.com/BerriAI/litellm/issues/7483"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6387
class ProviderConfigManager:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6712
def get_end_user_id_for_cost_tracking(litellm_params: dict, service_type: Literal[(?, ?)]="litellm_logging") -> Optional[str]:
    """Used for enforcing `disable_end_user_cost_tracking` param.

    service_type: "litellm_logging" or "prometheus" - used to allow prometheus only disable cost tracking."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6738
def should_use_cohere_v1_client(api_base: Optional[str], present_version_params: List[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6751
def is_prompt_caching_valid_prompt(model: str, messages: Optional[List[AllMessageValues]], tools: Optional[List[ChatCompletionToolParam]], custom_llm_provider: Optional[str]) -> bool:
    """Returns true if the prompt is valid for prompt caching.

    OpenAI + Anthropic providers have a minimum token count of 1024 for prompt caching."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6781
def extract_duration_from_srt_or_vtt(srt_or_vtt_content: str) -> Optional[float]:
    """Extracts the total duration (in seconds) from SRT or VTT content.

    Args:
        srt_or_vtt_content (str): The content of an SRT or VTT file as a string.

    Returns:
        Optional[float]: The total duration in seconds, or None if no timestamps are found."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6812
def _add_path_to_api_base(api_base: str, ending_path: str) -> str:
    """Adds an ending path to an API base URL while preventing duplicate path segments.

    Args:
        api_base: Base URL string
        ending_path: Path to append to the base URL

    Returns:
        Modified URL string with proper path handling"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6850
def get_non_default_completion_params(kwargs: dict) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6859
def add_openai_metadata(metadata: dict) -> dict:
    """Add metadata to openai optional parameters, excluding hidden params.

    OpenAI 'metadata' only supports string values.

    Args:
        params (dict): Dictionary of API parameters
        metadata (dict, optional): Metadata to include in the request

    Returns:
        dict: Updated parameters dictionary with visible metadata only"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6884
def return_raw_request(endpoint: CallTypes, kwargs: dict) -> RawRequestTypedDict:
    """Return the json str of the request

    This is currently in BETA, and tested for `/chat/completions` -> `litellm.completion` calls."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/utils.py Line: 6929
def jsonify_tools(tools: List[Any]) -> List[Dict]:
    """Fixes https://github.com/BerriAI/litellm/issues/9321

    Where user passes in a pydantic base model"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/timeout.py Line: 23
def timeout(timeout_duration: float, exception_to_raise=Timeout):
    """Wraps a function to raise the specified exception if execution time
    is greater than the specified timeout.

    Works with both synchronous and asynchronous callables, but with synchronous ones will introduce
    some overhead due to the backend use of threads and asyncio.

        :param float timeout_duration: Timeout duration in seconds. If none callable won't time out.
        :param OpenAIError exception_to_raise: Exception to raise when the callable times out.
            Defaults to TimeoutError.
        :return: The decorated function.
        :rtype: callable"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/timeout.py Line: 92
class _LoopWrapper(Thread):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/budget_manager.py Line: 26
class BudgetManager:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 249
class LiteLLM:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 264
class Chat:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 276
class Completions:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 294
class AsyncCompletions:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 313
async def acompletion(model: str, messages: List, functions: Optional[List], function_call: Optional[str], timeout: Optional[Union[(float, int)]], temperature: Optional[float], top_p: Optional[float], n: Optional[int], stream: Optional[bool], stream_options: Optional[dict], stop, max_tokens: Optional[int], max_completion_tokens: Optional[int], modalities: Optional[List[ChatCompletionModality]], prediction: Optional[ChatCompletionPredictionContentParam], audio: Optional[ChatCompletionAudioParam], presence_penalty: Optional[float], frequency_penalty: Optional[float], logit_bias: Optional[dict], user: Optional[str], response_format: Optional[Union[(dict, Type[BaseModel])]], seed: Optional[int], tools: Optional[List], tool_choice: Optional[str], parallel_tool_calls: Optional[bool], logprobs: Optional[bool], top_logprobs: Optional[int], deployment_id, reasoning_effort: Optional[Literal[(?, ?, ?)]], base_url: Optional[str], api_version: Optional[str], api_key: Optional[str], model_list: Optional[list], extra_headers: Optional[dict], thinking: Optional[AnthropicThinkingParam], **kwargs) -> Union[(ModelResponse, CustomStreamWrapper)]:
    """Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)

    Parameters:
        model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/
        messages (List): A list of message objects representing the conversation context (default is an empty list).

        OPTIONAL PARAMS
        functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).
        function_call (str, optional): The name of the function to call within the conversation (default is an empty string).
        temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).
        top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).
        n (int, optional): The number of completions to generate (default is 1).
        stream (bool, optional): If True, return a streaming response (default is False).
        stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.
        stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.
        max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).
        max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.
        modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `["text", "audio"]`
        prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.
        audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: ["audio"]
        presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.
        frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.
        logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.
        user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.
        metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.
        api_base (str, optional): Base URL for the API (default is None).
        api_version (str, optional): API version (default is None).
        api_key (str, optional): API key (default is None).
        model_list (list, optional): List of api base, version, keys
        timeout (float, optional): The maximum execution time in seconds for the completion request.

        LITELLM Specific Params
        mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).
        custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model="amazon.titan-tg1-large" and custom_llm_provider="bedrock"
    Returns:
        ModelResponse: A response object containing the generated completion and associated metadata.

    Notes:
        - This function is an asynchronous version of the `completion` function.
        - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.
        - If `stream` is True, the function returns an async generator that yields completion lines."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 505
async def _async_streaming(response, model, custom_llm_provider, args):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 522
def _handle_mock_potential_exceptions(mock_response: Union[(str, Exception, dict)], model: str, custom_llm_provider: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 581
def _handle_mock_timeout(mock_timeout: Optional[bool], timeout: Optional[Union[(float, str, ?)]], model: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 595
async def _handle_mock_timeout_async(mock_timeout: Optional[bool], timeout: Optional[Union[(float, str, ?)]], model: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 609
def _sleep_for_timeout(timeout: Union[(float, str, ?)]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 618
async def _sleep_for_timeout_async(timeout: Union[(float, str, ?)]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 627
def mock_completion(model: str, messages: List, stream: Optional[bool], n: Optional[int], mock_response: Union[(str, Exception, dict)]="This is a mock request", mock_tool_calls: Optional[List], mock_timeout: Optional[bool], logging, custom_llm_provider, timeout: Optional[Union[(float, str, ?)]], **kwargs):
    """Generate a mock completion response for testing or debugging purposes.

    This is a helper function that simulates the response structure of the OpenAI completion API.

    Parameters:
        model (str): The name of the language model for which the mock response is generated.
        messages (List): A list of message objects representing the conversation context.
        stream (bool, optional): If True, returns a mock streaming response (default is False).
        mock_response (str, optional): The content of the mock response (default is "This is a mock request").
        mock_timeout (bool, optional): If True, the mock response will be a timeout error (default is False).
        timeout (float, optional): The timeout value to use for the mock response (default is None).
        **kwargs: Additional keyword arguments that can be used but are not required.

    Returns:
        litellm.ModelResponse: A ModelResponse simulating a completion response with the specified model, messages, and mock response.

    Raises:
        Exception: If an error occurs during the generation of the mock completion response.
    Note:
        - This function is intended for testing or debugging purposes to generate mock completion responses.
        - If 'stream' is True, it returns a response that mimics the behavior of a streaming completion."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 778
def completion(model: str, messages: List, timeout: Optional[Union[(float, str, ?)]], temperature: Optional[float], top_p: Optional[float], n: Optional[int], stream: Optional[bool], stream_options: Optional[dict], stop, max_completion_tokens: Optional[int], max_tokens: Optional[int], modalities: Optional[List[ChatCompletionModality]], prediction: Optional[ChatCompletionPredictionContentParam], audio: Optional[ChatCompletionAudioParam], presence_penalty: Optional[float], frequency_penalty: Optional[float], logit_bias: Optional[dict], user: Optional[str], reasoning_effort: Optional[Literal[(?, ?, ?)]], response_format: Optional[Union[(dict, Type[BaseModel])]], seed: Optional[int], tools: Optional[List], tool_choice: Optional[Union[(str, dict)]], logprobs: Optional[bool], top_logprobs: Optional[int], parallel_tool_calls: Optional[bool], deployment_id, extra_headers: Optional[dict], functions: Optional[List], function_call: Optional[str], base_url: Optional[str], api_version: Optional[str], api_key: Optional[str], model_list: Optional[list], thinking: Optional[AnthropicThinkingParam], **kwargs) -> Union[(ModelResponse, CustomStreamWrapper)]:
    """Perform a completion() using any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)
    Parameters:
        model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/
        messages (List): A list of message objects representing the conversation context (default is an empty list).

        OPTIONAL PARAMS
        functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).
        function_call (str, optional): The name of the function to call within the conversation (default is an empty string).
        temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).
        top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).
        n (int, optional): The number of completions to generate (default is 1).
        stream (bool, optional): If True, return a streaming response (default is False).
        stream_options (dict, optional): A dictionary containing options for the streaming response. Only set this when you set stream: true.
        stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.
        max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).
        max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.
        modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request.. You can use `["text", "audio"]`
        prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.
        audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: ["audio"]
        presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.
        frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.
        logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.
        user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.
        logprobs (bool, optional): Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message
        top_logprobs (int, optional): An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
        metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.
        api_base (str, optional): Base URL for the API (default is None).
        api_version (str, optional): API version (default is None).
        api_key (str, optional): API key (default is None).
        model_list (list, optional): List of api base, version, keys
        extra_headers (dict, optional): Additional headers to include in the request.

        LITELLM Specific Params
        mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).
        custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model="amazon.titan-tg1-large" and custom_llm_provider="bedrock"
        max_retries (int, optional): The number of retries to attempt (default is 0).
    Returns:
        ModelResponse: A response object containing the generated completion and associated metadata.

    Note:
        - This function is used to perform completions() using the specified language model.
        - It supports various optional parameters for customizing the completion behavior.
        - If 'mock_response' is provided, a mock completion response is returned for testing or debugging."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 3191
def completion_with_retries(*args, **kwargs):
    "Executes a litellm.completion() with 3 retries"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 3223
async def acompletion_with_retries(*args, **kwargs):
    """[DEPRECATED]. Use 'acompletion' or router.acompletion instead!
    Executes a litellm.completion() with 3 retries"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 3255
async def aembedding(*args, **kwargs) -> EmbeddingResponse:
    """Asynchronously calls the `embedding` function with the given arguments and keyword arguments.

    Parameters:
    - `args` (tuple): Positional arguments to be passed to the `embedding` function.
    - `kwargs` (dict): Keyword arguments to be passed to the `embedding` function.

    Returns:
    - `response` (Any): The response returned by the `embedding` function."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 3318
def embedding(model, input, dimensions: Optional[int], encoding_format: Optional[str], timeout=600, api_base: Optional[str], api_version: Optional[str], api_key: Optional[str], api_type: Optional[str], caching: bool, user: Optional[str], custom_llm_provider, litellm_call_id, logger_fn, **kwargs) -> Union[(EmbeddingResponse, Coroutine[(Any, Any, EmbeddingResponse)])]:
    """Embedding function that calls an API to generate embeddings for the given input.

    Parameters:
    - model: The embedding model to use.
    - input: The input for which embeddings are to be generated.
    - encoding_format: Optional[str] The format to return the embeddings in. Can be either `float` or `base64`
    - dimensions: The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models.
    - timeout: The timeout value for the API call, default 10 mins
    - litellm_call_id: The call ID for litellm logging.
    - litellm_logging_obj: The litellm logging object.
    - logger_fn: The logger function.
    - api_base: Optional. The base URL for the API.
    - api_version: Optional. The version of the API.
    - api_key: Optional. The API key to use.
    - api_type: Optional. The type of the API.
    - caching: A boolean indicating whether to enable caching.
    - custom_llm_provider: The custom llm provider.

    Returns:
    - response: The response received from the API call.

    Raises:
    - exception_type: If an exception occurs during the API call."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4013
async def atext_completion(*args, **kwargs) -> Union[(TextCompletionResponse, TextCompletionStreamWrapper)]:
    "Implemented to handle async streaming for the text completion endpoint"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4087
def text_completion(prompt: Union[(str, List[Union[(str, List[Union[(str, List[int])]])]])], model: Optional[str], best_of: Optional[int], echo: Optional[bool], frequency_penalty: Optional[float], logit_bias: Optional[Dict[(int, int)]], logprobs: Optional[int], max_tokens: Optional[int], n: Optional[int], presence_penalty: Optional[float], stop: Optional[Union[(str, List[str])]], stream: Optional[bool], stream_options: Optional[dict], suffix: Optional[str], temperature: Optional[float], top_p: Optional[float], user: Optional[str], api_base: Optional[str], api_version: Optional[str], api_key: Optional[str], model_list: Optional[list], custom_llm_provider: Optional[str], *args, **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4356
async def aadapter_completion(adapter_id: str, **kwargs) -> Optional[Union[(BaseModel, AdapterCompletionStreamWrapper)]]:
    "Implemented to handle async calls for adapter_completion()"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4397
def adapter_completion(adapter_id: str, **kwargs) -> Optional[Union[(BaseModel, AdapterCompletionStreamWrapper)]]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4435
def moderation(input: str, model: Optional[str], api_key: Optional[str], **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4460
async def amoderation(input: str, model: Optional[str], api_key: Optional[str], custom_llm_provider: Optional[str], **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4512
async def aimage_generation(*args, **kwargs) -> ImageResponse:
    """Asynchronously calls the `image_generation` function with the given arguments and keyword arguments.

    Parameters:
    - `args` (tuple): Positional arguments to be passed to the `image_generation` function.
    - `kwargs` (dict): Keyword arguments to be passed to the `image_generation` function.

    Returns:
    - `response` (Any): The response returned by the `image_generation` function."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4566
def image_generation(prompt: str, model: Optional[str], n: Optional[int], quality: Optional[str], response_format: Optional[str], size: Optional[str], style: Optional[str], user: Optional[str], timeout=600, api_key: Optional[str], api_base: Optional[str], api_version: Optional[str], custom_llm_provider, **kwargs) -> ImageResponse:
    """Maps the https://api.openai.com/v1/images/generations endpoint.

    Currently supports just Azure + OpenAI."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4855
async def aimage_variation(*args, **kwargs) -> ImageResponse:
    """Asynchronously calls the `image_variation` function with the given arguments and keyword arguments.

    Parameters:
    - `args` (tuple): Positional arguments to be passed to the `image_variation` function.
    - `kwargs` (dict): Keyword arguments to be passed to the `image_variation` function.

    Returns:
    - `response` (Any): The response returned by the `image_variation` function."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 4910
def image_variation(image: FileTypes, model: str="dall-e-2", n: int=1, response_format: Literal[(?, ?)]="url", size: Optional[str], user: Optional[str], **kwargs) -> ImageResponse:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5009
async def atranscription(*args, **kwargs) -> TranscriptionResponse:
    """Calls openai + azure whisper endpoints.

    Allows router to load balance between them"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5060
def transcription(model: str, file: FileTypes, language: Optional[str], prompt: Optional[str], response_format: Optional[Literal[(?, ?, ?, ?, ?)]], timestamp_granularities: Optional[List[Literal[(?, ?)]]], temperature: Optional[int], user: Optional[str], timeout=600, api_key: Optional[str], api_base: Optional[str], api_version: Optional[str], max_retries: Optional[int], custom_llm_provider, **kwargs) -> Union[(TranscriptionResponse, Coroutine[(Any, Any, TranscriptionResponse)])]:
    """Calls openai + azure whisper endpoints.

    Allows router to load balance between them"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5259
async def aspeech(*args, **kwargs) -> HttpxBinaryResponseContent:
    "Calls openai tts endpoints."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5300
def speech(model: str, input: str, voice: Optional[Union[(str, dict)]], api_key: Optional[str], api_base: Optional[str], api_version: Optional[str], organization: Optional[str], project: Optional[str], max_retries: Optional[int], metadata: Optional[dict], timeout: Optional[Union[(float, ?)]], response_format: Optional[str], speed: Optional[int], client, headers: Optional[dict], custom_llm_provider: Optional[str], aspeech: Optional[bool], **kwargs) -> HttpxBinaryResponseContent:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5512
async def ahealth_check_wildcard_models(model: str, custom_llm_provider: str, model_params: dict) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5536
async def ahealth_check(model_params: dict, mode: Optional[Literal[(?, ?, ?, ?, ?, ?, ?, ?, ?)]]="chat", prompt: Optional[str], input: Optional[List]):
    """Support health checks for different providers. Return remaining rate limit, etc.

    Returns:
        {
            "x-ratelimit-remaining-requests": int,
            "x-ratelimit-remaining-tokens": int,
            "x-ms-region": str,
        }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5673
def print_verbose(print_statement):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5682
def config_completion(**kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5693
def stream_chunk_builder_text_completion(chunks: list, messages: Optional[List]) -> TextCompletionResponse:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/main.py Line: 5767
def stream_chunk_builder(chunks: list, messages: Optional[list], start_time, end_time) -> Optional[Union[(ModelResponse, TextCompletionResponse)]]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 91
def _cost_per_token_custom_pricing_helper(prompt_tokens: float, completion_tokens: float, response_time_ms: Optional[float], custom_cost_per_token: Optional[CostPerToken], custom_cost_per_second: Optional[float]) -> Optional[Tuple[(float, float)]]:
    "Internal helper function for calculating cost, if custom pricing given"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 114
def cost_per_token(model: str="", prompt_tokens: int, completion_tokens: int, response_time_ms: Optional[float], custom_llm_provider: Optional[str], region_name, prompt_characters: Optional[int], completion_characters: Optional[int], cache_creation_input_tokens: Optional[int], cache_read_input_tokens: Optional[int], custom_cost_per_token: Optional[CostPerToken], custom_cost_per_second: Optional[float], number_of_queries: Optional[int], usage_object: Optional[Usage], rerank_billed_units: Optional[RerankBilledUnits], call_type: CallTypesLiteral="completion", audio_transcription_file_duration: float) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Parameters:
        model (str): The name of the model to use. Default is ""
        prompt_tokens (int): The number of tokens in the prompt.
        completion_tokens (int): The number of tokens in the completion.
        response_time (float): The amount of time, in milliseconds, it took the call to complete.
        prompt_characters (float): The number of characters in the prompt. Used for vertex ai cost calculation.
        completion_characters (float): The number of characters in the completion response. Used for vertex ai cost calculation.
        custom_llm_provider (str): The llm provider to whom the call was made (see init.py for full list)
        custom_cost_per_token: Optional[CostPerToken]: the cost per input + output token for the llm api call.
        custom_cost_per_second: Optional[float]: the cost per second for the llm api call.
        call_type: Optional[str]: the call type

    Returns:
        tuple: A tuple containing the cost in USD dollars for prompt tokens and completion tokens, respectively."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 366
def get_replicate_completion_pricing(completion_response: dict, total_time):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 378
def has_hidden_params(obj: Any) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 382
def _get_provider_for_cost_calc(model: Optional[str], custom_llm_provider: Optional[str]) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 401
def _select_model_name_for_cost_calc(model: Optional[str], completion_response: Optional[Any], base_model: Optional[str], custom_pricing: Optional[bool], custom_llm_provider: Optional[str], router_model_id: Optional[str]) -> Optional[str]:
    """1. If custom pricing is true, return received model name
    2. If base_model is set (e.g. for azure models), return that
    3. If completion response has model set return that
    4. Check if model is passed in return that"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 468
def _model_contains_known_llm_provider(model: str) -> bool:
    "Check if the model contains a known llm provider"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 476
def _get_usage_object(completion_response: Any) -> Optional[Usage]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 511
def _is_known_usage_objects(usage_obj):
    "Returns True if the usage obj is a known Usage type"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 518
def _infer_call_type(call_type: Optional[CallTypesLiteral], completion_response: Any) -> Optional[CallTypesLiteral]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 545
def completion_cost(completion_response, model: Optional[str], prompt="", messages: List, completion="", total_time: Optional[float], call_type: Optional[CallTypesLiteral], custom_llm_provider, region_name, size: Optional[str], quality: Optional[str], n: Optional[int], custom_cost_per_token: Optional[CostPerToken], custom_cost_per_second: Optional[float], optional_params: Optional[dict], custom_pricing: Optional[bool], base_model: Optional[str], standard_built_in_tools_params: Optional[StandardBuiltInToolsParams], litellm_model_name: Optional[str], router_model_id: Optional[str]) -> float:
    """Calculate the cost of a given completion call fot GPT-3.5-turbo, llama2, any litellm supported llm.

    Parameters:
        completion_response (litellm.ModelResponses): [Required] The response received from a LiteLLM completion request.

        [OPTIONAL PARAMS]
        model (str): Optional. The name of the language model used in the completion calls
        prompt (str): Optional. The input prompt passed to the llm
        completion (str): Optional. The output completion text from the llm
        total_time (float, int): Optional. (Only used for Replicate LLMs) The total time used for the request in seconds
        custom_cost_per_token: Optional[CostPerToken]: the cost per input + output token for the llm api call.
        custom_cost_per_second: Optional[float]: the cost per second for the llm api call.

    Returns:
        float: The cost in USD dollars for the completion based on the provided parameters.

    Exceptions:
        Raises exception if model not in the litellm model cost map. Register model, via custom pricing or PR - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json


    Note:
        - If completion_response is provided, the function extracts token information and the model name from it.
        - If completion_response is not provided, the function calculates token counts based on the model and input text.
        - The cost is calculated based on the model, prompt tokens, and completion tokens.
        - For certain models containing "togethercomputer" in the name, prices are based on the model size.
        - For un-mapped Replicate models, the cost is calculated based on the total time used for the request."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 915
def get_response_cost_from_hidden_params(hidden_params: Union[(dict, BaseModel)]) -> Optional[float]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 935
def response_cost_calculator(response_object: Union[(ModelResponse, EmbeddingResponse, ImageResponse, TranscriptionResponse, TextCompletionResponse, HttpxBinaryResponseContent, RerankResponse, ResponsesAPIResponse, LiteLLMRealtimeStreamLoggingObject)], model: str, custom_llm_provider: Optional[str], call_type: Literal[(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)], optional_params: dict, cache_hit: Optional[bool], base_model: Optional[str], custom_pricing: Optional[bool], prompt: str="", standard_built_in_tools_params: Optional[StandardBuiltInToolsParams], litellm_model_name: Optional[str], router_model_id: Optional[str]) -> float:
    """Returns
    - float or None: cost of response"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 1013
def rerank_cost(model: str, custom_llm_provider: Optional[str], billed_units: Optional[RerankBilledUnits]) -> Tuple[(float, float)]:
    """Returns
    - float or None: cost of response OR none if error."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 1051
def transcription_cost(model: str, custom_llm_provider: Optional[str], duration: float) -> Tuple[(float, float)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 1059
def default_image_cost_calculator(model: str, custom_llm_provider: Optional[str], quality: Optional[str], n: Optional[int]=1, size: Optional[str]="1024-x-1024", optional_params: Optional[dict]) -> float:
    """Default image cost calculator for image generation

    Args:
        model (str): Model name
        image_response (ImageResponse): Response from image generation
        quality (Optional[str]): Image quality setting
        n (Optional[int]): Number of images generated
        size (Optional[str]): Image size (e.g. "1024x1024" or "1024-x-1024")

    Returns:
        float: Cost in USD for the image generation

    Raises:
        Exception: If model pricing not found in cost map"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 1132
def batch_cost_calculator(usage: Usage, model: str, custom_llm_provider: Optional[str]) -> Tuple[(float, float)]:
    "Calculate the cost of a batch job"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 1183
class RealtimeAPITokenUsageProcessor:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/cost_calculator.py Line: 1309
def handle_realtime_stream_cost_calculation(results: OpenAIRealtimeStreamList, combined_usage_object: Usage, custom_llm_provider: str, litellm_model_name: str) -> float:
    """Handles the cost calculation for realtime stream responses.

    Pick the 'response.done' events. Calculate total cost across all 'response.done' events.

    Args:
        results: A list of OpenAIRealtimeStreamBaseObject objects"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 20
class AuthenticationError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 66
class NotFoundError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 111
class BadRequestError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 157
class UnprocessableEntityError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 196
class Timeout:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 241
class PermissionDeniedError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 280
class RateLimitError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 333
class ContextWindowExceededError(BadRequestError):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 377
class RejectedRequestError(BadRequestError):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 419
class ContentPolicyViolationError(BadRequestError):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 461
class ServiceUnavailableError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 507
class InternalServerError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 554
class APIError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 595
class APIConnectionError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 634
class APIResponseValidationError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 671
class JSONSchemaValidationError(APIResponseValidationError):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 685
class OpenAIError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 691
class UnsupportedParamsError(BadRequestError):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 741
class BudgetExceededError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 756
class InvalidRequestError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 773
class MockException:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/exceptions.py Line: 798
class LiteLLMUnknownProvider(BadRequestError):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_service_logger.py Line: 25
class ServiceLogging(CustomLogger):
    "Separate class used for monitoring health of litellm-adjacent services (redis/postgres)."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/__init__.py Line: 333
class MyLocal:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/__init__.py Line: 341
def identify(event_details):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/__init__.py Line: 436
def is_bedrock_pricing_only_model(key: str) -> bool:
    """Excludes keys with the pattern 'bedrock/<region>/<model>'. These are in the model_prices_and_context_window.json file for pricing purposes only.

    Args:
        key (str): A key to filter.

    Returns:
        bool: True if the key matches the Bedrock pattern, False otherwise."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/__init__.py Line: 456
def is_openai_finetune_model(key: str) -> bool:
    """Excludes model cost keys with the pattern 'ft:<model>'. These are in the model_prices_and_context_window.json file for pricing purposes only.

    Args:
        key (str): A key to filter.

    Returns:
        bool: True if the key matches the OpenAI finetune pattern, False otherwise."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/__init__.py Line: 469
def add_known_models():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/scheduler.py Line: 12
class SchedulerCacheKeys:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/scheduler.py Line: 19
class FlowItem(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/scheduler.py Line: 25
class Scheduler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router.py Line: 165
class RoutingArgs:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router.py Line: 169
class Router:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 26
def _get_redis_kwargs():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 43
def _get_redis_url_kwargs(client):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 62
def _get_redis_cluster_kwargs(client):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 78
def _get_redis_env_kwarg_mapping():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 84
def _redis_kwargs_from_environment():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 95
def get_redis_url_from_environment():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 114
def _get_redis_client_logic(**env_overrides):
    "Common functionality across sync + async redis client implementations"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 176
def init_redis_cluster(redis_kwargs) -> ?:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 204
def _init_redis_sentinel(redis_kwargs) -> ?:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 228
def _init_async_redis_sentinel(redis_kwargs) -> ?:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 252
def get_redis_client(**env_overrides):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 273
def get_redis_async_client(**env_overrides) -> ?:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/_redis.py Line: 318
def get_redis_connection_pool(**env_overrides):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/fallback_utils.py Line: 10
async def async_completion_with_fallbacks(**kwargs):
    """Asynchronously attempts completion with fallback models if the primary model fails.

    Args:
        **kwargs: Keyword arguments for completion, including:
            - model (str): Primary model to use
            - fallbacks (List[Union[str, dict]]): List of fallback models/configs
            - Other completion parameters

    Returns:
        ModelResponse: The completion response from the first successful model

    Raises:
        Exception: If all models fail and no response is generated"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/fallback_utils.py Line: 64
def completion_with_fallbacks(**kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/json_validation_rule.py Line: 4
def validate_schema(schema: dict, response: str):
    """Validate if the returned json response follows the schema.

    Params:
    - schema - dict: JSON schema
    - response - str: Received json response as string."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/initialize_dynamic_callback_params.py Line: 7
def initialize_standard_callback_dynamic_params(kwargs: Optional[Dict]) -> StandardCallbackDynamicParams:
    """Initialize the standard callback dynamic params from the kwargs

    checks if langfuse_secret_key, gcs_bucket_name in kwargs and sets the corresponding attributes in StandardCallbackDynamicParams"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/streaming_chunk_builder_utils.py Line: 25
class ChunkProcessor:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/streaming_chunk_builder_utils.py Line: 415
def concatenate_base64_list(base64_strings: List[str]) -> str:
    """Concatenates a list of base64-encoded strings.

    Args:
        base64_strings (List[str]): A list of base64 strings to concatenate.

    Returns:
        str: The concatenated result as a base64-encoded string."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/mock_functions.py Line: 12
def mock_embedding(model: str, mock_response: Optional[List[float]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/mock_functions.py Line: 22
def mock_image_generation(model: str, mock_response: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/sensitive_data_masker.py Line: 6
class SensitiveDataMasker:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/dot_notation_indexing.py Line: 12
def get_nested_value(data: Dict[(str, Any)], key_path: str, default: Optional[T]) -> Optional[T]:
    """Retrieves a value from a nested dictionary using dot notation.

    Args:
        data: The dictionary to search in
        key_path: The path to the value using dot notation (e.g., "a.b.c")
        default: The default value to return if the path is not found

    Returns:
        The value at the specified path, or the default value if not found

    Example:
        >>> data = {"a": {"b": {"c": "value"}}}
        >>> get_nested_value(data, "a.b.c")
        'value'
        >>> get_nested_value(data, "a.b.d", "default")
        'default'"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/safe_json_dumps.py Line: 6
def safe_dumps(data: Any, max_depth: int=DEFAULT_MAX_RECURSE_DEPTH) -> str:
    """Recursively serialize data while detecting circular references.
    If a circular reference is detected then a marker string is returned."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/health_check_utils.py Line: 6
def _filter_model_params(model_params: dict) -> dict:
    "Remove 'messages' param from model params."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/health_check_utils.py Line: 11
def _create_health_check_response(response_headers: dict) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/get_model_cost_map.py Line: 16
def get_model_cost_map(url: str) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/dd_tracing.py Line: 18
class NullSpan:
    "A no-op span implementation."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/dd_tracing.py Line: 32
def null_tracer(name, **kwargs):
    "Context manager that yields a no-op span."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/dd_tracing.py Line: 37
class NullTracer:
    "A no-op tracer implementation."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/dd_tracing.py Line: 55
def _should_use_dd_tracer():
    "Returns True if `USE_DDTRACE` is set to True in .env"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/get_litellm_params.py Line: 4
def _get_base_model_from_litellm_call_metadata(metadata: Optional[dict]) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/get_litellm_params.py Line: 20
def get_litellm_params(api_key: Optional[str], force_timeout=600, azure, logger_fn, verbose, hugging_face, replicate, together_ai, custom_llm_provider: Optional[str], api_base: Optional[str], litellm_call_id, model_alias_map, completion_call_id, metadata: Optional[dict], model_info, proxy_server_request, acompletion, aembedding, preset_cache_key, no_log, input_cost_per_second, input_cost_per_token, output_cost_per_token, output_cost_per_second, cooldown_time, text_completion, azure_ad_token_provider, user_continue_message, base_model: Optional[str], litellm_trace_id: Optional[str], hf_model_name: Optional[str], custom_prompt_dict: Optional[dict], litellm_metadata: Optional[dict], disable_add_transform_inline_image_block: Optional[bool], drop_params: Optional[bool], prompt_id: Optional[str], prompt_variables: Optional[dict], async_call: Optional[bool], ssl_verify: Optional[bool], merge_reasoning_content_in_choices: Optional[bool], api_version: Optional[str], max_retries: Optional[int], **kwargs) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/core_helpers.py Line: 18
def map_finish_reason(finish_reason: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/core_helpers.py Line: 57
def remove_index_from_tool_calls(messages: Optional[List[AllMessageValues]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/core_helpers.py Line: 73
def get_litellm_metadata_from_kwargs(kwargs: dict):
    """Helper to get litellm metadata from all litellm request kwargs

    Return `litellm_metadata` if it exists, otherwise return `metadata`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/core_helpers.py Line: 92
def _get_parent_otel_span_from_kwargs(kwargs: Optional[dict]) -> Union[(Span, ?)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/core_helpers.py Line: 118
def process_response_headers(response_headers: Union[(?, dict)]) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/token_counter.py Line: 22
def get_modified_max_tokens(model: str, base_model: str, messages: Optional[list], user_max_tokens: Optional[int], buffer_perc: Optional[float], buffer_num: Optional[float]) -> Optional[int]:
    """Params:

    Returns the user's max output tokens, adjusted for:
    - the size of input - for models where input + output can't exceed X
    - model max output tokens - for models where there is a separate output token limit"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/token_counter.py Line: 99
def resize_image_high_res(width: int, height: int) -> Tuple[(int, int)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/token_counter.py Line: 141
def calculate_tiles_needed(resized_width, resized_height, tile_width=MAX_TILE_WIDTH, tile_height=MAX_TILE_HEIGHT):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/token_counter.py Line: 153
def get_image_type(image_data: bytes) -> Union[(str, ?)]:
    """take an image (really only the first ~100 bytes max are needed)
    and return 'png' 'gif' 'jpeg' 'webp' 'heic' or None. method added to
    allow deprecation of imghdr in 3.13"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/token_counter.py Line: 176
def get_image_dimensions(data: str) -> Tuple[(int, int)]:
    """Async Function to get the dimensions of an image from a URL or base64 encoded string.

    Args:
        data (str): The URL or base64 encoded string of the image.

    Returns:
        Tuple[int, int]: The width and height of the image."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/token_counter.py Line: 245
def calculate_img_tokens(data, mode: Literal[(?, ?, ?)]="auto", base_tokens: int=85, use_default_image_token_count: bool):
    """Calculate the number of tokens for an image.

    Args:
        data (str): The URL or base64 encoded string of the image.
        mode (Literal["low", "high", "auto"]): The mode to use for calculating the number of tokens.
        base_tokens (int): The base number of tokens for an image.
        use_default_image_token_count (bool): When True, will NOT make a GET request to the image URL and instead return the default image dimensions.

    Returns:
        int: The number of tokens for the image."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py Line: 27
def get_error_message(error_obj) -> Optional[str]:
    """OpenAI Returns Error message that is nested, this extract the message

    Example:
    {
        'request': "<Request('POST', 'https://api.openai.com/v1/chat/completions')>",
        'message': "Error code: 400 - {'error': {'message': "Invalid 'temperature': decimal above maximum value. Expected a value <= 2, but got 200 instead.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'decimal_above_max_value'}}",
        'body': {
            'message': "Invalid 'temperature': decimal above maximum value. Expected a value <= 2, but got 200 instead.",
            'type': 'invalid_request_error',
            'param': 'temperature',
            'code': 'decimal_above_max_value'
        },
        'code': 'decimal_above_max_value',
        'param': 'temperature',
        'type': 'invalid_request_error',
        'response': "<Response [400 Bad Request]>",
        'status_code': 400,
        'request_id': 'req_f287898caa6364cd42bc01355f74dd2a'
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py Line: 66
def _get_response_headers(original_exception: Exception) -> Optional[?]:
    """Extract and return the response headers from an exception, if present.

    Used for accurate retry logic."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py Line: 91
def extract_and_raise_litellm_exception(response: Optional[Any], error_str: str, model: str, custom_llm_provider: str):
    """Covers scenario where litellm sdk calling proxy.

    Enables raising the special errors raised by litellm, eg. ContextWindowExceededError.

    Relevant Issue: https://github.com/BerriAI/litellm/issues/7259"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py Line: 123
def exception_type(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs):
    "Maps an LLM Provider Exception to OpenAI Exception Format"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py Line: 2232
def exception_logging(additional_args, logger_fn, exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py Line: 2262
def _add_key_name_and_team_to_alert(request_info: str, metadata: dict) -> str:
    """Internal helper function for litellm proxy
    Add the Key Name + Team Name to the error
    Only gets added if the metadata contains the user_api_key_alias and user_api_key_team_alias

    [Non-Blocking helper function]"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/model_param_helper.py Line: 19
class ModelParamHelper:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/realtime_streaming.py Line: 51
class RealTimeStreaming:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_callback_manager.py Line: 9
class LoggingCallbackManager:
    """A centralized class that allows easy add / remove callbacks for litellm.

    Goals of this class:
    - Prevent adding duplicate callbacks / success_callback / failure_callback
    - Keep a reasonable MAX_CALLBACKS limit (this ensures callbacks don't exponentially grow and consume CPU Resources)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_request_utils.py Line: 6
def _ensure_extra_body_is_safe(extra_body: Optional[Dict]) -> Optional[Dict]:
    """Ensure that the extra_body sent in the request is safe,  otherwise users will see this error

    "Object of type TextPromptClient is not JSON serializable


    Relevant Issue: https://github.com/BerriAI/litellm/issues/4140"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_request_utils.py Line: 33
def pick_cheapest_chat_models_from_llm_provider(custom_llm_provider: str, n=1):
    """Pick the n cheapest chat models from the LLM provider.

    Args:
        custom_llm_provider (str): The name of the LLM provider.
        n (int): The number of cheapest models to return.

    Returns:
        list[str]: A list of the n cheapest chat models."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/streaming_handler.py Line: 36
def is_async_iterable(obj: Any) -> bool:
    """Check if an object is an async iterable (can be used with 'async for').

    Args:
        obj: Any Python object to check

    Returns:
        bool: True if the object is async iterable, False otherwise"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/streaming_handler.py Line: 49
def print_verbose(print_statement):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/streaming_handler.py Line: 57
class CustomStreamWrapper:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/streaming_handler.py Line: 1862
def calculate_total_usage(chunks: List[ModelResponse]) -> Usage:
    "Assume most recent usage chunk has total usage uptil then."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/streaming_handler.py Line: 1882
def generic_chunk_has_all_required_fields(chunk: dict) -> bool:
    """Checks if the provided chunk dictionary contains all required fields for GenericStreamingChunk.

    :param chunk: The dictionary to check.
    :return: True if all required fields are present, False otherwise."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/rules.py Line: 6
class Rules:
    """Fail calls based on the input or llm api output

    Example usage:
    import litellm
    def my_custom_rule(input): # receives the model response
            if "i don't think i can answer" in input: # trigger fallback if the model refuses to answer
                    return False
            return True

    litellm.post_call_rules = [my_custom_rule] # have these be functions that can be called to fail a call

    response = litellm.completion(model="gpt-3.5-turbo", messages=[{"role": "user",
        "content": "Hey, how's it going?"}], fallbacks=["openrouter/mythomax"])"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/redact_messages.py Line: 28
def redact_message_input_output_from_custom_logger(litellm_logging_obj: LiteLLMLoggingObject, result, custom_logger: CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/redact_messages.py Line: 39
def perform_redaction(model_call_details: dict, result):
    "Performs the actual redaction on the logging object and result."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/redact_messages.py Line: 76
def should_redact_message_logging(model_call_details: dict) -> bool:
    "Determine if message logging should be redacted."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/redact_messages.py Line: 118
def redact_message_input_output_from_logging(model_call_details: dict, result, input: Optional[Any]) -> Any:
    """Removes messages, prompts, input, response from logging. This modifies the data in-place
    only redacts when litellm.turn_off_message_logging == True"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/redact_messages.py Line: 130
def _get_turn_off_message_logging_from_dynamic_params(model_call_details: dict) -> Optional[bool]:
    """gets the value of `turn_off_message_logging` from the dynamic params, if it exists.

    handles boolean and string values of `turn_off_message_logging`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/redact_messages.py Line: 152
def redact_user_api_key_info(metadata: dict) -> dict:
    """removes any user_api_key_info before passing to logging object, if flag set

    Usage:

    SDK
    ```python
    litellm.redact_user_api_key_info = True
    ```

    PROXY:
    ```yaml
    litellm_settings:
        redact_user_api_key_info: true
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/asyncify.py Line: 13
def function_has_argument(function: Callable, arg_name: str) -> bool:
    "Helper function to check if a function has a specific argument."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/asyncify.py Line: 21
def asyncify(function: Callable[(T_ParamSpec, T_Retval)], *, cancellable: bool, limiter: Optional[?]) -> Callable[(T_ParamSpec, Awaitable[T_Retval])]:
    """Take a blocking function and create an async one that receives the same
    positional and keyword arguments, and that when called, calls the original function
    in a worker thread using `anyio.to_thread.run_sync()`.

    If the `cancellable` option is enabled and the task waiting for its completion is
    cancelled, the thread will still run its course but its return value (or any raised
    exception) will be ignored.

    ## Arguments
    - `function`: a blocking regular callable (e.g. a function)
    - `cancellable`: `True` to allow cancellation of the operation
    - `limiter`: capacity limiter to use to limit the total amount of threads running
        (if omitted, the default limiter is used)

    ## Return
    An async function that takes the same positional and keyword arguments as the
    original one, that when called runs the same original function in a thread worker
    and returns the result."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/asyncify.py Line: 72
def run_async_function(async_function, *args, **kwargs):
    """Helper utility to run an async function in a sync context.
    Handles the case where there is an existing event loop running.

    Args:
        async_function (Callable): The async function to run
        *args: Positional arguments to pass to the async function
        **kwargs: Keyword arguments to pass to the async function

    Returns:
        The result of the async function execution

    Example:
        ```python
        async def my_async_func(x, y):
            return x + y

        result = run_async_function(my_async_func, 1, 2)
        ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/get_supported_openai_params.py Line: 8
def get_supported_openai_params(model: str, custom_llm_provider: Optional[str], request_type: Literal[(?, ?, ?)]="chat_completion") -> Optional[list]:
    """Returns the supported openai params for a given model + provider

    Example:
    ```
    get_supported_openai_params(model="anthropic.claude-3", custom_llm_provider="bedrock")
    ```

    Returns:
    - List if custom_llm_provider is mapped
    - None if unmapped"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py Line: 12
def _is_non_openai_azure_model(model: str) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py Line: 25
def handle_cohere_chat_model_custom_llm_provider(model: str, custom_llm_provider: Optional[str]) -> Tuple[(str, Optional[str])]:
    """if user sets model = "cohere/command-r" -> use custom_llm_provider = "cohere_chat"

    Args:
        model:
        custom_llm_provider:

    Returns:
        model, custom_llm_provider"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py Line: 55
def handle_anthropic_text_model_custom_llm_provider(model: str, custom_llm_provider: Optional[str]) -> Tuple[(str, Optional[str])]:
    """if user sets model = "anthropic/claude-2" -> use custom_llm_provider = "anthropic_text"

    Args:
        model:
        custom_llm_provider:

    Returns:
        model, custom_llm_provider"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py Line: 88
def get_llm_provider(model: str, custom_llm_provider: Optional[str], api_base: Optional[str], api_key: Optional[str], litellm_params: Optional[LiteLLM_Params]) -> Tuple[(str, str, Optional[str], Optional[str])]:
    """Returns the provider for a given model name - e.g. 'azure/chatgpt-v-2' -> 'azure'

    For router -> Can also give the whole litellm param dict -> this function will extract the relevant details

    Raises Error - if unable to map model to a provider

    Return model, custom_llm_provider, dynamic_api_key, api_base"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py Line: 375
def _get_openai_compatible_provider_info(model: str, api_base: Optional[str], api_key: Optional[str], dynamic_api_key: Optional[str]) -> Tuple[(str, str, Optional[str], Optional[str])]:
    """Returns:
        Tuple[str, str, Optional[str], Optional[str]]:
            model: str
            custom_llm_provider: str
            dynamic_api_key: Optional[str]
            api_base: Optional[str]"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/credential_accessor.py Line: 9
class CredentialAccessor:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 178
class ServiceTraceIDCache:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 197
class Logging(LiteLLMLoggingBaseClass):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 2529
def _get_masked_values(sensitive_object: dict, ignore_sensitive_values: bool, mask_all_values: bool, unmasked_length: int=4, number_of_asterisks: Optional[int]=4) -> dict:
    """Internal debugging helper function

    Masks the headers of the request sent from LiteLLM

    Args:
        masked_length: Optional length for the masked portion (number of *). If set, will use exactly this many *
                     regardless of original string length. The total length will be unmasked_length + masked_length."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 2581
def set_callbacks(callback_list, function_id):
    "Globally sets the callback client"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 2677
def _init_custom_logger_compatible_class(logging_integration: _custom_logger_compatible_callbacks_literal, internal_usage_cache: Optional[DualCache], llm_router: Optional[Any], custom_logger_init_args: Optional[dict]) -> Optional[CustomLogger]:
    "Initialize a custom logger compatible class"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 2990
def get_custom_logger_compatible_class(logging_integration: _custom_logger_compatible_callbacks_literal) -> Optional[CustomLogger]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3123
def _get_custom_logger_settings_from_proxy_server(callback_name: str) -> Dict:
    """Get the settings for a custom logger from the proxy server config.yaml

    Proxy server config.yaml defines callback_settings as:

    callback_settings:
        otel:
            message_logging: False"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3140
def use_custom_pricing_for_model(litellm_params: Optional[dict]) -> bool:
    """Check if the model uses custom pricing

    Returns True if any of `SPECIAL_MODEL_INFO_PARAMS` are present in `litellm_params` or `model_info`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3161
def is_valid_sha256_hash(value: str) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3166
class StandardLoggingPayloadSetup:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3504
def get_standard_logging_object_payload(kwargs: Optional[dict], init_response_obj: Union[(Any, BaseModel, dict)], start_time: dt_object, end_time: dt_object, logging_obj: Logging, status: StandardLoggingPayloadStatus, error_str: Optional[str], original_exception: Optional[Exception], standard_built_in_tools_params: Optional[StandardBuiltInToolsParams]) -> Optional[StandardLoggingPayload]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3709
def emit_standard_logging_payload(payload: StandardLoggingPayload):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3714
def get_standard_logging_metadata(metadata: Optional[Dict[(str, Any)]]) -> StandardLoggingMetadata:
    """Clean and filter the metadata dictionary to include only the specified keys in StandardLoggingMetadata.

    Args:
        metadata (Optional[Dict[str, Any]]): The original metadata dictionary.

    Returns:
        StandardLoggingMetadata: A StandardLoggingMetadata object containing the cleaned metadata.

    Note:
        - If the input metadata is None or not a dictionary, an empty StandardLoggingMetadata object is returned.
        - If 'user_api_key' is present in metadata and is a valid SHA256 hash, it's stored as 'user_api_key_hash'."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3766
def scrub_sensitive_keys_in_metadata(litellm_params: Optional[dict]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3792
def modify_integration(integration_name, integration_params):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3800
def _get_traceback_str_for_error(error_str: str) -> str:
    "function wrapped with lru_cache to limit the number of times `traceback.format_exc()` is called"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py Line: 3813
def create_dummy_standard_logging_payload() -> StandardLoggingPayload:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py Line: 32
def convert_litellm_response_object_to_str(response_obj: Union[(Any, LiteLLMModelResponse)]) -> Optional[str]:
    "Get the string of the response object from LiteLLM"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py Line: 50
def _assemble_complete_response_from_streaming_chunks(result: Union[(ModelResponse, TextCompletionResponse, ModelResponseStream)], start_time: datetime, end_time: datetime, request_kwargs: dict, streaming_chunks: List[Any], is_async: bool):
    """Assemble a complete response from a streaming chunks

    - assemble a complete streaming response if result.choices[0].finish_reason is not None
    - else append the chunk to the streaming_chunks


    Args:
        result: ModelResponse
        start_time: datetime
        end_time: datetime
        request_kwargs: dict
        streaming_chunks: List[Any]
        is_async: bool

    Returns:
        Optional[Union[ModelResponse, TextCompletionResponse]]: Complete streaming response"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py Line: 106
def _set_duration_in_model_call_details(logging_obj: Any, start_time: datetime, end_time: datetime):
    "Helper to set duration in model_call_details, with error handling"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py Line: 124
def track_llm_api_timing():
    """Decorator to track LLM API call timing for both sync and async functions.
    The logging_obj is expected to be passed as an argument to the decorated function."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/duration_parser.py Line: 15
def _extract_from_regex(duration: str) -> Tuple[(int, str)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/duration_parser.py Line: 27
def get_last_day_of_month(year, month):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/duration_parser.py Line: 37
def duration_in_seconds(duration: str) -> int:
    """Parameters:
    - duration:
        - "<number>s" - seconds
        - "<number>m" - minutes
        - "<number>h" - hours
        - "<number>d" - days
        - "<number>w" - weeks
        - "<number>mo" - months

    Returns time in seconds till when budget needs to be reset"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/fine_tuning/main.py Line: 41
async def acreate_fine_tuning_job(model: str, training_file: str, hyperparameters: Optional[dict], suffix: Optional[str], validation_file: Optional[str], integrations: Optional[List[str]], seed: Optional[int], custom_llm_provider: Literal[(?, ?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> FineTuningJob:
    "Async: Creates and executes a batch from an uploaded file of request"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/fine_tuning/main.py Line: 95
def create_fine_tuning_job(model: str, training_file: str, hyperparameters: Optional[dict], suffix: Optional[str], validation_file: Optional[str], integrations: Optional[List[str]], seed: Optional[int], custom_llm_provider: Literal[(?, ?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Union[(FineTuningJob, Coroutine[(Any, Any, FineTuningJob)])]:
    """Creates a fine-tuning job which begins the process of creating a new model from a given dataset.

    Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/fine_tuning/main.py Line: 290
async def acancel_fine_tuning_job(fine_tuning_job_id: str, custom_llm_provider: Literal[(?, ?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> FineTuningJob:
    "Async: Immediately cancel a fine-tune job."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/fine_tuning/main.py Line: 327
def cancel_fine_tuning_job(fine_tuning_job_id: str, custom_llm_provider: Literal[(?, ?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Union[(FineTuningJob, Coroutine[(Any, Any, FineTuningJob)])]:
    """Immediately cancel a fine-tune job.

    Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/fine_tuning/main.py Line: 446
async def alist_fine_tuning_jobs(after: Optional[str], limit: Optional[int], custom_llm_provider: Literal[(?, ?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs):
    "Async: List your organization's fine-tuning jobs"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/fine_tuning/main.py Line: 485
def list_fine_tuning_jobs(after: Optional[str], limit: Optional[int], custom_llm_provider: Literal[(?, ?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs):
    """List your organization's fine-tuning jobs

    Params:

    - after: Optional[str] = None, Identifier for the last job from the previous pagination request.
    - limit: Optional[int] = None, Number of fine-tuning jobs to retrieve. Defaults to 20"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/fine_tuning/main.py Line: 609
async def aretrieve_fine_tuning_job(fine_tuning_job_id: str, custom_llm_provider: Literal[(?, ?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> FineTuningJob:
    "Async: Get info about a fine-tuning job."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/fine_tuning/main.py Line: 646
def retrieve_fine_tuning_job(fine_tuning_job_id: str, custom_llm_provider: Literal[(?, ?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Union[(FineTuningJob, Coroutine[(Any, Any, FineTuningJob)])]:
    "Get info about a fine-tuning job."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/utils.py Line: 9
def get_optional_params_add_message(role: Optional[str], content: Optional[Union[(str, List[Union[(MessageContentTextObject, MessageContentImageFileObject, MessageContentImageURLObject)]])]], attachments: Optional[List[Attachment]], metadata: Optional[dict], custom_llm_provider: str, **kwargs):
    """Azure doesn't support 'attachments' for creating a message

    Reference - https://learn.microsoft.com/en-us/azure/ai-services/openai/assistants-reference-messages?tabs=python#create-message"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/utils.py Line: 87
def get_optional_params_image_gen(n: Optional[int], quality: Optional[str], response_format: Optional[str], size: Optional[str], style: Optional[str], user: Optional[str], custom_llm_provider: Optional[str], **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 37
async def aget_assistants(custom_llm_provider: Literal[(?, ?)], client: Optional[AsyncOpenAI], **kwargs) -> AsyncCursorPage[Assistant]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 74
def get_assistants(custom_llm_provider: Literal[(?, ?)], client: Optional[Any], api_key: Optional[str], api_base: Optional[str], api_version: Optional[str], **kwargs) -> SyncCursorPage[Assistant]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 207
async def acreate_assistants(custom_llm_provider: Literal[(?, ?)], client: Optional[AsyncOpenAI], **kwargs) -> Assistant:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 246
def create_assistants(custom_llm_provider: Literal[(?, ?)], model: str, name: Optional[str], description: Optional[str], instructions: Optional[str], tools: Optional[List[Dict[(str, Any)]]], tool_resources: Optional[Dict[(str, Any)]], metadata: Optional[Dict[(str, str)]], temperature: Optional[float], top_p: Optional[float], response_format: Optional[Union[(str, Dict[(str, str)])]], client: Optional[Any], api_key: Optional[str], api_base: Optional[str], api_version: Optional[str], **kwargs) -> Union[(Assistant, Coroutine[(Any, Any, Assistant)])]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 407
async def adelete_assistant(custom_llm_provider: Literal[(?, ?)], client: Optional[AsyncOpenAI], **kwargs) -> AssistantDeleted:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 445
def delete_assistant(custom_llm_provider: Literal[(?, ?)], assistant_id: str, client: Optional[Any], api_key: Optional[str], api_base: Optional[str], api_version: Optional[str], **kwargs) -> Union[(AssistantDeleted, Coroutine[(Any, Any, AssistantDeleted)])]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 588
async def acreate_thread(custom_llm_provider: Literal[(?, ?)], **kwargs) -> Thread:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 623
def create_thread(custom_llm_provider: Literal[(?, ?)], messages: Optional[Iterable[OpenAICreateThreadParamsMessage]], metadata: Optional[dict], tool_resources: Optional[OpenAICreateThreadParamsToolResources], client: Optional[OpenAI], **kwargs) -> Thread:
    """- get the llm provider
    - if openai - route it there
    - pass through relevant params

    ```
    from litellm import create_thread

    create_thread(
        custom_llm_provider="openai",
        ### OPTIONAL ###
        messages =  {
            "role": "user",
            "content": "Hello, what is AI?"
            },
            {
            "role": "user",
            "content": "How does AI work? Explain it in simple terms."
        }]
    )
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 766
async def aget_thread(custom_llm_provider: Literal[(?, ?)], thread_id: str, client: Optional[AsyncOpenAI], **kwargs) -> Thread:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 804
def get_thread(custom_llm_provider: Literal[(?, ?)], thread_id: str, client, **kwargs) -> Thread:
    "Get the thread object, given a thread_id"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 923
async def a_add_message(custom_llm_provider: Literal[(?, ?)], thread_id: str, role: Literal[(?, ?)], content: str, attachments: Optional[List[Attachment]], metadata: Optional[dict], client, **kwargs) -> OpenAIMessage:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 976
def add_message(custom_llm_provider: Literal[(?, ?)], thread_id: str, role: Literal[(?, ?)], content: str, attachments: Optional[List[Attachment]], metadata: Optional[dict], client, **kwargs) -> OpenAIMessage:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 1107
async def aget_messages(custom_llm_provider: Literal[(?, ?)], thread_id: str, client: Optional[AsyncOpenAI], **kwargs) -> AsyncCursorPage[OpenAIMessage]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 1152
def get_messages(custom_llm_provider: Literal[(?, ?)], thread_id: str, client: Optional[Any], **kwargs) -> SyncCursorPage[OpenAIMessage]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 1267
def arun_thread_stream(event_handler: Optional[AssistantEventHandler], **kwargs) -> AsyncAssistantStreamManager[AsyncAssistantEventHandler]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 1276
async def arun_thread(custom_llm_provider: Literal[(?, ?)], thread_id: str, assistant_id: str, additional_instructions: Optional[str], instructions: Optional[str], metadata: Optional[dict], model: Optional[str], stream: Optional[bool], tools: Optional[Iterable[AssistantToolParam]], client: Optional[Any], **kwargs) -> Run:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 1335
def run_thread_stream(event_handler: Optional[AssistantEventHandler], **kwargs) -> AssistantStreamManager[AssistantEventHandler]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/assistants/main.py Line: 1343
def run_thread(custom_llm_provider: Literal[(?, ?)], thread_id: str, assistant_id: str, additional_instructions: Optional[str], instructions: Optional[str], metadata: Optional[dict], model: Optional[str], stream: Optional[bool], tools: Optional[Iterable[AssistantToolParam]], client: Optional[Any], event_handler: Optional[AssistantEventHandler], **kwargs) -> Run:
    "Run a given thread + assistant."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/main.py Line: 45
async def acreate_batch(completion_window: Literal[?], endpoint: Literal[(?, ?, ?)], input_file_id: str, custom_llm_provider: Literal[(?, ?, ?)]="openai", metadata: Optional[Dict[(str, str)]], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Batch:
    """Async: Creates and executes a batch from an uploaded file of request

    LiteLLM Equivalent of POST: https://api.openai.com/v1/batches"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/main.py Line: 93
def create_batch(completion_window: Literal[?], endpoint: Literal[(?, ?, ?)], input_file_id: str, custom_llm_provider: Literal[(?, ?, ?)]="openai", metadata: Optional[Dict[(str, str)]], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Union[(LiteLLMBatch, Coroutine[(Any, Any, LiteLLMBatch)])]:
    """Creates and executes a batch from an uploaded file of request

    LiteLLM Equivalent of POST: https://api.openai.com/v1/batches"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/main.py Line: 267
async def aretrieve_batch(batch_id: str, custom_llm_provider: Literal[(?, ?, ?)]="openai", metadata: Optional[Dict[(str, str)]], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> LiteLLMBatch:
    """Async: Retrieves a batch.

    LiteLLM Equivalent of GET https://api.openai.com/v1/batches/{batch_id}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/main.py Line: 309
def retrieve_batch(batch_id: str, custom_llm_provider: Literal[(?, ?, ?)]="openai", metadata: Optional[Dict[(str, str)]], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Union[(LiteLLMBatch, Coroutine[(Any, Any, LiteLLMBatch)])]:
    """Retrieves a batch.

    LiteLLM Equivalent of GET https://api.openai.com/v1/batches/{batch_id}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/main.py Line: 470
async def alist_batches(after: Optional[str], limit: Optional[int], custom_llm_provider: Literal[(?, ?)]="openai", metadata: Optional[Dict[(str, str)]], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs):
    "Async: List your organization's batches."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/main.py Line: 511
def list_batches(after: Optional[str], limit: Optional[int], custom_llm_provider: Literal[(?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs):
    """Lists batches

    List your organization's batches."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/main.py Line: 628
async def acancel_batch(batch_id: str, custom_llm_provider: Literal[(?, ?)]="openai", metadata: Optional[Dict[(str, str)]], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Batch:
    """Async: Cancels a batch.

    LiteLLM Equivalent of POST https://api.openai.com/v1/batches/{batch_id}/cancel"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/main.py Line: 669
def cancel_batch(batch_id: str, custom_llm_provider: Literal[(?, ?)]="openai", metadata: Optional[Dict[(str, str)]], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Union[(Batch, Coroutine[(Any, Any, Batch)])]:
    """Cancels a batch.

    LiteLLM Equivalent of POST https://api.openai.com/v1/batches/{batch_id}/cancel"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 10
async def _handle_completed_batch(batch: Batch, custom_llm_provider: Literal[(?, ?, ?)]) -> Tuple[(float, Usage, List[str])]:
    "Helper function to process a completed batch and handle logging"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 35
def _get_batch_models_from_file_content(file_content_dictionary: List[dict]) -> List[str]:
    "Get the models from the file content"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 51
async def _batch_cost_calculator(file_content_dictionary: List[dict], custom_llm_provider: Literal[(?, ?, ?)]="openai") -> float:
    "Calculate the cost of a batch based on the output file id"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 68
async def _get_batch_output_file_content_as_dictionary(batch: Batch, custom_llm_provider: Literal[(?, ?, ?)]="openai") -> List[dict]:
    "Get the batch output file content as a list of dictionaries"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 90
def _get_file_content_as_dictionary(file_content: bytes) -> List[dict]:
    "Get the file content as a list of dictionaries from JSON Lines format"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 107
def _get_batch_job_cost_from_file_content(file_content_dictionary: List[dict], custom_llm_provider: Literal[(?, ?, ?)]="openai") -> float:
    "Get the cost of a batch job from the file content"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 135
def _get_batch_job_total_usage_from_file_content(file_content_dictionary: List[dict], custom_llm_provider: Literal[(?, ?, ?)]="openai") -> Usage:
    "Get the tokens of a batch job from the file content"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 159
def _get_batch_job_usage_from_response_body(response_body: dict) -> Usage:
    "Get the tokens of a batch job from the response body"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 168
def _get_response_from_batch_job_output_file(batch_job_output_file: dict) -> Any:
    "Get the response from the batch job output file"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batches/batch_utils.py Line: 177
def _batch_response_was_successful(batch_job_output_file: dict) -> bool:
    "Check if the batch job response status == 200"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/tag_based_routing.py Line: 22
def is_valid_deployment_tag(deployment_tags: List[str], request_tags: List[str]) -> bool:
    "Check if a tag is valid"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/tag_based_routing.py Line: 46
async def get_deployments_for_tag(llm_router_instance: LitellmRouter, model: str, healthy_deployments: Union[(List[Any], Dict[(Any, Any)])], request_kwargs: Optional[Dict[(Any, Any)]]):
    """Returns a list of deployments that match the requested model and tags in the request.

    Executes tag based filtering based on the tags in request metadata and the tags on the deployments"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/tag_based_routing.py Line: 125
def _get_tags_from_request_kwargs(request_kwargs: Optional[Dict[(Any, Any)]]) -> List[str]:
    """Helper to get tags from request kwargs

    Args:
        request_kwargs: The request kwargs to get tags from

    Returns:
        List[str]: The tags from the request kwargs"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/budget_limiter.py Line: 44
class RouterBudgetLimiting(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/lowest_tpm_rpm.py Line: 15
class RoutingArgs(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/lowest_tpm_rpm.py Line: 19
class LowestTPMLoggingHandler(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/base_routing_strategy.py Line: 16
class BaseRoutingStrategy(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/simple_shuffle.py Line: 21
def simple_shuffle(llm_router_instance: LitellmRouter, healthy_deployments: Union[(List[Any], Dict[(Any, Any)])], model: str) -> Dict:
    """Returns a random deployment from the list of healthy deployments.

    If weights are provided, it will return a deployment based on the weights.

    If users pass `rpm` or `tpm`, we do a random weighted pick - based on `rpm`/`tpm`.

    Args:
        llm_router_instance: LitellmRouter instance
        healthy_deployments: List of healthy deployments
        model: Model name

    Returns:
        Dict: A single healthy deployment"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/lowest_latency.py Line: 22
class RoutingArgs(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/lowest_latency.py Line: 28
class LowestLatencyLoggingHandler(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/least_busy.py Line: 16
class LeastBusyLoggingHandler(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/lowest_cost.py Line: 13
class LowestCostLoggingHandler(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/lowest_tpm_rpm_v2.py Line: 28
class RoutingArgs(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_strategy/lowest_tpm_rpm_v2.py Line: 32
class LowestTPMLoggingHandler_v2(BaseRoutingStrategy, CustomLogger):
    """Updated version of TPM/RPM Logging.

    Meant to work across instances.

    Caches individual models, not model_groups

    Uses batch get (redis.mget)

    Increments tpm/rpm limit using redis.incr"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/experimental_mcp_client/tools.py Line: 17
def transform_mcp_tool_to_openai_tool(mcp_tool: MCPTool) -> ChatCompletionToolParam:
    "Convert an MCP tool to an OpenAI tool."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/experimental_mcp_client/tools.py Line: 30
async def load_mcp_tools(session: ClientSession, format: Literal[(?, ?)]="mcp") -> Union[(List[MCPTool], List[ChatCompletionToolParam])]:
    """Load all available MCP tools

    Args:
        session: The MCP session to use
        format: The format to convert the tools to
    By default, the tools are returned in MCP format.

    If format is set to "openai", the tools are converted to OpenAI API compatible tools."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/experimental_mcp_client/tools.py Line: 56
async def call_mcp_tool(session: ClientSession, call_tool_request_params: MCPCallToolRequestParams) -> MCPCallToolResult:
    "Call an MCP tool."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/experimental_mcp_client/tools.py Line: 68
def _get_function_arguments(function: FunctionDefinition) -> dict:
    "Helper to safely get and parse function arguments."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/experimental_mcp_client/tools.py Line: 79
def transform_openai_tool_call_request_to_mcp_tool_call_request(openai_tool: Union[(ChatCompletionMessageToolCall, Dict)]) -> MCPCallToolRequestParams:
    "Convert an OpenAI ChatCompletionMessageToolCall to an MCP CallToolRequestParams."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/experimental_mcp_client/tools.py Line: 90
async def call_openai_tool(session: ClientSession, openai_tool: ChatCompletionMessageToolCall) -> MCPCallToolResult:
    """Call an OpenAI tool using MCP client.

    Args:
        session: The MCP session to use
        openai_tool: The OpenAI tool to call. You can get this from the `choices[0].message.tool_calls[0]` of the response from the OpenAI API.
    Returns:
        The result of the MCP tool call."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/realtime_api/main.py Line: 21
async def _arealtime(model: str, websocket: Any, api_base: Optional[str], api_key: Optional[str], api_version: Optional[str], azure_ad_token: Optional[str], client: Optional[Any], timeout: Optional[float], **kwargs):
    """Private function to handle the realtime API call.

    For PROXY use only."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/realtime_api/main.py Line: 111
async def _realtime_health_check(model: str, custom_llm_provider: str, api_key: Optional[str], api_base: Optional[str], api_version: Optional[str]):
    """Health check for realtime API - tries connection to the realtime API websocket

    Args:
        model: str - model name
        api_base: str - api base
        api_version: Optional[str] - api version
        api_key: str - api key
        custom_llm_provider: str - custom llm provider

    Returns:
        bool - True if connection is successful, False otherwise
    Raises:
        Exception - if the connection is not successful"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/caching.py Line: 38
def print_verbose(print_statement):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/caching.py Line: 47
class CacheMode(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/caching.py Line: 53
class Cache:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/caching.py Line: 671
def enable_cache(type: Optional[LiteLLMCacheType]=..., host: Optional[str], port: Optional[str], password: Optional[str], supported_call_types: Optional[List[CachingSupportedCallTypes]]=['completion', 'acompletion', 'embedding', 'aembedding', 'atranscription', 'transcription', 'atext_completion', 'text_completion', 'arerank', 'rerank'], **kwargs):
    """Enable cache with the specified configuration.

    Args:
        type (Optional[Literal["local", "redis", "s3", "disk"]]): The type of cache to enable. Defaults to "local".
        host (Optional[str]): The host address of the cache server. Defaults to None.
        port (Optional[str]): The port number of the cache server. Defaults to None.
        password (Optional[str]): The password for the cache server. Defaults to None.
        supported_call_types (Optional[List[Literal["completion", "acompletion", "embedding", "aembedding"]]]):
            The supported call types for the cache. Defaults to ["completion", "acompletion", "embedding", "aembedding"].
        **kwargs: Additional keyword arguments.

    Returns:
        None

    Raises:
        None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/caching.py Line: 729
def update_cache(type: Optional[LiteLLMCacheType]=..., host: Optional[str], port: Optional[str], password: Optional[str], supported_call_types: Optional[List[CachingSupportedCallTypes]]=['completion', 'acompletion', 'embedding', 'aembedding', 'atranscription', 'transcription', 'atext_completion', 'text_completion', 'arerank', 'rerank'], **kwargs):
    """Update the cache for LiteLLM.

    Args:
        type (Optional[Literal["local", "redis", "s3", "disk"]]): The type of cache. Defaults to "local".
        host (Optional[str]): The host of the cache. Defaults to None.
        port (Optional[str]): The port of the cache. Defaults to None.
        password (Optional[str]): The password for the cache. Defaults to None.
        supported_call_types (Optional[List[Literal["completion", "acompletion", "embedding", "aembedding"]]]):
            The supported call types for the cache. Defaults to ["completion", "acompletion", "embedding", "aembedding"].
        **kwargs: Additional keyword arguments for the cache.

    Returns:
        None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/caching.py Line: 777
def disable_cache():
    """Disable the cache used by LiteLLM.

    This function disables the cache used by the LiteLLM module. It removes the cache-related callbacks from the input_callback, success_callback, and _async_success_callback lists. It also sets the litellm.cache attribute to None.

    Parameters:
    None

    Returns:
    None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/disk_cache.py Line: 14
class DiskCache(BaseCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/caching_handler.py Line: 60
class CachingHandlerResponse(BaseModel):
    """This is the response object for the caching handler. We need to separate embedding cached responses and (completion / text_completion / transcription) cached responses

    For embeddings there can be a cache hit for some of the inputs in the list and a cache miss for others"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/caching_handler.py Line: 72
class LLMCachingHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/caching_handler.py Line: 888
def convert_args_to_kwargs(original_function: Callable, args: Optional[Tuple[(Any, ?)]]) -> Dict[(str, Any)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/dual_cache.py Line: 34
class LimitedSizeOrderedDict(OrderedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/dual_cache.py Line: 46
class DualCache(BaseCache):
    """DualCache is a cache implementation that updates both Redis and an in-memory cache simultaneously.
    When data is updated or inserted, it is written to both the in-memory cache + Redis.
    This ensures that even if Redis hasn't been updated yet, the in-memory cache reflects the most recent data."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/redis_semantic_cache.py Line: 28
class RedisSemanticCache(BaseCache):
    """Redis-backed semantic cache for LLM responses.

    This cache uses vector similarity to find semantically similar prompts that have been
    previously sent to the LLM, allowing for cache hits even when prompts are not identical
    but carry similar meaning."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/in_memory_cache.py Line: 23
class InMemoryCache(BaseCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/llm_caching_handler.py Line: 10
class LLMClientCache(InMemoryCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/_internal_lru_cache.py Line: 7
def lru_cache_wrapper(maxsize: Optional[int]) -> Callable[(?, Callable[(?, T)])]:
    "Wrapper for lru_cache that caches success and exceptions"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/redis_cache.py Line: 46
class RedisCache(BaseCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/qdrant_semantic_cache.py Line: 24
class QdrantSemanticCache(BaseCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/redis_cluster_cache.py Line: 26
class RedisClusterCache(RedisCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/s3_cache.py Line: 22
class S3Cache(BaseCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/caching/base_cache.py Line: 22
class BaseCache(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/rerank_api/rerank_utils.py Line: 7
def get_optional_rerank_params(rerank_provider_config: BaseRerankConfig, model: str, drop_params: bool, query: str, documents: List[Union[(str, Dict[(str, Any)])]], custom_llm_provider: Optional[str], top_n: Optional[int], rank_fields: Optional[List[str]], return_documents: Optional[bool]=True, max_chunks_per_doc: Optional[int], max_tokens_per_doc: Optional[int], non_default_params: Optional[dict]) -> OptionalRerankParams:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/rerank_api/main.py Line: 28
async def arerank(model: str, query: str, documents: List[Union[(str, Dict[(str, Any)])]], custom_llm_provider: Optional[Literal[(?, ?)]], top_n: Optional[int], rank_fields: Optional[List[str]], return_documents: Optional[bool], max_chunks_per_doc: Optional[int], **kwargs) -> Union[(RerankResponse, Coroutine[(Any, Any, RerankResponse)])]:
    "Async: Reranks a list of documents based on their relevance to the query"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/rerank_api/main.py Line: 73
def rerank(model: str, query: str, documents: List[Union[(str, Dict[(str, Any)])]], custom_llm_provider: Optional[Literal[(?, ?, ?, ?, ?)]], top_n: Optional[int], rank_fields: Optional[List[str]], return_documents: Optional[bool]=True, max_chunks_per_doc: Optional[int], max_tokens_per_doc: Optional[int], **kwargs) -> Union[(RerankResponse, Coroutine[(Any, Any, RerankResponse)])]:
    "Reranks a list of documents based on their relevance to the query"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/maritalk.py Line: 9
class MaritalkError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/maritalk.py Line: 19
class MaritalkConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/baseten.py Line: 9
class BasetenError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/baseten.py Line: 18
def validate_environment(api_key):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/baseten.py Line: 28
def completion(model: str, messages: list, model_response: ModelResponse, print_verbose: Callable, encoding, api_key, logging_obj, optional_params: dict, litellm_params, logger_fn):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/baseten.py Line: 170
def embedding():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_llm.py Line: 22
class CustomLLMError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_llm.py Line: 35
class CustomLLM(BaseLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_llm.py Line: 156
def custom_chat_llm_router(async_fn: bool, stream: Optional[bool], custom_llm: CustomLLM):
    """Routes call to CustomLLM completion/acompletion/streaming/astreaming functions, based on call type

    Validates if response is in expected format"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama_chat.py Line: 23
class OllamaError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama_chat.py Line: 34
class OllamaChatConfig(OpenAIGPTConfig):
    """Reference: https://github.com/ollama/ollama/blob/main/docs/api.md#parameters

    The class `OllamaConfig` provides the configuration for the Ollama's API interface. Below are the parameters:

    - `mirostat` (int): Enable Mirostat sampling for controlling perplexity. Default is 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0. Example usage: mirostat 0

    - `mirostat_eta` (float): Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. Default: 0.1. Example usage: mirostat_eta 0.1

    - `mirostat_tau` (float): Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. Default: 5.0. Example usage: mirostat_tau 5.0

    - `num_ctx` (int): Sets the size of the context window used to generate the next token. Default: 2048. Example usage: num_ctx 4096

    - `num_gqa` (int): The number of GQA groups in the transformer layer. Required for some models, for example it is 8 for llama2:70b. Example usage: num_gqa 1

    - `num_gpu` (int): The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Example usage: num_gpu 0

    - `num_thread` (int): Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Example usage: num_thread 8

    - `repeat_last_n` (int): Sets how far back for the model to look back to prevent repetition. Default: 64, 0 = disabled, -1 = num_ctx. Example usage: repeat_last_n 64

    - `repeat_penalty` (float): Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. Default: 1.1. Example usage: repeat_penalty 1.1

    - `temperature` (float): The temperature of the model. Increasing the temperature will make the model answer more creatively. Default: 0.8. Example usage: temperature 0.7

    - `seed` (int): Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. Example usage: seed 42

    - `stop` (string[]): Sets the stop sequences to use. Example usage: stop "AI assistant:"

    - `tfs_z` (float): Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. Default: 1. Example usage: tfs_z 1

    - `num_predict` (int): Maximum number of tokens to predict when generating text. Default: 128, -1 = infinite generation, -2 = fill context. Example usage: num_predict 42

    - `top_k` (int): Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. Default: 40. Example usage: top_k 40

    - `top_p` (float): Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. Default: 0.9. Example usage: top_p 0.9

    - `system` (string): system prompt for model (overrides what is defined in the Modelfile)

    - `template` (string): the full prompt or prompt template (overrides what is defined in the Modelfile)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama_chat.py Line: 202
def get_ollama_response(model_response: ModelResponse, messages: list, optional_params: dict, model: str, logging_obj: Any, api_base="http://localhost:11434", api_key: Optional[str], acompletion: bool, encoding, client: Optional[Union[(HTTPHandler, AsyncHTTPHandler)]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama_chat.py Line: 376
def ollama_completion_stream(url, api_key, data, logging_obj):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama_chat.py Line: 439
async def ollama_async_streaming(url, api_key, data, model_response, encoding, logging_obj):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama_chat.py Line: 523
async def ollama_acompletion(url, api_key: Optional[str], data, model_response: ?, encoding, logging_obj, function_name):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/volcengine.py Line: 6
class VolcEngineConfig(OpenAILikeChatConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base.py Line: 11
class BaseLLM:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/google_kms.py Line: 18
def validate_environment():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/google_kms.py Line: 29
def load_google_kms(use_google_kms: Optional[bool]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/get_azure_ad_token_provider.py Line: 5
def get_azure_ad_token_provider() -> Callable[(?, str)]:
    """Get Azure AD token provider based on Service Principal with Secret workflow.

    Based on: https://github.com/openai/openai-python/blob/main/examples/azure_ad.py
    See Also:
        https://learn.microsoft.com/en-us/python/api/overview/azure/identity-readme?view=azure-python#service-principal-with-secret;
        https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.clientsecretcredential?view=azure-python.

    Returns:
        Callable that returns a temporary authentication token."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/google_secret_manager.py Line: 14
class GoogleSecretManager(GCSBucketBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/hashicorp_secret_manager.py Line: 20
class HashicorpSecretManager(BaseSecretManager):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/main.py Line: 22
def _is_base64(s):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/main.py Line: 29
def str_to_bool(value: Optional[str]) -> Optional[bool]:
    """Converts a string to a boolean if it's a recognized boolean string.
    Returns None if the string is not a recognized boolean value.

    :param value: The string to be checked.
    :return: True or False if the string is a recognized boolean, otherwise None."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/main.py Line: 53
def get_secret_str(secret_name: str, default_value: Optional[Union[(str, bool)]]) -> Optional[str]:
    "Guarantees response from 'get_secret' is either string or none. Used for fixing linting errors."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/main.py Line: 67
def get_secret_bool(secret_name: str, default_value: Optional[bool]) -> Optional[bool]:
    """Guarantees response from 'get_secret' is either boolean or none. Used for fixing linting errors.

    Args:
        secret_name: The name of the secret to get.
        default_value: The default value to return if the secret is not found.

    Returns:
        The secret value as a boolean or None if the secret is not found."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/main.py Line: 90
def get_secret(secret_name: str, default_value: Optional[Union[(str, bool)]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/main.py Line: 339
def _should_read_secret_from_secret_manager() -> bool:
    """Returns True if the secret manager should be used to read the secret, False otherwise

    - If the secret manager client is not set, return False
    - If the `_key_management_settings` access mode is "read_only" or "read_and_write", return True
    - Otherwise, return False"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/aws_secret_manager.py Line: 21
def validate_environment():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/aws_secret_manager.py Line: 26
def load_aws_kms(use_aws_kms: Optional[bool]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/aws_secret_manager.py Line: 44
class AWSKeyManagementService_V2:
    "V2 Clean Class for decrypting keys from AWS KeyManagementService"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/aws_secret_manager.py Line: 127
def decrypt_env_var() -> Dict[(str, Any)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/aws_secret_manager_v2.py Line: 35
class AWSSecretsManagerV2(BaseAWSLLM, BaseSecretManager):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/secret_managers/base_secret_manager.py Line: 9
class BaseSecretManager(ABC):
    "Abstract base class for secret management implementations."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/caching.py Line: 7
class LiteLLMCacheType(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/caching.py Line: 30
class RedisPipelineIncrementOperation(TypedDict):
    "TypeDict for 1 Redis Pipeline Increment Operation"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/caching.py Line: 58
class CachePingResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/caching.py Line: 69
class HealthCheckCacheParams(BaseModel):
    "Cache Params returned on /cache/ping call"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 7
class ChatCompletionSystemMessageParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 22
class ChatCompletionContentPartTextParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 30
class ImageURL(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 42
class ChatCompletionContentPartImageParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 54
class ChatCompletionUserMessageParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 69
class FunctionCall(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 82
class Function(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 95
class ChatCompletionToolMessageParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 106
class ChatCompletionFunctionMessageParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 117
class ChatCompletionMessageToolCallParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 128
class ChatCompletionAssistantMessageParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/completion.py Line: 165
class CompletionRequest(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 45
def _generate_id():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 49
class LiteLLMPydanticObjectBase(BaseModel):
    "Implements default functions, all pydantic objects should have."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 71
class LiteLLMCommonStrings(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 79
class CostPerToken(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 84
class ProviderField(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 91
class ProviderSpecificModelInfo(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 109
class SearchContextCostPerQuery(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 115
class ModelInfoBase(ProviderSpecificModelInfo, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 172
class ModelInfo(ModelInfoBase, total=...):
    "Model info for a given model, this is information found in litellm.model_prices_and_context_window.json"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 180
class GenericStreamingChunk(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 195
class CallTypes(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 286
class PassthroughCallTypes(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 290
class TopLogprob(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 311
class ChatCompletionTokenLogprob(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 352
class ChoiceLogprobs(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 369
class FunctionCall(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 374
class Function(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 423
class ChatCompletionDeltaToolCall(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 430
class HiddenParams(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 457
class ChatCompletionMessageToolCall(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 501
class ChatCompletionAudioResponse(ChatCompletionAudio):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 541
def add_provider_specific_fields(object: BaseModel, provider_specific_fields: Optional[Dict[(str, Any)]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 549
class Message(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 658
class Delta(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 744
class Choices(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 794
class CompletionTokensDetailsWrapper(CompletionTokensDetails):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 801
class PromptTokensDetailsWrapper(PromptTokensDetails):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 829
class Usage(CompletionUsage):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 941
class StreamingChoices(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 992
class StreamingChatCompletionChunk(OpenAIChatCompletionChunk):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1006
class ModelResponseBase(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1031
class ModelResponseStream(ModelResponseBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1103
class ModelResponse(ModelResponseBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1213
class Embedding(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1231
class EmbeddingResponse(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1303
class Logprobs(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1310
class TextChoices(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1354
class TextCompletionResponse(OpenAIObject):
    """{
        "id": response["id"],
        "object": "text_completion",
        "created": response["created"],
        "model": response["model"],
        "choices": [
        {
            "text": response["choices"][0]["message"]["content"],
            "index": response["choices"][0]["index"],
            "logprobs": transformed_logprobs,
            "finish_reason": response["choices"][0]["finish_reason"]
        }
        ],
        "usage": response["usage"]
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1464
class ImageObject(OpenAIImage):
    """Represents the url or the content of an image generated by the OpenAI API.

    Attributes:
    b64_json: The base64-encoded JSON of the generated image, if response_format is b64_json.
    url: The URL of the generated image, if response_format is url (default).
    revised_prompt: The prompt that was used to generate the image, if there was any revision to the prompt.

    https://platform.openai.com/docs/api-reference/images/object"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1510
class ImageResponse(OpenAIImageResponse):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1574
class TranscriptionResponse(OpenAIObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1607
class GenericImageParsingChunk(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1613
class ResponseFormatChunk(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1618
class LoggedLiteLLMParams(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1637
class AdapterCompletionStreamWrapper:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1670
class StandardLoggingUserAPIKeyMetadata(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1681
class StandardLoggingMCPToolCall(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1708
class StandardBuiltInToolsParams(TypedDict, total=...):
    """Standard built-in OpenAItools parameters

    This is used to calculate the cost of built-in tools, insert any standard built-in tools parameters here

    OpenAI charges users based on the `web_search_options` parameter"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1721
class StandardLoggingPromptManagementMetadata(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1727
class StandardLoggingMetadata(StandardLoggingUserAPIKeyMetadata):
    "Specific metadata k,v pairs logged to integration for easier cost tracking and prompt management"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1743
class StandardLoggingAdditionalHeaders(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1750
class StandardLoggingHiddenParams(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1764
class StandardLoggingModelInformation(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1769
class StandardLoggingModelCostFailureDebugInformation(TypedDict, total=...):
    """Debug information, if cost tracking fails.

    Avoid logging sensitive information like response or optional params"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1786
class StandardLoggingPayloadErrorInformation(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1794
class StandardLoggingGuardrailInformation(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1804
class StandardLoggingPayload(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1847
class CustomStreamingDecoder:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1861
class StandardPassThroughResponseObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1875
class StandardCallbackDynamicParams(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 1994
class KeyGenerationConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2000
class TeamUIKeyGenerationConfig(KeyGenerationConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2004
class PersonalUIKeyGenerationConfig(KeyGenerationConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2008
class StandardKeyGenerationConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2013
class BudgetConfig(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2034
class LlmProviders(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2111
class LiteLLMLoggingBaseClass:
    """Base class for logging pre and post call

    Meant to simplify type checking for logging obj."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2127
class CustomHuggingfaceTokenizer(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2133
class LITELLM_IMAGE_VARIATION_PROVIDERS(Enum):
    "Try using an enum for endpoints. This should make it easier to track what provider is supported for what endpoint."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2142
class HttpHandlerRequestFields(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2149
class ProviderSpecificHeader(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2154
class SelectTokenizerResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2159
class LiteLLMBatch(Batch):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2183
class LiteLLMRealtimeStreamLoggingObject(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2208
class RawRequestTypedDict(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2215
class CredentialBase(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2220
class CredentialItem(CredentialBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2224
class CreateCredentialItem(CredentialBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2236
class ExtractedFileData(TypedDict):
    """TypedDict for storing processed file data

    Attributes:
        filename: Name of the file if provided
        content: The file content in bytes
        content_type: MIME type of the file
        headers: Any additional headers for the file"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/utils.py Line: 2253
class SpecialEnums(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/adapter.py Line: 8
class AdapterItem(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 21
class SupportedGuardrailIntegrations(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 31
class Role(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 40
class GuardrailItemSpec(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 48
class GuardrailItem(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 78
class LakeraCategoryThresholds(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 83
class LitellmParams(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 109
class Guardrail(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 115
class guardrailConfig(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 119
class GuardrailEventHooks(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 126
class BedrockTextContent(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 130
class BedrockContentItem(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 134
class BedrockRequest(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 139
class DynamicGuardrailParams(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 143
class GuardrailLiteLLMParamsResponse(BaseModel):
    "The returned LiteLLM Params object for /guardrails/list"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 158
class GuardrailInfoResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/guardrails.py Line: 167
class ListGuardrailsResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/scheduler.py Line: 4
class DefaultPriorities(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/rerank.py Line: 13
class RerankRequest(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/rerank.py Line: 24
class OptionalRerankParams(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/rerank.py Line: 34
class RerankBilledUnits(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/rerank.py Line: 39
class RerankTokens(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/rerank.py Line: 44
class RerankResponseMeta(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/rerank.py Line: 50
class RerankResponseDocument(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/rerank.py Line: 54
class RerankResponseResult(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/rerank.py Line: 60
class RerankResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 26
class ConfigurableClientsideParamsCustomAuth(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 35
class ModelConfig(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 44
class RouterConfig(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 75
class UpdateRouterConfig(BaseModel):
    "Set of params that you can modify via `router.update_settings()`."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 95
class ModelInfo(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 148
class CredentialLiteLLMParams(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 167
class CustomPricingLiteLLMParams(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 175
class GenericLiteLLMParams(CredentialLiteLLMParams, CustomPricingLiteLLMParams):
    "LiteLLM Params without 'model' arg (used across completion / assistants api)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 280
class LiteLLM_Params(GenericLiteLLMParams):
    "LiteLLM Params with 'model' requirement - used for completions"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 343
class updateLiteLLMParams(GenericLiteLLMParams):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 349
class updateDeployment(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 357
class LiteLLMParamsTypedDict(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 404
class DeploymentTypedDict(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 418
class Deployment(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 477
class RouterErrors:
    "Enum for router specific errors with common codes"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 492
class AllowedFailsPolicy(BaseModel):
    """Use this to set a custom number of allowed fails/minute before cooling down a deployment
    If `AuthenticationErrorAllowedFails = 1000`, then 1000 AuthenticationError will be allowed before cooling down a deployment

    Mapping of Exception type to allowed_fails for each exception
    https://docs.litellm.ai/docs/exception_mapping"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 509
class RetryPolicy(BaseModel):
    """Use this to set a custom number of retries per exception type
    If RateLimitErrorRetries = 3, then 3 retries will be made for RateLimitError
    Mapping of Exception type to number of retries
    https://docs.litellm.ai/docs/exception_mapping"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 525
class AlertingConfig(BaseModel):
    """Use this configure alerting for the router. Receive alerts on the following events
    - LLM API Exceptions
    - LLM Responses Too Slow
    - LLM Requests Hanging

    Args:
        webhook_url: str            - webhook url for alerting, slack provides a webhook url to send alerts to
        alerting_threshold: Optional[float] = None - threshold for slow / hanging llm responses (in seconds)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 541
class ModelGroupInfo(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 579
class AssistantsTypedDict(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 584
class FineTuningConfig(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 588
class CustomRoutingStrategyBase:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 638
class RouterGeneralSettings(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 647
class RouterRateLimitErrorBasic(ValueError):
    "Raise a basic error inside helper functions."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 661
class RouterRateLimitError(ValueError):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 677
class RouterModelGroupAliasItem(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 689
class RoutingStrategy:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 698
class RouterCacheEnum:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 703
class GenericBudgetWindowDetails(BaseModel):
    "Details about a provider's budget window"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/router.py Line: 719
class LiteLLM_RouterFileObject(TypedDict, total=...):
    "Tracking the litellm params hash, used for mapping the file id to the right model"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/fine_tuning.py Line: 4
class OpenAIFineTuningHyperparameters(Hyperparameters):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/embedding.py Line: 6
class EmbeddingRequest(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 10
class FileType(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 154
def get_file_extension_from_mime_type(mime_type: str) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 161
def get_file_type_from_extension(extension: str) -> FileType:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 169
def get_file_extension_for_file_type(file_type: FileType) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 173
def get_file_mime_type_for_file_type(file_type: FileType) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 177
def get_file_mime_type_from_extension(extension: str) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 197
def is_image_file_type(file_type):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 216
def is_video_file_type(file_type):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 233
def is_audio_file_type(file_type):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 241
def is_text_file_type(file_type):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/files.py Line: 282
def is_gemini_1_5_accepted_file_type(file_type: FileType) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/tag_management.py Line: 7
class TagBase(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/tag_management.py Line: 14
class TagConfig(TagBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/tag_management.py Line: 20
class TagNewRequest(TagBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/tag_management.py Line: 24
class TagUpdateRequest(TagBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/tag_management.py Line: 28
class TagDeleteRequest(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/tag_management.py Line: 32
class TagInfoRequest(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/tag_management.py Line: 36
class LiteLLM_DailyTagSpendTable(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/services.py Line: 9
class ServiceMetrics:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/services.py Line: 15
class ServiceTypes(str):
    "Enum for litellm + litellm-adjacent services (redis/postgres/etc.)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/services.py Line: 43
class ServiceConfig(TypedDict):
    "Configuration for services and their metrics"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/services.py Line: 97
class ServiceEventMetadata(TypedDict, total=...):
    """The metadata logged during service success/failure

    Add any extra fields you expect to access in the service_success_hook/service_failure_hook"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/services.py Line: 109
class ServiceLoggerPayload(BaseModel):
    "The payload logged during service success/failure"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 49
async def acreate_file(file: FileTypes, purpose: Literal[(?, ?, ?)], custom_llm_provider: Literal[(?, ?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> OpenAIFileObject:
    """Async: Files are used to upload documents that can be used with features like Assistants, Fine-tuning, and Batch API.

    LiteLLM Equivalent of POST: POST https://api.openai.com/v1/files"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 93
def create_file(file: FileTypes, purpose: Literal[(?, ?, ?)], custom_llm_provider: Optional[Literal[(?, ?, ?)]], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Union[(OpenAIFileObject, Coroutine[(Any, Any, OpenAIFileObject)])]:
    """Files are used to upload documents that can be used with features like Assistants, Fine-tuning, and Batch API.

    LiteLLM Equivalent of POST: POST https://api.openai.com/v1/files

    Specify either provider_list or custom_llm_provider."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 269
async def afile_retrieve(file_id: str, custom_llm_provider: Literal[(?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs):
    """Async: Get file contents

    LiteLLM Equivalent of GET https://api.openai.com/v1/files"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 309
def file_retrieve(file_id: str, custom_llm_provider: Literal[(?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> FileObject:
    """Returns the contents of the specified file.

    LiteLLM Equivalent of POST: POST https://api.openai.com/v1/files"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 422
async def afile_delete(file_id: str, custom_llm_provider: Literal[(?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Coroutine[(Any, Any, FileObject)]:
    """Async: Delete file

    LiteLLM Equivalent of DELETE https://api.openai.com/v1/files"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 462
def file_delete(file_id: str, custom_llm_provider: Literal[(?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> FileDeleted:
    """Delete file

    LiteLLM Equivalent of DELETE https://api.openai.com/v1/files"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 576
async def afile_list(custom_llm_provider: Literal[(?, ?)]="openai", purpose: Optional[str], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs):
    """Async: List files

    LiteLLM Equivalent of GET https://api.openai.com/v1/files"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 616
def file_list(custom_llm_provider: Literal[(?, ?)]="openai", purpose: Optional[str], extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs):
    """List files

    LiteLLM Equivalent of GET https://api.openai.com/v1/files"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 727
async def afile_content(file_id: str, custom_llm_provider: Literal[(?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> HttpxBinaryResponseContent:
    """Async: Get file contents

    LiteLLM Equivalent of GET https://api.openai.com/v1/files"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/files/main.py Line: 767
def file_content(file_id: str, custom_llm_provider: Literal[(?, ?)]="openai", extra_headers: Optional[Dict[(str, str)]], extra_body: Optional[Dict[(str, str)]], **kwargs) -> Union[(HttpxBinaryResponseContent, Coroutine[(Any, Any, HttpxBinaryResponseContent)])]:
    """Returns the contents of the specified file.

    LiteLLM Equivalent of POST: POST https://api.openai.com/v1/files"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batch_completion/main.py Line: 11
def batch_completion(model: str, messages: List, functions: Optional[List], function_call: Optional[str], temperature: Optional[float], top_p: Optional[float], n: Optional[int], stream: Optional[bool], stop, max_tokens: Optional[int], presence_penalty: Optional[float], frequency_penalty: Optional[float], logit_bias: Optional[dict], user: Optional[str], deployment_id, request_timeout: Optional[int], timeout: Optional[int]=600, max_workers: Optional[int]=100, **kwargs):
    """Batch litellm.completion function for a given model.

    Args:
        model (str): The model to use for generating completions.
        messages (List, optional): List of messages to use as input for generating completions. Defaults to [].
        functions (List, optional): List of functions to use as input for generating completions. Defaults to [].
        function_call (str, optional): The function call to use as input for generating completions. Defaults to "".
        temperature (float, optional): The temperature parameter for generating completions. Defaults to None.
        top_p (float, optional): The top-p parameter for generating completions. Defaults to None.
        n (int, optional): The number of completions to generate. Defaults to None.
        stream (bool, optional): Whether to stream completions or not. Defaults to None.
        stop (optional): The stop parameter for generating completions. Defaults to None.
        max_tokens (float, optional): The maximum number of tokens to generate. Defaults to None.
        presence_penalty (float, optional): The presence penalty for generating completions. Defaults to None.
        frequency_penalty (float, optional): The frequency penalty for generating completions. Defaults to None.
        logit_bias (dict, optional): The logit bias for generating completions. Defaults to {}.
        user (str, optional): The user string for generating completions. Defaults to "".
        deployment_id (optional): The deployment ID for generating completions. Defaults to None.
        request_timeout (int, optional): The request timeout for generating completions. Defaults to None.
        max_workers (int,optional): The maximum number of threads to use for parallel processing.

    Returns:
        list: A list of completion results."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batch_completion/main.py Line: 129
def batch_completion_models(*args, **kwargs):
    """Send a request to multiple language models concurrently and return the response
    as soon as one of the models responds.

    Args:
        *args: Variable-length positional arguments passed to the completion function.
        **kwargs: Additional keyword arguments:
            - models (str or list of str): The language models to send requests to.
            - Other keyword arguments to be passed to the completion function.

    Returns:
        str or None: The response from one of the language models, or None if no response is received.

    Note:
        This function utilizes a ThreadPoolExecutor to parallelize requests to multiple models.
        It sends requests concurrently and returns the response from the first model that responds."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/batch_completion/main.py Line: 215
def batch_completion_models_all_responses(*args, **kwargs):
    """Send a request to multiple language models concurrently and return a list of responses
    from all models that respond.

    Args:
        *args: Variable-length positional arguments passed to the completion function.
        **kwargs: Additional keyword arguments:
            - models (str or list of str): The language models to send requests to.
            - Other keyword arguments to be passed to the completion function.

    Returns:
        list: A list of responses from the language models that responded.

    Note:
        This function utilizes a ThreadPoolExecutor to parallelize requests to multiple models.
        It sends requests concurrently and collects responses from all models that respond."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/streaming_iterator.py Line: 24
class BaseResponsesAPIStreamingIterator:
    """Base class for streaming iterators that process responses from the Responses API.

    This class contains shared logic for both synchronous and asynchronous iterators."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/streaming_iterator.py Line: 112
class ResponsesAPIStreamingIterator(BaseResponsesAPIStreamingIterator):
    "Async iterator for processing streaming responses from the Responses API."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/streaming_iterator.py Line: 182
class SyncResponsesAPIStreamingIterator(BaseResponsesAPIStreamingIterator):
    "Synchronous iterator for processing streaming responses from the Responses API."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/streaming_iterator.py Line: 251
class MockResponsesAPIStreamingIterator(BaseResponsesAPIStreamingIterator):
    """Mock iteratorfake a stream by slicing the full response text into
    5 char deltas, then emit a completed event.

    Models like o1-pro don't support streaming, so we fake it."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/utils.py Line: 16
class ResponsesAPIRequestUtils:
    "Helper utils for constructing ResponseAPI requests"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/utils.py Line: 180
class ResponseAPILoggingUtils:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/main.py Line: 41
async def aresponses(input: Union[(str, ResponseInputParam)], model: str, include: Optional[List[ResponseIncludable]], instructions: Optional[str], max_output_tokens: Optional[int], metadata: Optional[Dict[(str, Any)]], parallel_tool_calls: Optional[bool], previous_response_id: Optional[str], reasoning: Optional[Reasoning], store: Optional[bool], stream: Optional[bool], temperature: Optional[float], text: Optional[ResponseTextConfigParam], tool_choice: Optional[ToolChoice], tools: Optional[Iterable[ToolParam]], top_p: Optional[float], truncation: Optional[Literal[(?, ?)]], user: Optional[str], extra_headers: Optional[Dict[(str, Any)]], extra_query: Optional[Dict[(str, Any)]], extra_body: Optional[Dict[(str, Any)]], timeout: Optional[Union[(float, ?)]], custom_llm_provider: Optional[str], **kwargs) -> Union[(ResponsesAPIResponse, BaseResponsesAPIStreamingIterator)]:
    "Async: Handles responses API requests by reusing the synchronous function"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/main.py Line: 140
def responses(input: Union[(str, ResponseInputParam)], model: str, include: Optional[List[ResponseIncludable]], instructions: Optional[str], max_output_tokens: Optional[int], metadata: Optional[Dict[(str, Any)]], parallel_tool_calls: Optional[bool], previous_response_id: Optional[str], reasoning: Optional[Reasoning], store: Optional[bool], stream: Optional[bool], temperature: Optional[float], text: Optional[ResponseTextConfigParam], tool_choice: Optional[ToolChoice], tools: Optional[Iterable[ToolParam]], top_p: Optional[float], truncation: Optional[Literal[(?, ?)]], user: Optional[str], extra_headers: Optional[Dict[(str, Any)]], extra_query: Optional[Dict[(str, Any)]], extra_body: Optional[Dict[(str, Any)]], timeout: Optional[Union[(float, ?)]], custom_llm_provider: Optional[str], **kwargs):
    """Synchronous version of the Responses API.
    Uses the synchronous HTTP handler to make requests."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/main.py Line: 281
async def adelete_responses(response_id: str, extra_headers: Optional[Dict[(str, Any)]], extra_query: Optional[Dict[(str, Any)]], extra_body: Optional[Dict[(str, Any)]], timeout: Optional[Union[(float, ?)]], custom_llm_provider: Optional[str], **kwargs) -> DeleteResponseResult:
    """Async version of the DELETE Responses API

    DELETE /v1/responses/{response_id} endpoint in the responses API"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/main.py Line: 346
def delete_responses(response_id: str, extra_headers: Optional[Dict[(str, Any)]], extra_query: Optional[Dict[(str, Any)]], extra_body: Optional[Dict[(str, Any)]], timeout: Optional[Union[(float, ?)]], custom_llm_provider: Optional[str], **kwargs) -> Union[(DeleteResponseResult, Coroutine[(Any, Any, DeleteResponseResult)])]:
    """Synchronous version of the DELETE Responses API

    DELETE /v1/responses/{response_id} endpoint in the responses API"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/literal_ai.py Line: 20
class LiteralAILogger(CustomBatchLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/email_alerting.py Line: 16
async def get_all_team_member_emails(team_id: Optional[str]) -> list:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/email_alerting.py Line: 69
async def send_team_budget_alert(webhook_event: WebhookEvent) -> bool:
    """Send an Email Alert to All Team Members when the Team Budget is crossed
    Returns -> True if sent, False if not."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/s3.py Line: 12
class S3Logger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/s3.py Line: 182
def get_s3_object_key(s3_path: str, team_alias_prefix: str, start_time: datetime, s3_file_name: str) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prometheus.py Line: 32
class PrometheusLogger(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prometheus.py Line: 1824
def prometheus_label_factory(supported_enum_labels: List[str], enum_values: UserAPIKeyLabelValues, tag: Optional[str]) -> dict:
    """Returns a dictionary of label + values for prometheus.

    Ensures end_user param is not sent to prometheus if it is not supported."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prometheus.py Line: 1862
def get_custom_labels_from_metadata(metadata: dict) -> Dict[(str, str)]:
    "Get custom labels from metadata"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/weights_biases.py Line: 179
class WeightsBiasesLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/traceloop.py Line: 6
class TraceloopLogger:
    """WARNING: DEPRECATED
    Use the OpenTelemetry standard integration instead"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/mlflow.py Line: 9
class MlflowLogger(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/custom_batch_logger.py Line: 16
class CustomBatchLogger(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/argilla.py Line: 31
def is_serializable(value):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/argilla.py Line: 41
class ArgillaLogger(CustomBatchLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prompt_management_base.py Line: 8
class PromptManagementClient(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prompt_management_base.py Line: 16
class PromptManagementBase(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/helicone.py Line: 9
class HeliconeLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/humanloop.py Line: 21
class PromptManagementClient(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/humanloop.py Line: 28
class HumanLoopPromptManager(DualCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/humanloop.py Line: 149
class HumanloopLogger(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/openmeter.py Line: 18
def get_utc_datetime():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/openmeter.py Line: 28
class OpenMeterLogger(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prometheus_services.py Line: 20
class PrometheusServicesLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/athina.py Line: 6
class AthinaLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/galileo.py Line: 16
class LLMResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/galileo.py Line: 36
class GalileoObserve(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/supabase.py Line: 12
class Supabase:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/lago.py Line: 21
def get_utc_datetime():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/lago.py Line: 31
class LagoLogger(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/custom_prompt_management.py Line: 12
class CustomPromptManagement(CustomLogger, PromptManagementBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opentelemetry.py Line: 48
class OpenTelemetryConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opentelemetry.py Line: 77
class OpenTelemetry(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/greenscale.py Line: 8
class GreenscaleLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/braintrust_logging.py Line: 29
def get_utc_datetime():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/braintrust_logging.py Line: 39
class BraintrustLogger(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langtrace.py Line: 14
class LangtraceAttributes:
    "This class is used to save trace attributes to Langtrace's spans"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/logfire_logger.py Line: 16
class SpanConfig(NamedTuple):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/logfire_logger.py Line: 21
class LogfireLevel(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/logfire_logger.py Line: 26
class LogfireLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/custom_logger.py Line: 38
class CustomLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langsmith.py Line: 26
def is_serializable(value):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langsmith.py Line: 36
class LangsmithLogger(CustomBatchLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/dynamodb.py Line: 12
class DyanmoDBLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prompt_layer.py Line: 11
class PromptLayerLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/anthropic_cache_control_hook.py Line: 22
class AnthropicCacheControlHook(CustomPromptManagement):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/lunary.py Line: 11
def parse_usage(usage):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/lunary.py Line: 18
def parse_tool_calls(tool_calls):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/lunary.py Line: 37
def parse_messages(input):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/lunary.py Line: 69
class LunaryLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/additional_logging_utils.py Line: 15
class AdditionalLoggingUtils(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/custom_guardrail.py Line: 9
class CustomGuardrail(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/custom_guardrail.py Line: 213
def log_guardrail_information(func):
    """Decorator to add standard logging guardrail information to any function

    Add this decorator to ensure your guardrail response is logged to DataDog, OTEL, s3, GCS etc.

    Logs for:
        - pre_call
        - during_call
        - TODO: log post_call. This is more involved since the logs are sent to DD, s3 before the guardrail is even run"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_callbacks.py Line: 21
async def router_cooldown_event_callback(litellm_router_instance: LitellmRouter, deployment_id: str, exception_status: Union[(str, int)], cooldown_time: float):
    """Callback triggered when a deployment is put into cooldown by litellm

    - Updates deployment state on Prometheus
    - Increments cooldown metric for deployment on Prometheus"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_callbacks.py Line: 85
def _get_prometheus_logger_from_callbacks() -> Optional[PrometheusLogger]:
    "Checks if prometheus is a initalized callback, if yes returns it"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 38
def _is_cooldown_required(litellm_router_instance: LitellmRouter, model_id: str, exception_status: Union[(str, int)], exception_str: Optional[str]) -> bool:
    """A function to determine if a cooldown is required based on the exception status.

    Parameters:
        model_id (str) The id of the model in the model list
        exception_status (Union[str, int]): The status of the exception.

    Returns:
        bool: True if a cooldown is required, False otherwise."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 94
def _should_run_cooldown_logic(litellm_router_instance: LitellmRouter, deployment: Optional[str], exception_status: Union[(str, int)], original_exception: Any) -> bool:
    """Helper that decides if cooldown logic should be run
    Returns False if cooldown logic should not be run

    Does not run cooldown logic when:
    - router.disable_cooldowns is True
    - deployment is None
    - _is_cooldown_required() returns False
    - deployment is in litellm_router_instance.provider_default_deployment_ids
    - exception_status is not one that should be immediately retried (e.g. 401)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 150
def _should_cooldown_deployment(litellm_router_instance: LitellmRouter, deployment: str, exception_status: Union[(str, int)], original_exception: Any) -> bool:
    """Helper that decides if a deployment should be put in cooldown

    Returns True if the deployment should be put in cooldown
    Returns False if the deployment should not be put in cooldown


    Deployment is put in cooldown when:
    - v2 logic (Current):
    cooldown if:
        - got a 429 error from LLM API
        - if %fails/%(successes + fails) > ALLOWED_FAILURE_RATE_PER_MINUTE
        - got 401 Auth error, 404 NotFounder - checked by litellm._should_retry()



    - v1 logic (Legacy): if allowed fails or allowed fail policy set, coolsdown if num fails in this minute > allowed fails"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 242
def _set_cooldown_deployments(litellm_router_instance: LitellmRouter, original_exception: Any, exception_status: Union[(str, int)], deployment: Optional[str], time_to_cooldown: Optional[float]) -> bool:
    """Add a model to the list of models being cooled down for that minute, if it exceeds the allowed fails / minute

    or

    the exception is not one that should be immediately retried (e.g. 401)

    Returns:
    - True if the deployment should be put in cooldown
    - False if the deployment should not be put in cooldown"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 302
async def _async_get_cooldown_deployments(litellm_router_instance: LitellmRouter, parent_otel_span: Optional[Span]) -> List[str]:
    "Async implementation of '_get_cooldown_deployments'"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 330
async def _async_get_cooldown_deployments_with_debug_info(litellm_router_instance: LitellmRouter, parent_otel_span: Optional[Span]) -> List[tuple]:
    "Async implementation of '_get_cooldown_deployments'"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 348
def _get_cooldown_deployments(litellm_router_instance: LitellmRouter, parent_otel_span: Optional[Span]) -> List[str]:
    "Get the list of models being cooled down for this minute"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 377
def should_cooldown_based_on_allowed_fails_policy(litellm_router_instance: LitellmRouter, deployment: str, original_exception: Any) -> bool:
    """Check if fails are within the allowed limit and update the number of fails.

    Returns:
    - True if fails exceed the allowed limit (should cooldown)
    - False if fails are within the allowed limit (should not cooldown)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 412
def _is_allowed_fails_set_on_router(litellm_router_instance: LitellmRouter) -> bool:
    """Check if Router.allowed_fails is set or is Non-default Value

    Returns:
    - True if Router.allowed_fails is set or is Non-default Value
    - False if Router.allowed_fails is None or is Default Value"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_handlers.py Line: 429
def cast_exception_status_to_int(exception_status: Union[(str, int)]) -> int:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/handle_error.py Line: 23
async def send_llm_exception_alert(litellm_router_instance: LitellmRouter, request_kwargs: dict, error_traceback_str: str, original_exception):
    """Only runs if router.slack_alerting_logger is set
    Sends a Slack / MS Teams alert for the LLM API call failure. Only if router.slack_alerting_logger is set.

    Parameters:
        litellm_router_instance (_Router): The LitellmRouter instance.
        original_exception (Any): The original exception that occurred.

    Returns:
        None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/handle_error.py Line: 68
async def async_raise_no_deployment_exception(litellm_router_instance: LitellmRouter, model: str, parent_otel_span: Optional[Span]):
    "Raises a RouterRateLimitError if no deployment is found for the given model."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/fallback_event_handlers.py Line: 20
def _check_stripped_model_group(model_group: str, fallback_key: str) -> bool:
    """Handles wildcard routing scenario

    where fallbacks set like:
    [{"gpt-3.5-turbo": ["claude-3-haiku"]}]

    but model_group is like:
    "openai/gpt-3.5-turbo"

    Returns:
    - True if the stripped model group == fallback_key"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/fallback_event_handlers.py Line: 45
def get_fallback_model_group(fallbacks: List[Any], model_group: str) -> Tuple[(Optional[List[str]], Optional[int])]:
    """Returns:
    - fallback_model_group: List[str] of fallback model groups. example: ["gpt-4", "gpt-3.5-turbo"]
    - generic_fallback_idx: int of the index of the generic fallback in the fallbacks list.

    Checks:
    - exact match
    - stripped model group match
    - generic fallback"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/fallback_event_handlers.py Line: 85
async def run_async_fallback(litellm_router: LitellmRouter, fallback_model_group: List[str], original_model_group: str, original_exception: Exception, max_fallbacks: int, fallback_depth: int, *args, **kwargs) -> Any:
    """Loops through all the fallback model groups and calls kwargs["original_function"] with the arguments and keyword arguments provided.

    If the call is successful, it logs the success and returns the response.
    If the call fails, it logs the failure and continues to the next fallback model group.
    If all fallback model groups fail, it raises the most recent exception.

    Args:
        litellm_router: The litellm router instance.
        *args: Positional arguments.
        fallback_model_group: List[str] of fallback model groups. example: ["gpt-4", "gpt-3.5-turbo"]
        original_model_group: The original model group. example: "gpt-3.5-turbo"
        original_exception: The original exception.
        **kwargs: Keyword arguments.

    Returns:
        The response from the successful fallback model group.
    Raises:
        The most recent exception if all fallback model groups fail."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/fallback_event_handlers.py Line: 164
async def log_success_fallback_event(original_model_group: str, kwargs: dict, original_exception: Exception):
    """Log a successful fallback event to all registered callbacks.

    This function iterates through all callbacks, initializing _known_custom_logger_compatible_callbacks  if needed,
    and calls the log_success_fallback_event method on CustomLogger instances.

    Args:
        original_model_group (str): The original model group before fallback.
        kwargs (dict): kwargs for the request

    Note:
        Errors during logging are caught and reported but do not interrupt the process."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/fallback_event_handlers.py Line: 221
async def log_failure_fallback_event(original_model_group: str, kwargs: dict, original_exception: Exception):
    """Log a failed fallback event to all registered callbacks.

    This function iterates through all callbacks, initializing _known_custom_logger_compatible_callbacks if needed,
    and calls the log_failure_fallback_event method on CustomLogger instances.

    Args:
        original_model_group (str): The original model group before fallback.
        kwargs (dict): kwargs for the request

    Note:
        Errors during logging are caught and reported but do not interrupt the process."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/fallback_event_handlers.py Line: 278
def _check_non_standard_fallback_format(fallbacks: Optional[List[Any]]) -> bool:
    """Checks if the fallbacks list is a list of strings or a list of dictionaries.

    If
    - List[str]: e.g. ["claude-3-haiku", "openai/o-1"]
    - List[Dict[<LiteLLMParamsTypedDict>, Any]]: e.g. [{"model": "claude-3-haiku", "messages": [{"role": "user", "content": "Hey, how's it going?"}]}]

    If [{"gpt-3.5-turbo": ["claude-3-haiku"]}] then standard format."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/fallback_event_handlers.py Line: 300
def run_non_standard_fallback_format(fallbacks: Union[(List[str], List[Dict[(str, Any)]])], model_group: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/batch_utils.py Line: 6
class InMemoryFile:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/batch_utils.py Line: 12
def replace_model_in_jsonl(file_content: Union[(bytes, Tuple[(str, bytes, str)])], new_model_name: str) -> Optional[InMemoryFile]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/batch_utils.py Line: 51
def _get_router_metadata_variable_name(function_name) -> str:
    """Helper to return what the "metadata" field should be called in the request data

    For all /thread or /assistant endpoints we need to call this "litellm_metadata"

    For ALL other endpoints we call this "metadata"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_cache.py Line: 20
class CooldownCacheValue(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/cooldown_cache.py Line: 27
class CooldownCache:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/prompt_caching_cache.py Line: 25
class PromptCachingCacheValue(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/prompt_caching_cache.py Line: 29
class PromptCachingCache:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/get_retry_from_policy.py Line: 19
def get_num_retries_from_retry_policy(exception: Exception, retry_policy: Optional[Union[(RetryPolicy, dict)]], model_group: Optional[str], model_group_retry_policy: Optional[Dict[(str, RetryPolicy)]]):
    """BadRequestErrorRetries: Optional[int] = None
    AuthenticationErrorRetries: Optional[int] = None
    TimeoutErrorRetries: Optional[int] = None
    RateLimitErrorRetries: Optional[int] = None
    ContentPolicyViolationErrorRetries: Optional[int] = None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/get_retry_from_policy.py Line: 70
def reset_retry_policy() -> RetryPolicy:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/client_initalization_utils.py Line: 14
class InitalizeCachedClient:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/add_retry_fallback_headers.py Line: 8
def _add_headers_to_response(response: Any, headers: dict) -> Any:
    "Helper function to add headers to a response's hidden params"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/add_retry_fallback_headers.py Line: 31
def add_retry_headers_to_response(response: Any, attempted_retries: int, max_retries: Optional[int]) -> Any:
    "Add retry headers to the request"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/add_retry_fallback_headers.py Line: 48
def add_fallback_headers_to_response(response: Any, attempted_fallbacks: int) -> Any:
    """Add fallback headers to the response

    Args:
        response: The response to add the headers to
        attempted_fallbacks: The number of fallbacks attempted

    Returns:
        The response with the headers added

    Note: It's intentional that we don't add max_fallbacks in response headers
    Want to avoid bloat in the response headers for performance."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/clientside_credential_handler.py Line: 17
def is_clientside_credential(request_kwargs: dict) -> bool:
    "Check if the credential is a clientside credential."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/clientside_credential_handler.py Line: 24
def get_dynamic_litellm_params(litellm_params: dict, request_kwargs: dict) -> dict:
    """Generate a unique model_id for the deployment.

    Returns
    - litellm_params: dict

    for generating a unique model_id."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/pattern_match_deployments.py Line: 14
class PatternUtils:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/pattern_match_deployments.py Line: 52
class PatternMatchRouter:
    """Class to handle llm wildcard routing and regex pattern matching

    doc: https://docs.litellm.ai/docs/proxy/configs#provider-specific-wildcard-routing

    This class will store a mapping for regex pattern: List[Deployments]"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/common_utils.py Line: 7
def get_litellm_params_sensitive_credential_hash(litellm_params: dict) -> str:
    "Hash of the credential params, used for mapping the file id to the right model"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/mcp_tools.py Line: 4
def get_current_time(params: Optional[Dict[(str, Any)]]) -> str:
    """Get the current time (hardcoded sample implementation)

    Args:
        params: Optional dictionary with parameters
            - format: The format of the time to return (e.g., "short")

    Returns:
        A string representing the current time"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/mcp_tools.py Line: 21
def get_current_date(params: Optional[Dict[(str, Any)]]) -> str:
    """Get the current date (hardcoded sample implementation)

    Args:
        params: Optional dictionary with parameters
            - format: The format of the date to return (e.g., "short")

    Returns:
        A string representing the current date"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_logging.py Line: 15
class JsonFormatter(Formatter):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/post_call_rules.py Line: 1
def post_response_rule(input):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/route_llm_request.py Line: 28
class ProxyModelNotFoundError(HTTPException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/route_llm_request.py Line: 36
async def route_request(data: dict, llm_router: Optional[LitellmRouter], user_model: Optional[str], route_type: Literal[(?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]):
    "Common helper to route the request"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 98
def print_verbose(print_statement):
    """Prints the given `print_statement` to the console if `litellm.set_verbose` is True.
    Also logs the `print_statement` at the debug level using `verbose_proxy_logger`.

    :param print_statement: The statement to be printed and logged.
    :type print_statement: Any"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 113
def safe_deep_copy(data):
    """Safe Deep Copy

    The LiteLLM Request has some object that can-not be pickled / deep copied

    Use this function to safely deep copy the LiteLLM Request"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 140
class InternalUsageCache:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 244
class ProxyLogging:
    """Logging/Custom Handlers for proxy.

    Implemented mainly to:
    - log successful/failed db read/writes
    - support the max parallel request integration"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 1126
def on_backoff(details):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 1131
def jsonify_object(data: dict) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 1144
class PrismaClient:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2344
async def _cache_user_row(user_id: str, cache: DualCache, db: PrismaClient):
    """Check if a user_id exists in cache,
    if not retrieve it."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2365
async def send_email(receiver_email, subject, html):
    """smtp_host,
    smtp_port,
    smtp_username,
    smtp_password,
    sender_name,
    sender_email,"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2416
def hash_token(token: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2425
def _hash_token_if_needed(token: str) -> str:
    """Hash the token if it's a string and starts with "sk-"

    Else return the token as is"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2437
class ProxyUpdateSpend:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2568
async def update_spend(prisma_client: PrismaClient, db_writer_client: Optional[HTTPHandler], proxy_logging_obj: ProxyLogging):
    """Batch write updates to db.

    Triggered every minute.

    Requires:
    user_id_list: dict,
    keys_list: list,
    team_list: list,
    spend_logs: list,"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2605
def _raise_failed_update_spend_exception(e: Exception, start_time: float, proxy_logging_obj: ProxyLogging):
    """Raise an exception for failed update spend logs

    - Calls proxy_logging_obj.failure_handler to log the error
    - Ensures error messages says "Non-Blocking""""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2633
def _is_projected_spend_over_limit(current_spend: float, soft_budget_limit: Optional[float]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2667
def _get_projected_spend_over_limit(current_spend: float, soft_budget_limit: Optional[float]) -> Optional[tuple]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2696
def _is_valid_team_configs(team_id, team_config, request_data):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2710
def _to_ns(dt):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2714
def get_error_message_str(e: Exception) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2734
def _get_redoc_url() -> str:
    """Get the redoc URL from the environment variables.

    - If REDOC_URL is set, return it.
    - Otherwise, default to "/redoc"."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2744
def _get_docs_url() -> Optional[str]:
    """Get the docs URL from the environment variables.

    - If DOCS_URL is set, return it.
    - If NO_DOCS is True, return None.
    - Otherwise, default to "/"."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2763
def handle_exception_on_proxy(e: Exception) -> ProxyException:
    "Returns an Exception as ProxyException, this ensures all exceptions are OpenAI API compatible"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/utils.py Line: 2786
def _premium_user_check():
    "Raises an HTTPException if the user is not a premium user"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/custom_sso.py Line: 21
async def custom_sso_handler(userIDPInfo: OpenID) -> SSOUserDefinedValues:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/caching_routes.py Line: 22
def _extract_cache_params() -> Dict[(str, Any)]:
    """Safely extracts and cleans cache parameters.

    The health check UI needs to display specific cache parameters, to show users how they set up their cache.

    eg.
        {
            "host": "localhost",
            "port": 6379,
            "redis_kwargs": {"db": 0},
            "namespace": "test",
        }

    Returns:
        Dict containing cleaned and masked cache parameters"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/caching_routes.py Line: 57
async def cache_ping():
    "Endpoint for checking if cache can be pinged"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/caching_routes.py Line: 122
async def cache_delete(request: Request):
    """Endpoint for deleting a key from the cache. All responses from litellm proxy have `x-litellm-cache-key` in the headers

    Parameters:
    - **keys**: *Optional[List[str]]* - A list of keys to delete from the cache. Example {"keys": ["key1", "key2"]}

    ```shell
    curl -X POST "http://0.0.0.0:4000/cache/delete"     -H "Authorization: Bearer sk-1234"     -d '{"keys": ["key1", "key2"]}'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/caching_routes.py Line: 166
async def cache_redis_info():
    "Endpoint for getting /redis/info"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/caching_routes.py Line: 203
async def cache_flushall():
    """A function to flush all items from the cache. (All items will be deleted from the cache with this)
    Raises HTTPException if the cache is not initialized or if the cache type does not support flushing.
    Returns a dictionary with the status of the operation.

    Usage:
    ```
    curl -X POST http://0.0.0.0:4000/cache/flushall -H "Authorization: Bearer sk-1234"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_request_processing.py Line: 33
class ProxyBaseLLMRequestProcessing:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 41
def parse_cache_control(cache_control):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 55
def _get_metadata_variable_name(request: Request) -> str:
    """Helper to return what the "metadata" field should be called in the request data

    For all /thread or /assistant endpoints we need to call this "litellm_metadata"

    For ALL other endpoints we call this "metadata"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 82
def safe_add_api_version_from_query_params(data: dict, request: Request):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 96
def convert_key_logging_metadata_to_callback(data: AddTeamCallback, team_callback_settings_obj: Optional[TeamCallbackMetadata]) -> TeamCallbackMetadata:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 142
def _get_dynamic_logging_metadata(user_api_key_dict: UserAPIKeyAuth, proxy_config: ProxyConfig) -> Optional[TeamCallbackMetadata]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 183
def clean_headers(headers: Headers, litellm_key_header_name: Optional[str]) -> dict:
    "Removes litellm api key from headers"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 201
class LiteLLMProxyRequestSetup:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 450
async def add_litellm_data_to_request(data: dict, request: Request, user_api_key_dict: UserAPIKeyAuth, proxy_config: ProxyConfig, general_settings: Optional[Dict[(str, Any)]], version: Optional[str]):
    """Adds LiteLLM-specific data to the request.

    Args:
        data (dict): The data dictionary to be modified.
        request (Request): The incoming request.
        user_api_key_dict (UserAPIKeyAuth): The user API key dictionary.
        general_settings (Optional[Dict[str, Any]], optional): General settings. Defaults to None.
        version (Optional[str], optional): Version. Defaults to None.

    Returns:
        dict: The modified data dictionary."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 718
def _update_model_if_team_alias_exists(data: dict, user_api_key_dict: UserAPIKeyAuth) -> ?:
    """Update the model if the team alias exists

    If a alias map has been set on a team, then we want to make the request with the model the team alias is pointing to

    eg.
        - user calls `gpt-4o`
        - team.model_alias_map = {
            "gpt-4o": "gpt-4o-team-1"
        }
        - requested_model = "gpt-4o-team-1""""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 744
def _get_enforced_params(general_settings: Optional[dict], user_api_key_dict: UserAPIKeyAuth) -> Optional[list]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 766
def check_if_token_is_service_account(valid_token: UserAPIKeyAuth) -> bool:
    """Checks if the token is a service account

    Returns:
        bool: True if token is a service account"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 780
def _enforced_params_check(request_body: dict, general_settings: Optional[dict], user_api_key_dict: UserAPIKeyAuth, premium_user: bool) -> bool:
    "If enforced params are set, check if the request body contains the enforced params."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 819
def _add_guardrails_from_key_or_team_metadata(key_metadata: Optional[dict], team_metadata: Optional[dict], data: dict, metadata_variable_name: str) -> ?:
    """Helper add guardrails from key or team metadata to request data

    Args:
        key_metadata: The key metadata dictionary to check for guardrails
        team_metadata: The team metadata dictionary to check for guardrails
        data: The request data to update
        metadata_variable_name: The name of the metadata field in data"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 847
def move_guardrails_to_metadata(data: dict, _metadata_variable_name: str, user_api_key_dict: UserAPIKeyAuth):
    """Helper to add guardrails from request to metadata

    - If guardrails set on API Key metadata then sets guardrails on request metadata
    - If guardrails not set on API key, then checks request metadata"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 876
def add_provider_specific_headers_to_request(data: dict, headers: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/litellm_pre_call_utils.py Line: 898
def _add_otel_traceparent_to_data(data: dict, request: Request):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 49
def showwarning(message, category, filename, lineno, file, line):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 89
def generate_feedback_box():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 428
def cleanup_router_config_variables():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 444
async def proxy_shutdown_event():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 476
async def proxy_startup_event(app: FastAPI):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 612
def get_openapi_schema():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 663
def custom_openapi():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 683
class UserAPIKeyCacheTTLEnum:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 688
async def openai_exception_handler(request: Request, exc: ProxyException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 764
def swagger_monkey_patch(*args, **kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 844
async def check_request_disconnection(request: Request, llm_api_call_task):
    """Asynchronously checks if the request is disconnected at regular intervals.
    If the request is disconnected
    - cancel the litellm.router task
    - raises an HTTPException with status code 499 and detail "Client disconnected the request".

    Parameters:
    - request: Request: The request object to check for disconnection.
    Returns:
    - None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 872
def _resolve_typed_dict_type(typ):
    "Resolve the actual TypedDict class from a potentially wrapped type."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 886
def _resolve_pydantic_type(typ) -> List:
    "Resolve the actual TypedDict class from a potentially wrapped type."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 903
def load_from_azure_key_vault(use_azure_key_vault: bool):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 934
def cost_tracking():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 940
async def update_cache(token: Optional[str], user_id: Optional[str], end_user_id: Optional[str], team_id: Optional[str], response_cost: Optional[float], parent_otel_span: Optional[Span]):
    """Use this to update the cache with new user spend.

    Put any alerting logic in here."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 1190
def run_ollama_serve():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 1204
async def _run_background_health_check():
    """Periodically run health checks in the background on the endpoints.

    Update health_check_results, based on this."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 1235
class StreamingCallbackError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 1239
class ProxyConfig:
    "Abstraction class on top of config loading/updating logic. Gives us one place to control all config updating logic."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 2712
def save_worker_config(**data):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 2718
async def initialize(model, alias, api_base, api_version, debug, detailed_debug, temperature, max_tokens, request_timeout=600, max_budget, telemetry, drop_params=True, add_function_to_prompt=True, headers, save, use_queue, config):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 2837
def data_generator(response):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 2847
async def async_assistants_data_generator(response, user_api_key_dict: UserAPIKeyAuth, request_data: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 2900
async def async_data_generator(response, user_api_key_dict: UserAPIKeyAuth, request_data: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 2962
def select_data_generator(response, user_api_key_dict: UserAPIKeyAuth, request_data: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 2972
def get_litellm_model_info(model: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 2986
def on_backoff(details):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 2991
def giveup(e):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 3010
class ProxyStartupEvent:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 3261
async def model_list(user_api_key_dict: UserAPIKeyAuth=..., return_wildcard_routes: Optional[bool], team_id: Optional[str]):
    """Use `/model/info` - to get detailed model information, example - pricing, mode, etc.

    This is just for compatibility with openai projects like aider."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 3351
async def chat_completion(request: Request, fastapi_response: Response, model: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Follows the exact same API spec as `OpenAI's Chat API https://platform.openai.com/docs/api-reference/chat`

    ```bash
    curl -X POST http://localhost:4000/v1/chat/completions 
    -H "Content-Type: application/json" 
    -H "Authorization: Bearer sk-1234" 
    -d '{
        "model": "gpt-4o",
        "messages": [
            {
                "role": "user",
                "content": "Hello!"
            }
        ]
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 3460
async def completion(request: Request, fastapi_response: Response, model: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Follows the exact same API spec as `OpenAI's Completions API https://platform.openai.com/docs/api-reference/completions`

    ```bash
    curl -X POST http://localhost:4000/v1/completions 
    -H "Content-Type: application/json" 
    -H "Authorization: Bearer sk-1234" 
    -d '{
        "model": "gpt-3.5-turbo-instruct",
        "prompt": "Once upon a time",
        "max_tokens": 50,
        "temperature": 0.7
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 3681
async def embeddings(request: Request, fastapi_response: Response, model: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Follows the exact same API spec as `OpenAI's Embeddings API https://platform.openai.com/docs/api-reference/embeddings`

    ```bash
    curl -X POST http://localhost:4000/v1/embeddings 
    -H "Content-Type: application/json" 
    -H "Authorization: Bearer sk-1234" 
    -d '{
        "model": "text-embedding-ada-002",
        "input": "The quick brown fox jumps over the lazy dog"
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 3879
async def image_generation(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 3997
async def audio_speech(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Same params as:

    https://platform.openai.com/docs/api-reference/audio/createSpeech"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 4108
async def audio_transcriptions(request: Request, fastapi_response: Response, file: UploadFile=..., user_api_key_dict: UserAPIKeyAuth=...):
    """Same params as:

    https://platform.openai.com/docs/api-reference/audio/createTranscription?lang=curl"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 4261
async def websocket_endpoint(websocket: WebSocket, model: str, user_api_key_dict=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 4350
async def get_assistants(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Returns a list of assistants.

    API Reference docs - https://platform.openai.com/docs/api-reference/assistants/listAssistants"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 4448
async def create_assistant(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Create assistant

    API Reference docs - https://platform.openai.com/docs/api-reference/assistants/createAssistant"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 4546
async def delete_assistant(request: Request, assistant_id: str, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Delete assistant

    API Reference docs - https://platform.openai.com/docs/api-reference/assistants/createAssistant"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 4643
async def create_threads(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Create a thread.

    API Reference - https://platform.openai.com/docs/api-reference/threads/createThread"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 4740
async def get_thread(request: Request, thread_id: str, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Retrieves a thread.

    API Reference - https://platform.openai.com/docs/api-reference/threads/getThread"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 4835
async def add_messages(request: Request, thread_id: str, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Create a message.

    API Reference - https://platform.openai.com/docs/api-reference/messages/createMessage"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 4934
async def get_messages(request: Request, thread_id: str, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Returns a list of messages for a given thread.

    API Reference - https://platform.openai.com/docs/api-reference/messages/listMessages"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5029
async def run_thread(request: Request, thread_id: str, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Create a run.

    API Reference: https://platform.openai.com/docs/api-reference/runs/createRun"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5140
async def moderations(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """The moderations endpoint is a tool you can use to check whether content complies with an LLM Providers policies.

    Quick Start
    ```
    curl --location 'http://0.0.0.0:4000/moderations'     --header 'Content-Type: application/json'     --header 'Authorization: Bearer sk-1234'     --data '{"input": "Sample text goes here", "model": "text-moderation-stable"}'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5266
async def token_counter(request: TokenCountRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5329
async def supported_openai_params(model: str):
    """Returns supported openai params for a given litellm model name

    e.g. `gpt-4` vs `gpt-3.5-turbo`

    Example curl:
    ```
    curl -X GET --location 'http://localhost:4000/utils/supported_openai_params?model=gpt-3.5-turbo-16k'         --header 'Authorization: Bearer sk-1234'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5360
async def transform_request(request: TransformRequestBody):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5366
async def _check_if_model_is_user_added(models: List[Dict], user_api_key_dict: UserAPIKeyAuth, prisma_client: Optional[PrismaClient]) -> List[Dict]:
    """Check if model is in db

    Check if db model is 'created_by' == user_api_key_dict.user_id

    Only return models that match"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5397
def _check_if_model_is_team_model(models: List[DeploymentTypedDict], user_row: LiteLLM_UserTable) -> List[Dict]:
    """Check if model is a team model

    Check if user is a member of the team that the model belongs to"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5417
async def non_admin_all_models(all_models: List[Dict], llm_router: Router, user_api_key_dict: UserAPIKeyAuth, prisma_client: Optional[PrismaClient]):
    """Check if model is in db

    Check if db model is 'created_by' == user_api_key_dict.user_id

    Only return models that match"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5469
async def model_info_v2(user_api_key_dict: UserAPIKeyAuth=..., model: Optional[str]=..., user_models_only: Optional[bool]=..., debug: Optional[bool]):
    "BETA ENDPOINT. Might change unexpectedly. Use `/v1/model/info` for now."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5582
async def model_streaming_metrics(user_api_key_dict: UserAPIKeyAuth=..., _selected_model_group: Optional[str], startTime: Optional[datetime], endTime: Optional[datetime]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5714
async def model_metrics(user_api_key_dict: UserAPIKeyAuth=..., _selected_model_group: Optional[str]="gpt-4-32k", startTime: Optional[datetime], endTime: Optional[datetime], api_key: Optional[str], customer: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5829
async def model_metrics_slow_responses(user_api_key_dict: UserAPIKeyAuth=..., _selected_model_group: Optional[str]="gpt-4-32k", startTime: Optional[datetime], endTime: Optional[datetime], api_key: Optional[str], customer: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 5919
async def model_metrics_exceptions(user_api_key_dict: UserAPIKeyAuth=..., _selected_model_group: Optional[str], startTime: Optional[datetime], endTime: Optional[datetime], api_key: Optional[str], customer: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6000
def _get_proxy_model_info(model: dict) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6051
async def model_info_v1(user_api_key_dict: UserAPIKeyAuth=..., litellm_model_id: Optional[str]):
    """Provides more info about each model in /models, including config.yaml descriptions (except api key and api base)

    Parameters:
        litellm_model_id: Optional[str] = None (this is the value of `x-litellm-model-id` returned in response headers)

        - When litellm_model_id is passed, it will return the info for that specific model
        - When litellm_model_id is not passed, it will return the info for all models

    Returns:
        Returns a dictionary containing information about each model.

    Example Response:
    ```json
    {
        "data": [
                    {
                        "model_name": "fake-openai-endpoint",
                        "litellm_params": {
                            "api_base": "https://exampleopenaiendpoint-production.up.railway.app/",
                            "model": "openai/fake"
                        },
                        "model_info": {
                            "id": "112f74fab24a7a5245d2ced3536dd8f5f9192c57ee6e332af0f0512e08bed5af",
                            "db_model": false
                        }
                    }
                ]
    }

    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6184
def _get_model_group_info(llm_router: Router, all_models_str: List[str], model_group: Optional[str]) -> List[ModelGroupInfo]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6210
async def model_group_info(user_api_key_dict: UserAPIKeyAuth=..., model_group: Optional[str]):
    """Get information about all the deployments on litellm proxy, including config.yaml descriptions (except api key and api base)

    - /model_group/info returns all model groups. End users of proxy should use /model_group/info since those models will be used for /chat/completions, /embeddings, etc.
    - /model_group/info?model_group=rerank-english-v3.0 returns all model groups for a specific model group (`model_name` in config.yaml)



    Example Request (All Models):
    ```shell
    curl -X 'GET'     'http://localhost:4000/model_group/info'     -H 'accept: application/json'     -H 'x-api-key: sk-1234'
    ```

    Example Request (Specific Model Group):
    ```shell
    curl -X 'GET'     'http://localhost:4000/model_group/info?model_group=rerank-english-v3.0'     -H 'accept: application/json'     -H 'Authorization: Bearer sk-1234'
    ```

    Example Request (Specific Wildcard Model Group): (e.g. `model_name: openai/*` on config.yaml)
    ```shell
    curl -X 'GET'     'http://localhost:4000/model_group/info?model_group=openai/tts-1'
    -H 'accept: application/json'     -H 'Authorization: Bearersk-1234'
    ```

    Learn how to use and set wildcard models [here](https://docs.litellm.ai/docs/wildcard_routing)

    Example Response:
    ```json
        {
            "data": [
                {
                "model_group": "rerank-english-v3.0",
                "providers": [
                    "cohere"
                ],
                "max_input_tokens": null,
                "max_output_tokens": null,
                "input_cost_per_token": 0.0,
                "output_cost_per_token": 0.0,
                "mode": null,
                "tpm": null,
                "rpm": null,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_function_calling": false,
                "supported_openai_params": [
                    "stream",
                    "temperature",
                    "max_tokens",
                    "logit_bias",
                    "top_p",
                    "frequency_penalty",
                    "presence_penalty",
                    "stop",
                    "n",
                    "extra_headers"
                ]
                },
                {
                "model_group": "gpt-3.5-turbo",
                "providers": [
                    "openai"
                ],
                "max_input_tokens": 16385.0,
                "max_output_tokens": 4096.0,
                "input_cost_per_token": 1.5e-06,
                "output_cost_per_token": 2e-06,
                "mode": "chat",
                "tpm": null,
                "rpm": null,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_function_calling": true,
                "supported_openai_params": [
                    "frequency_penalty",
                    "logit_bias",
                    "logprobs",
                    "top_logprobs",
                    "max_tokens",
                    "max_completion_tokens",
                    "n",
                    "presence_penalty",
                    "seed",
                    "stop",
                    "stream",
                    "stream_options",
                    "temperature",
                    "top_p",
                    "tools",
                    "tool_choice",
                    "function_call",
                    "functions",
                    "max_retries",
                    "extra_headers",
                    "parallel_tool_calls",
                    "response_format"
                ]
                },
                {
                "model_group": "llava-hf",
                "providers": [
                    "openai"
                ],
                "max_input_tokens": null,
                "max_output_tokens": null,
                "input_cost_per_token": 0.0,
                "output_cost_per_token": 0.0,
                "mode": null,
                "tpm": null,
                "rpm": null,
                "supports_parallel_function_calling": false,
                "supports_vision": true,
                "supports_function_calling": false,
                "supported_openai_params": [
                    "frequency_penalty",
                    "logit_bias",
                    "logprobs",
                    "top_logprobs",
                    "max_tokens",
                    "max_completion_tokens",
                    "n",
                    "presence_penalty",
                    "seed",
                    "stop",
                    "stream",
                    "stream_options",
                    "temperature",
                    "top_p",
                    "tools",
                    "tool_choice",
                    "function_call",
                    "functions",
                    "max_retries",
                    "extra_headers",
                    "parallel_tool_calls",
                    "response_format"
                ]
                }
            ]
            }
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6414
async def model_settings():
    """Used by UI to generate 'model add' page
    {
        field_name=field_name,
        field_type=allowed_args[field_name]["type"], # string/int
        field_description=field_info.description or "", # human-friendly description
        field_value=general_settings.get(field_name, None), # example value
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6447
async def alerting_settings(user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6556
async def async_queue_request(request: Request, fastapi_response: Response, model: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6667
async def fallback_login(request: Request):
    """Create Proxy API Keys using Google Workspace SSO. Requires setting PROXY_BASE_URL in .env
    PROXY_BASE_URL should be the your deployed proxy endpoint, e.g. PROXY_BASE_URL="https://litellm-production-7002.up.railway.app/"
    Example:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6696
async def login(request: Request):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 6917
async def onboarding(invite_link: str):
    """- Get the invite link
    - Validate it's still 'valid'
    - Invalidate the link (prevents abuse)
    - Get user from db
    - Pass in user_email if set"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7034
async def claim_onboarding_link(data: InvitationClaim):
    """Special route. Allows UI link share user to update their password.

    - Get the invite link
    - Validate it's still 'valid'
    - Check if user within initial session (prevents abuse)
    - Get user from db
    - Update user password

    This route can only update user password."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7110
def get_image():
    "Get logo to show on admin UI"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7151
async def new_invitation(data: InvitationNew, user_api_key_dict: UserAPIKeyAuth=...):
    """Allow admin to create invite links, to onboard new users to Admin UI.

    ```
    curl -X POST 'http://localhost:4000/invitation/new'         -H 'Content-Type: application/json'         -d '{
            "user_id": "1234" //  id of user in 'LiteLLM_UserTable'
        }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7217
async def invitation_info(invitation_id: str, user_api_key_dict: UserAPIKeyAuth=...):
    """Allow admin to create invite links, to onboard new users to Admin UI.

    ```
    curl -X POST 'http://localhost:4000/invitation/new'         -H 'Content-Type: application/json'         -d '{
            "user_id": "1234" //  id of user in 'LiteLLM_UserTable'
        }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7269
async def invitation_update(data: InvitationUpdate, user_api_key_dict: UserAPIKeyAuth=...):
    """Update when invitation is accepted

    ```
    curl -X POST 'http://localhost:4000/invitation/update'         -H 'Content-Type: application/json'         -d '{
            "invitation_id": "1234" //  id of invitation in 'LiteLLM_InvitationTable'
            "is_accepted": True // when invitation is accepted
        }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7330
async def invitation_delete(data: InvitationDelete, user_api_key_dict: UserAPIKeyAuth=...):
    """Delete invitation link

    ```
    curl -X POST 'http://localhost:4000/invitation/delete'         -H 'Content-Type: application/json'         -d '{
            "invitation_id": "1234" //  id of invitation in 'LiteLLM_InvitationTable'
        }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7383
async def update_config(config_info: ConfigYAML):
    """For Admin UI - allows admin to update config via UI

    Currently supports modifying General Settings + LiteLLM settings"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7535
async def update_config_general_settings(data: ConfigFieldUpdate, user_api_key_dict: UserAPIKeyAuth=...):
    "Update a specific field in litellm general settings"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7614
async def get_config_general_settings(field_name: str, user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7675
async def get_config_list(config_type: Literal[?], user_api_key_dict: UserAPIKeyAuth=...) -> List[ConfigList]:
    "List the available fields + current values for a given type of setting (currently just 'general_settings'user_api_key_dict: UserAPIKeyAuth = Depends(user_api_key_auth),)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7812
async def delete_config_general_settings(data: ConfigFieldDelete, user_api_key_dict: UserAPIKeyAuth=...):
    "Delete the db value of this field in litellm general settings. Resets it to it's initial default value on litellm."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 7884
async def get_config():
    """For Admin UI - allows admin to view config via UI
    # return the callbacks and the env variables for the callback"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 8075
async def config_yaml_endpoint(config_info: ConfigYAML):
    """This is a mock endpoint, to show what you can set in config.yaml details in the Swagger UI.

    Parameters:

    The config.yaml object has the following attributes:
    - **model_list**: *Optional[List[ModelParams]]* - A list of supported models on the server, along with model-specific configurations. ModelParams includes "model_name" (name of the model), "litellm_params" (litellm-specific parameters for the model), and "model_info" (additional info about the model such as id, mode, cost per token, etc).

    - **litellm_settings**: *Optional[dict]*: Settings for the litellm module. You can specify multiple properties like "drop_params", "set_verbose", "api_base", "cache".

    - **general_settings**: *Optional[ConfigGeneralSettings]*: General settings for the server like "completion_model" (default model for chat completion calls), "use_azure_key_vault" (option to load keys from azure key vault), "master_key" (key required for all calls to proxy), and others.

    Please, refer to each class's description for a better understanding of the specific attributes within them.

    Note: This is a mock endpoint primarily meant for demonstration purposes, and does not actually provide or change any configurations."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 8100
async def get_litellm_model_cost_map():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 8112
async def home(request: Request):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_server.py Line: 8117
async def get_routes():
    "Get a list of available routes in the FastAPI application."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/custom_prompt_management.py Line: 9
class X42PromptManagement(CustomPromptManagement):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_cli.py Line: 31
class LiteLLMDatabaseConnectionPool(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_cli.py Line: 36
def append_query_params(url, params) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_cli.py Line: 49
class ProxyInitializationHelpers:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/proxy_cli.py Line: 461
def run_server(host, port, api_base, api_version, model, alias, add_key, headers, save, debug, detailed_debug, temperature, max_tokens, request_timeout, drop_params, add_function_to_prompt, config, max_budget, telemetry, test, local, num_workers, test_async, iam_token_db_auth, num_requests, use_queue, health, version, run_gunicorn, run_hypercorn, ssl_keyfile_path, ssl_certfile_path, log_config, use_prisma_migrate):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_check.py Line: 26
def _get_random_llm_message():
    "Get a random message from the LLM."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_check.py Line: 35
def _clean_endpoint_data(endpoint_data: dict, details: Optional[bool]=True):
    "Clean the endpoint data for display to users."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_check.py Line: 47
def filter_deployments_by_id(model_list: List) -> List:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_check.py Line: 66
async def run_with_timeout(task, timeout):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_check.py Line: 83
async def _perform_health_check(model_list: list, details: Optional[bool]=True):
    "Perform a health check for each model in the list."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_check.py Line: 132
def _update_litellm_params_for_health_check(model_info: dict, litellm_params: dict) -> dict:
    """Update the litellm params for health check.

    - gets a short `messages` param for health check
    - updates the `model` param with the `health_check_model` if it exists Doc: https://docs.litellm.ai/docs/proxy/health#wildcard-routes"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_check.py Line: 148
async def perform_health_check(model_list: list, model: Optional[str], cli_model: Optional[str], details: Optional[bool]=True):
    """Perform a health check on the system.

    Returns:
        (bool): True if the health check passes, False otherwise."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/custom_validate.py Line: 4
def my_custom_validate(token: str) -> Literal[?]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 48
class LiteLLMTeamRoles:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 55
class LitellmUserRoles(str):
    """Admin Roles:
    PROXY_ADMIN: admin over the platform
    PROXY_ADMIN_VIEW_ONLY: can login, view all own keys, view all spend
    ORG_ADMIN: admin over a specific organization, can create teams, users only within their organization

    Internal User Roles:
    INTERNAL_USER: can login, view/create/delete their own keys, view their spend
    INTERNAL_USER_VIEW_ONLY: can login, view their own keys, view their own spend


    Team Roles:
    TEAM: used for JWT auth


    Customer Roles:
    CUSTOMER: External users -> these are customers"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 138
class LitellmTableNames(str):
    "Enum for Table Names used by LiteLLM"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 150
class Litellm_EntityType:
    """Enum for types of entities on litellm

    This enum allows specifying the type of entity that is being tracked in the database."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 165
def hash_token(token: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 174
class LiteLLM_UpperboundKeyGenerateParams(LiteLLMPydanticObjectBase):
    """Set default upperbound to max budget a key called via `/key/generate` can be.

    Args:
        max_budget (Optional[float], optional): Max budget a key can be. Defaults to None.
        budget_duration (Optional[str], optional): Duration of the budget. Defaults to None.
        duration (Optional[str], optional): Duration of the key. Defaults to None.
        max_parallel_requests (Optional[int], optional): Max number of requests that can be made in parallel. Defaults to None.
        tpm_limit (Optional[int], optional): Tpm limit. Defaults to None.
        rpm_limit (Optional[int], optional): Rpm limit. Defaults to None."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 195
class KeyManagementRoutes(str):
    "Enum for key management routes"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 217
class LiteLLMRoutes:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 489
class LiteLLMPromptInjectionParams(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 528
class ProxyChatCompletionRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 565
class ModelInfoDelete(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 569
class ModelInfo(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 609
class ProviderInfo(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 614
class BlockUsers(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 618
class ModelParams(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 635
class GenerateRequestBase(LiteLLMPydanticObjectBase):
    "Overlapping schema between key and user generate/update requests"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 667
class KeyRequestBase(GenerateRequestBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 675
class GenerateKeyRequest(KeyRequestBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 680
class GenerateKeyResponse(KeyRequestBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 716
class UpdateKeyRequest(KeyRequestBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 736
class RegenerateKeyRequest(GenerateKeyRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 745
class KeyRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 759
class LiteLLM_ModelTable(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 767
class LiteLLM_ProxyModelTable(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 791
class NewUserRequest(GenerateRequestBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 810
class NewUserResponse(GenerateKeyResponse):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 828
class UpdateUserRequest(GenerateRequestBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 854
class DeleteUserRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 861
class BudgetNewRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 890
class BudgetRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 894
class BudgetDeleteRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 898
class CustomerBase(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 909
class NewCustomerRequest(BudgetNewRequest):
    "Create a new customer, allocate a budget to them"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 934
class UpdateCustomerRequest(LiteLLMPydanticObjectBase):
    "Update a Customer, use this to update customer budgets etc"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 953
class DeleteCustomerRequest(LiteLLMPydanticObjectBase):
    "Delete multiple Customers"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 961
class MemberBase(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 975
class Member(MemberBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 982
class OrgMember(MemberBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 990
class TeamBase(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1010
class NewTeamRequest(TeamBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1018
class GlobalEndUsersSpend(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1024
class UpdateTeamRequest(LiteLLMPydanticObjectBase):
    """UpdateTeamRequest, used by /team/update when you need to update a team

    team_id: str
    team_alias: Optional[str] = None
    organization_id: Optional[str] = None
    metadata: Optional[dict] = None
    tpm_limit: Optional[int] = None
    rpm_limit: Optional[int] = None
    max_budget: Optional[float] = None
    models: Optional[list] = None
    blocked: Optional[bool] = None
    budget_duration: Optional[str] = None
    guardrails: Optional[List[str]] = None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1056
class ResetTeamBudgetRequest(LiteLLMPydanticObjectBase):
    """internal type used to reset the budget on a team
    used by reset_budget()

    team_id: str
    spend: float
    budget_reset_at: datetime"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1072
class DeleteTeamRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1076
class BlockTeamRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1080
class BlockKeyRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1084
class AddTeamCallback(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1106
class TeamCallbackMetadata(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1146
class LiteLLM_TeamTable(TeamBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1191
class LiteLLM_TeamTableCachedObj(LiteLLM_TeamTable):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1195
class TeamRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1199
class LiteLLM_BudgetTable(LiteLLMPydanticObjectBase):
    "Represents user-controllable params for a LiteLLM_BudgetTable record"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1213
class LiteLLM_TeamMemberTable(LiteLLM_BudgetTable):
    "Used to track spend of a user_id within a team_id"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1226
class NewOrganizationRequest(LiteLLM_BudgetTable):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1234
class OrganizationRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1238
class DeleteOrganizationRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1242
class KeyManagementSystem:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1252
class KeyManagementSettings(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1276
class TeamDefaultSettings(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1284
class DynamoDBArgs(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1305
class PassThroughGenericEndpoint(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1315
class PassThroughEndpointResponse(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1319
class ConfigFieldUpdate(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1325
class ConfigFieldDelete(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1330
class FieldDetail(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1338
class ConfigList(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1351
class ConfigGeneralSettings(LiteLLMPydanticObjectBase):
    "Documents all the fields supported by `general_settings` in config.yaml"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1457
class ConfigYAML(LiteLLMPydanticObjectBase):
    "Documents all the fields supported by the config.yaml"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1483
class LiteLLM_VerificationToken(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1518
class LiteLLM_VerificationTokenView(LiteLLM_VerificationToken):
    "Combined view of litellm verification token + litellm team table (select values)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1560
class UserAPIKeyAuth(LiteLLM_VerificationTokenView):
    "Return the row in the db"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1592
class UserInfoResponse(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1599
class LiteLLM_Config(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1604
class LiteLLM_OrganizationMembershipTable(LiteLLMPydanticObjectBase):
    "This is the table that track what organizations a user belongs to and users spend within the organization"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1624
class LiteLLM_OrganizationTable(LiteLLMPydanticObjectBase):
    "Represents user-controllable params for a LiteLLM_OrganizationTable record"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1637
class LiteLLM_OrganizationTableUpdate(LiteLLMPydanticObjectBase):
    "Represents user-controllable params for a LiteLLM_OrganizationTable record"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1649
class LiteLLM_OrganizationTableWithMembers(LiteLLM_OrganizationTable):
    "Returned by the /organization/info endpoint and /organization/list endpoint"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1659
class NewOrganizationResponse(LiteLLM_OrganizationTable):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1665
class LiteLLM_UserTable(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1700
class LiteLLM_UserTableFiltered(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1705
class LiteLLM_UserTableWithKeyCount(LiteLLM_UserTable):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1709
class LiteLLM_EndUserTable(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1728
class LiteLLM_SpendLogs(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1750
class LiteLLM_ErrorLogs(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1767
class LiteLLM_AuditLogs(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1786
class LiteLLM_SpendLogs_ResponseObject(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1790
class TokenCountRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1796
class TokenCountResponse(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1803
class CallInfo(LiteLLMPydanticObjectBase):
    "Used for slack budget alerting"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1820
class WebhookEvent(CallInfo):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1834
class SpecialModelNames:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1840
class InvitationNew(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1844
class InvitationUpdate(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1849
class InvitationDelete(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1853
class InvitationModel(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1865
class InvitationClaim(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1871
class ConfigFieldInfo(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1876
class CallbackOnUI(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1882
class AllCallbacks(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1961
class SpendLogsMetadata(TypedDict):
    "Specific metadata k,v pairs logged to spendlogs for easier cost tracking"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 1989
class SpendLogsPayload(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2017
class SpanAttributes(str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2065
class ManagementEndpointLoggingPayload(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2074
class ProxyException(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2121
class CommonProxyErrors(str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2133
class SpendCalculateRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2139
class ProxyErrorTypes(str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2231
class SSOUserDefinedValues(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2240
class VirtualKeyEvent(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2247
class CreatePassThroughEndpoint(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2253
class LiteLLM_TeamMembership(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2263
class MemberAddRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2282
class OrgMemberAddRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2301
class TeamAddMemberResponse(LiteLLM_TeamTable):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2306
class OrganizationAddMemberResponse(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2312
class MemberDeleteRequest(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2324
class MemberUpdateResponse(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2330
class TeamMemberAddRequest(MemberAddRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2335
class TeamMemberDeleteRequest(MemberDeleteRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2339
class TeamMemberUpdateRequest(TeamMemberDeleteRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2344
class TeamMemberUpdateResponse(MemberUpdateResponse):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2349
class TeamModelAddRequest(BaseModel):
    "Request to add models to a team"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2356
class TeamModelDeleteRequest(BaseModel):
    "Request to delete models from a team"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2364
class OrganizationMemberAddRequest(OrgMemberAddRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2371
class OrganizationMemberDeleteRequest(MemberDeleteRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2382
class OrganizationMemberUpdateRequest(OrganizationMemberDeleteRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2397
class OrganizationMemberUpdateResponse(MemberUpdateResponse):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2405
class TeamInfoResponseObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2412
class TeamListResponseObject(LiteLLM_TeamTable):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2417
class KeyListResponseObject(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2424
class CurrentItemRateLimit(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2430
class LoggingCallbackStatus(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2436
class KeyHealthResponse(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2441
class SpecialHeaders:
    "Used by user_api_key_auth.py to get litellm key"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2451
class LitellmDataForBackendLLMCall(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2457
class JWTKeyItem(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2464
class JWKUrlResponse(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2468
class UserManagementEndpointParamDocStringEnums(str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2519
class PassThroughEndpointLoggingTypedDict(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2540
class ProviderBudgetResponseObject(LiteLLMPydanticObjectBase):
    "Configuration for a single provider's budget settings"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2551
class ProviderBudgetResponse(LiteLLMPydanticObjectBase):
    """Complete provider budget configuration and status.
    Maps provider names to their budget configs."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2562
class ProxyStateVariables(TypedDict):
    "TypedDict for Proxy state variables."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2573
class JWTAuthBuilderResult(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2586
class ClientSideFallbackModel(TypedDict, total=...):
    "Dictionary passed when client configuring input"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2605
class OIDCPermissions(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2610
class RoleBasedPermissions(OIDCPermissions):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2618
class RoleMapping(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2623
class ScopeMapping(OIDCPermissions):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2631
class LiteLLM_JWTAuth(LiteLLMPydanticObjectBase):
    """A class to define the roles and permissions for a LiteLLM Proxy w/ JWT Auth.

    Attributes:
    - admin_jwt_scope: The JWT scope required for proxy admin roles.
    - admin_allowed_routes: list of allowed routes for proxy admin roles.
    - team_jwt_scope: The JWT scope required for proxy team roles.
    - team_id_jwt_field: The field in the JWT token that stores the team ID. Default - `client_id`.
    - team_allowed_routes: list of allowed routes for proxy team roles.
    - user_id_jwt_field: The field in the JWT token that stores the user id (maps to `LiteLLMUserTable`). Use this for internal employees.
    - user_email_jwt_field: The field in the JWT token that stores the user email (maps to `LiteLLMUserTable`). Use this for internal employees.
    - user_allowed_email_subdomain: If specified, only emails from specified subdomain will be allowed to access proxy.
    - end_user_id_jwt_field: The field in the JWT token that stores the end-user ID (maps to `LiteLLMEndUserTable`). Turn this off by setting to `None`. Enables end-user cost tracking. Use this for external customers.
    - public_key_ttl: Default - 600s. TTL for caching public JWT keys.
    - public_allowed_routes: list of allowed routes for authenticated but unknown litellm role jwt tokens.
    - enforce_rbac: If true, enforce RBAC for all routes.
    - custom_validate: A custom function to validates the JWT token.

    See `auth_checks.py` for the specific routes"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2737
class PrismaCompatibleUpdateDBModel(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2745
class SpecialManagementEndpointEnums:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2749
class TransformRequestBody(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2754
class DefaultInternalUserParams(LiteLLMPydanticObjectBase):
    "Default parameters to apply when a new user signs in via SSO or is created on the /user/new API endpoint"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2783
class BaseDailySpendTransaction(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2803
class DailyTeamSpendTransaction(BaseDailySpendTransaction):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2807
class DailyUserSpendTransaction(BaseDailySpendTransaction):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2811
class DailyTagSpendTransaction(BaseDailySpendTransaction):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2815
class DBSpendUpdateTransactions(TypedDict):
    "Internal Data Structure for buffering spend updates in Redis or in memory before committing them to the database"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2828
class SpendUpdateQueueItem(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_types.py Line: 2834
class LiteLLM_ManagedFileTable(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 35
def handle_any_messages_to_chat_completion_str_messages_conversion(messages: Any) -> List[Dict[(str, str)]]:
    """Handles any messages to chat completion str messages conversion

    Relevant Issue: https://github.com/BerriAI/litellm/issues/9494"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 64
def handle_messages_with_content_list_to_str_conversion(messages: List[AllMessageValues]) -> List[AllMessageValues]:
    "Handles messages with content list conversion"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 77
def strip_name_from_messages(messages: List[AllMessageValues], allowed_name_roles: List[str]=['user']) -> List[AllMessageValues]:
    "Removes 'name' from messages"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 93
def strip_none_values_from_message(message: AllMessageValues) -> AllMessageValues:
    "Strips None values from message"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 100
def convert_content_list_to_str(message: AllMessageValues) -> str:
    """- handles scenario where content is list and not string
    - content list is just text, and no images

    Motivation: mistral api + azure ai don't support content as a list"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 121
def get_str_from_messages(messages: List[AllMessageValues]) -> str:
    "Converts a list of messages to a string"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 131
def is_non_content_values_set(message: AllMessageValues) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 138
def _audio_or_image_in_message_content(message: AllMessageValues) -> bool:
    "Checks if message content contains an image or audio"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 151
def convert_openai_message_to_only_content_messages(messages: List[AllMessageValues]) -> List[Dict[(str, str)]]:
    """Converts OpenAI messages to only content messages

    Used for calling guardrails integrations which expect string content"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 173
def get_content_from_model_response(response: Union[(ModelResponse, dict)]) -> str:
    "Gets content from model response"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 197
def detect_first_expected_role(messages: List[AllMessageValues]) -> Optional[Literal[(?, ?)]]:
    """Detect the first expected role based on the message sequence.

    Rules:
    1. If messages list is empty, assume 'user' starts
    2. If first message is from assistant, expect 'user' next
    3. If first message is from user, expect 'assistant' next
    4. If first message is system, look at the next non-system message

    Returns:
        str: Either 'user' or 'assistant'
        None: If no 'user' or 'assistant' messages provided"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 224
def _insert_user_continue_message(messages: List[AllMessageValues], user_continue_message: Optional[ChatCompletionUserMessage], ensure_alternating_roles: bool) -> List[AllMessageValues]:
    """Inserts a user continue message into the messages list.
    Handles three cases:
    1. Initial assistant message
    2. Final assistant message
    3. Consecutive assistant messages

    Only inserts messages between consecutive assistant messages,
    ignoring all other role types."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 270
def _insert_assistant_continue_message(messages: List[AllMessageValues], assistant_continue_message: Optional[ChatCompletionAssistantMessage], ensure_alternating_roles: bool=True) -> List[AllMessageValues]:
    """Add assistant continuation messages between consecutive user messages.

    Args:
        messages: List of message dictionaries
        assistant_continue_message: Optional custom assistant message
        ensure_alternating_roles: Whether to enforce alternating roles

    Returns:
        Modified list of messages with inserted assistant messages"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 310
def get_completion_messages(messages: List[AllMessageValues], assistant_continue_message: Optional[ChatCompletionAssistantMessage], user_continue_message: Optional[ChatCompletionUserMessage], ensure_alternating_roles: bool) -> List[AllMessageValues]:
    """Ensures messages alternate between user and assistant roles by adding placeholders
    only when there are consecutive messages of the same role.

    1. ensure 'user' message before 1st 'assistant' message
    2. ensure 'user' message after last 'assistant' message"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 338
def get_format_from_file_id(file_id: Optional[str]) -> Optional[str]:
    """Gets format from file id

    unified_file_id = litellm_proxy:{};unified_id,{}
    If not a unified file id, returns 'file' as default format"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 368
def update_messages_with_model_file_ids(messages: List[AllMessageValues], model_id: str, model_file_id_mapping: Dict[(str, Dict[(str, str)])]) -> List[AllMessageValues]:
    """Updates messages with model file ids.

    model_file_id_mapping: Dict[str, Dict[str, str]] = {
        "litellm_proxy/file_id": {
            "model_id": "provider_file_id"
        }
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 409
def extract_file_data(file_data: FileTypes) -> ExtractedFileData:
    """Extracts and processes file data from various input formats.

    Args:
        file_data: Can be a tuple of (filename, content, [content_type], [headers]) or direct file content

    Returns:
        ExtractedFileData containing:
        - filename: Name of the file if provided
        - content: The file content in bytes
        - content_type: MIME type of the file
        - headers: Any additional headers"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 472
def unpack_defs(schema, defs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/common_utils.py Line: 505
def _get_image_mime_type_from_url(url: str) -> Optional[str]:
    """Get mime type for common image URLs
    See gemini mime types: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/image-understanding#image-requirements

    Supported by Gemini:
     application/pdf
    audio/mpeg
    audio/mp3
    audio/wav
    image/png
    image/jpeg
    image/webp
    text/plain
    video/mov
    video/mpeg
    video/mp4
    video/mpg
    video/avi
    video/wmv
    video/mpegps
    video/flv"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 42
def default_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 46
def prompt_injection_detection_default_pt():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 70
def map_system_message_pt(messages: list) -> list:
    """Convert 'system' message to 'user' message if provider doesn't support 'system' role.

    Enabled via `completion(...,supports_system_message=False)`

    If next message is a user message or assistant message -> merge system prompt into it

    if next message is system -> append a user message instead of the system message"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 106
def alpaca_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 127
def llama_2_chat_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 149
def convert_to_ollama_image(openai_image_url: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 169
def _handle_ollama_system_message(messages: list, prompt: str, msg_i: int) -> Tuple[(str, int)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 183
def ollama_pt(model: str, messages: list) -> Union[(str, OllamaVisionModelObject)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 273
def mistral_instruct_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 292
def falcon_instruct_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 308
def falcon_chat_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 322
def mpt_chat_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 335
def wizardcoder_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 348
def phind_codellama_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 360
def hf_chat_template(model: str, messages: list, chat_template: Optional[Any]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 490
def deepseek_r1_pt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 497
def claude_2_1_pt(messages: list):
    """Claude v2.1 allows system prompts (no Human: needed), but requires it be followed by Human:
    - you can't just pass a system message
    - you can't pass a system message and follow that with an assistant message
    if system message is passed in, you can only do system, human, assistant or system, human

    if a system message is passed in and followed by an assistant message, insert a blank human message between them.

    Additionally, you can "put words in Claude's mouth" by ending with an assistant message.
    See: https://docs.anthropic.com/claude/docs/put-words-in-claudes-mouth"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 534
def get_model_info(token, model):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 579
def ibm_granite_pt(messages: list):
    """IBM's Granite models uses the template:
    <|system|> {system_message} <|user|> {user_message} <|assistant|> {assistant_message}

    See: https://www.ibm.com/docs/en/watsonx-as-a-service?topic=solutions-supported-foundation-models"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 611
def anthropic_pt(messages: list):
    """You can "put words in Claude's mouth" by ending with an assistant message.
    See: https://docs.anthropic.com/claude/docs/put-words-in-claudes-mouth"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 642
def construct_format_parameters_prompt(parameters: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 652
def construct_format_tool_for_claude_prompt(name, description, parameters):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 667
def construct_tool_use_system_prompt(tools):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 699
def convert_generic_image_chunk_to_openai_image_obj(image_chunk: GenericImageParsingChunk) -> str:
    """Convert a generic image chunk to an OpenAI image object.

    Input:
    GenericImageParsingChunk(
        type="base64",
        media_type="image/jpeg",
        data="...",
    )

    Return:
    "data:image/jpeg;base64,{base64_image}""""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 719
def convert_to_anthropic_image_obj(openai_image_url: str, format: Optional[str]) -> GenericImageParsingChunk:
    """Input:
    "image_url": "data:image/jpeg;base64,{base64_image}",

    Return:
    "source": {
      "type": "base64",
      "media_type": "image/jpeg",
      "data": {base64_image},
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 759
def convert_to_anthropic_tool_result_xml(message: dict) -> str:
    """OpenAI message with a tool result looks like:
    {
        "tool_call_id": "tool_1",
        "role": "tool",
        "name": "get_current_weather",
        "content": "function result goes here",
    },"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 810
def convert_to_anthropic_tool_invoke_xml(tool_calls: list) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 837
def anthropic_messages_pt_xml(messages: list):
    """format messages for anthropic
    1. Anthropic supports roles like "user" and "assistant", (here litellm translates system-> assistant)
    2. The first message always needs to be of role "user"
    3. Each message must alternate between "user" and "assistant" (this is not addressed as now by litellm)
    4. final assistant content cannot end with trailing whitespace (anthropic raises an error otherwise)
    5. System messages are a separate param to the Messages API (used for tool calling)
    6. Ensure we only accept role, content. (message.name is not supported)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 930
def _azure_tool_call_invoke_helper(function_call_params: ChatCompletionToolCallFunctionChunk) -> Optional[ChatCompletionToolCallFunctionChunk]:
    "Azure requires 'arguments' to be a string."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 941
def convert_to_azure_openai_messages(messages: List[AllMessageValues]) -> List[AllMessageValues]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 955
def infer_protocol_value(value: Any) -> Literal[(?, ?, ?, ?, ?, ?, ?)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 982
def _gemini_tool_call_invoke_helper(function_call_params: ChatCompletionToolCallFunctionChunk) -> Optional[VertexFunctionCall]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 995
def convert_to_gemini_tool_call_invoke(message: ChatCompletionAssistantMessage) -> List[VertexPartType]:
    """    OpenAI tool invokes:
        {
          "role": "assistant",
          "content": null,
          "tool_calls": [
            {
              "id": "call_abc123",
              "type": "function",
              "function": {
                "name": "get_current_weather",
                "arguments": "{
    "location": "Boston, MA"
    }"
              }
            }
          ]
        },
        """

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1080
def convert_to_gemini_tool_call_result(message: Union[(ChatCompletionToolMessage, ChatCompletionFunctionMessage)], last_message_with_tool_calls: Optional[dict]) -> VertexPartType:
    """OpenAI message with a tool result looks like:
    {
        "tool_call_id": "tool_1",
        "role": "tool",
        "content": "function result goes here",
    },

    # NOTE: Function messages have been deprecated
    OpenAI message with a function call result looks like:
    {
        "role": "function",
        "name": "get_current_weather",
        "content": "function result goes here",
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1141
def convert_to_anthropic_tool_result(message: Union[(ChatCompletionToolMessage, ChatCompletionFunctionMessage)]) -> AnthropicMessagesToolResultParam:
    """OpenAI message with a tool result looks like:
    {
        "tool_call_id": "tool_1",
        "role": "tool",
        "name": "get_current_weather",
        "content": "function result goes here",
    },

    OpenAI message with a function call result looks like:
    {
        "role": "function",
        "name": "get_current_weather",
        "content": "function result goes here",
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1243
def convert_function_to_anthropic_tool_invoke(function_call: Union[(dict, ChatCompletionToolCallFunctionChunk)]) -> List[AnthropicMessagesToolUseParam]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1262
def convert_to_anthropic_tool_invoke(tool_calls: List[ChatCompletionAssistantToolCall]) -> List[AnthropicMessagesToolUseParam]:
    """    OpenAI tool invokes:
        {
          "role": "assistant",
          "content": null,
          "tool_calls": [
            {
              "id": "call_abc123",
              "type": "function",
              "function": {
                "name": "get_current_weather",
                "arguments": "{
    "location": "Boston, MA"
    }"
              }
            }
          ]
        },
        """

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1336
def add_cache_control_to_content(anthropic_content_element: Union[(dict, AnthropicMessagesImageParam, AnthropicMessagesTextParam, AnthropicMessagesDocumentParam, AnthropicMessagesToolUseParam, ChatCompletionThinkingBlock)], orignal_content_element: Union[(dict, AllMessageValues)]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1356
def _anthropic_content_element_factory(image_chunk: GenericImageParsingChunk) -> Union[(AnthropicMessagesImageParam, AnthropicMessagesDocumentParam)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1383
def anthropic_messages_pt(messages: List[AllMessageValues], model: str, llm_provider: str) -> List[Union[(AnthropicMessagesUserMessageParam, AnthopicMessagesAssistantMessageParam)]]:
    """format messages for anthropic
    1. Anthropic supports roles like "user" and "assistant" (system prompt sent separately)
    2. The first message always needs to be of role "user"
    3. Each message must alternate between "user" and "assistant" (this is not addressed as now by litellm)
    4. final assistant content cannot end with trailing whitespace (anthropic raises an error otherwise)
    5. System messages are a separate param to the Messages API
    6. Ensure we only accept role, content. (message.name is not supported)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1629
def extract_between_tags(tag: str, string: str, strip: bool) -> List[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1636
def contains_tag(tag: str, string: str) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1640
def parse_xml_params(xml_content, json_schema: Optional[dict]):
    """Compare the xml output to the json schema

    check if a value is a list - if so, get it's child elements"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1690
def get_system_prompt(messages):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1715
def convert_openai_message_to_cohere_tool_result(message: Union[(ChatCompletionToolMessage, ChatCompletionFunctionMessage)], tool_calls: List) -> ToolResultObject:
    """OpenAI message with a tool result looks like:
    {
            "tool_call_id": "tool_1",
            "role": "tool",
            "content": {"location": "San Francisco, CA", "unit": "fahrenheit", "temperature": "72"},
    },"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1806
def get_all_tool_calls(messages: List) -> List:
    """Returns extracted list of `tool_calls`.

    Done to handle openai no longer returning tool call 'name' in tool results."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1821
def convert_to_cohere_tool_invoke(tool_calls: list) -> List[ToolCallObject]:
    """    OpenAI tool invokes:
        {
          "role": "assistant",
          "content": null,
          "tool_calls": [
            {
              "id": "call_abc123",
              "type": "function",
              "function": {
                "name": "get_current_weather",
                "arguments": "{
    "location": "Boston, MA"
    }"
              }
            }
          ]
        },
        """

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 1866
def cohere_messages_pt_v2(messages: List, model: str, llm_provider: str) -> Tuple[(Union[(str, ToolResultObject)], ChatHistory)]:
    """Returns a tuple(Union[tool_result, message], chat_history)

    - if last message is tool result -> return 'tool_result'
    - if last message is text -> return message (str)

    - return preceding messages as 'chat_history'

    Note:
    - cannot specify message if the last entry in chat history contains tool results
    - message must be at least 1 token long or tool results must be specified.
    - cannot specify tool_results if the last entry in chat history contains a user message"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2008
def cohere_message_pt(messages: list):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2025
def amazon_titan_pt(messages: list):
    "Amazon Titan uses 'User:' and 'Bot: in it's prompt template"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2053
def _load_image_from_url(image_url):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2080
def _gemini_vision_convert_messages(messages: list):
    """Converts given messages for GPT-4 Vision to Gemini format.

    Args:
        messages (list): The messages to convert. Each message can be a dictionary with a "content" key. The content can be a string or a list of elements. If it is a string, it will be concatenated to the prompt. If it is a list, each element will be processed based on its type:
            - If the element is a dictionary with a "type" key equal to "text", its "text" value will be concatenated to the prompt.
            - If the element is a dictionary with a "type" key equal to "image_url", its "image_url" value will be added to the list of images.

    Returns:
        tuple: A tuple containing the prompt (a string) and the processed images (a list of objects representing the images)."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2149
def gemini_text_image_pt(messages: list):
    """{
        "contents":[
            {
            "parts":[
                {"text": "What is this picture?"},
                {
                "inline_data": {
                    "mime_type":"image/jpeg",
                    "data": "'$(base64 -w0 image.jpg)'"
                }
                }
            ]
            }
        ]
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2193
def azure_text_pt(messages: list):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2208
def stringify_json_tool_call_content(messages: List) -> List:
    """- Check 'content' in tool role -> convert to dict (if not) -> stringify

    Done for azure_ai/cohere calls to handle results of a tool call"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2255
def _parse_content_type(content_type: str) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2261
def _parse_mime_type(base64_data: str) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2269
class BedrockImageProcessor:
    "Handles both sync and async image processing for Bedrock conversations."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2440
def _convert_to_bedrock_tool_call_invoke(tool_calls: list) -> List[BedrockContentBlock]:
    """    OpenAI tool invokes:
        {
          "role": "assistant",
          "content": null,
          "tool_calls": [
            {
              "id": "call_abc123",
              "type": "function",
              "function": {
                "name": "get_current_weather",
                "arguments": "{
    "location": "Boston, MA"
    }"
              }
            }
          ]
        },
        """

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2501
def _convert_to_bedrock_tool_call_result(message: Union[(ChatCompletionToolMessage, ChatCompletionFunctionMessage)]) -> BedrockContentBlock:
    """OpenAI message with a tool result looks like:
    {
        "tool_call_id": "tool_1",
        "role": "tool",
        "name": "get_current_weather",
        "content": "function result goes here",
    },

    OpenAI message with a function call result looks like:
    {
        "role": "function",
        "name": "get_current_weather",
        "content": "function result goes here",
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2565
def _insert_assistant_continue_message(messages: List[BedrockMessageBlock], assistant_continue_message: Optional[Union[(str, ChatCompletionAssistantMessage)]]) -> List[BedrockMessageBlock]:
    """Add dummy message between user/tool result blocks.

    Conversation blocks and tool result blocks cannot be provided in the same turn. Issue: https://github.com/BerriAI/litellm/issues/6053"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2607
def get_user_message_block_or_continue_message(message: ChatCompletionUserMessage, user_continue_message: Optional[ChatCompletionUserMessage]) -> ChatCompletionUserMessage:
    """Returns the user content block
    if content block is an empty string, then return the default continue message

    Relevant Issue: https://github.com/BerriAI/litellm/issues/7169"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2672
def return_assistant_continue_message(assistant_continue_message: Optional[Union[(str, ChatCompletionAssistantMessage)]]) -> ChatCompletionAssistantMessage:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2688
def _skip_empty_dict_blocks(blocks: List[dict]) -> List[dict]:
    """Filter out empty text blocks from a list of dictionaries.

    Args:
        blocks: List of dictionaries representing message content blocks

    Returns:
        Filtered list of non-empty text blocks"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2706
def skip_empty_text_blocks(message: ChatCompletionAssistantMessage) -> ChatCompletionAssistantMessage:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2713
def skip_empty_text_blocks(message: ChatCompletionUserMessage) -> ChatCompletionUserMessage:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2719
def skip_empty_text_blocks(message: Union[(ChatCompletionAssistantMessage, ChatCompletionUserMessage)]) -> Union[(ChatCompletionAssistantMessage, ChatCompletionUserMessage)]:
    """Skips empty text blocks in message content text blocks.

    Do not insert content here. This is a helper function, which can also be used in base case."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2768
def process_empty_text_blocks(message: ChatCompletionAssistantMessage, assistant_continue_message: Optional[Union[(str, ChatCompletionAssistantMessage)]]) -> ChatCompletionAssistantMessage:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2807
def get_assistant_message_block_or_continue_message(message: ChatCompletionAssistantMessage, assistant_continue_message: Optional[Union[(str, ChatCompletionAssistantMessage)]]) -> ChatCompletionAssistantMessage:
    """Returns the user content block
    if content block is an empty string, then return the default continue message

    Relevant Issue: https://github.com/BerriAI/litellm/issues/7169"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 2858
class BedrockConverseMessagesProcessor:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 3146
def _bedrock_converse_messages_pt(messages: List, model: str, llm_provider: str, user_continue_message: Optional[ChatCompletionUserMessage], assistant_continue_message: Optional[Union[(str, ChatCompletionAssistantMessage)]]) -> List[BedrockMessageBlock]:
    """Converts given messages from OpenAI format to Bedrock format

    - Roles must alternate b/w 'user' and 'model' (same as anthropic -> merge consecutive roles)
    - Please ensure that function response turn comes immediately after a function call turn
    - Conversation blocks and tool result blocks cannot be provided in the same turn. Issue: https://github.com/BerriAI/litellm/issues/6053"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 3367
def make_valid_bedrock_tool_name(input_tool_name: str) -> str:
    """Replaces any invalid characters in the input tool name with underscores
    and ensures the resulting string is a valid identifier for Bedrock tools"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 3402
def _bedrock_tools_pt(tools: List) -> List[BedrockToolBlock]:
    """OpenAI tools looks like:
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Get the current weather in a given location",
                "parameters": {
                    "type": "object",
                    "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                    },
                    "required": ["location"],
                },
            }
        }
    ]"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 3484
def function_call_prompt(messages: list, functions: list):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 3501
def response_schema_prompt(model: str, response_schema: dict) -> str:
    """Decides if a user-defined custom prompt or default needs to be used

    Returns the prompt str that's passed to the model as a user message"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 3529
def default_response_schema_prompt(response_schema: dict) -> str:
    """Used if provider/model doesn't support 'response_schema' param.

    This is the default prompt. Allow user to override this with a custom_prompt."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 3545
def custom_prompt(role_dict: dict, messages: list, initial_prompt_value: str="", final_prompt_value: str="", bos_token: str="", eos_token: str="") -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 3593
def prompt_factory(model: str, messages: list, custom_llm_provider: Optional[str], api_key: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py Line: 3732
def get_attribute_or_key(tool_or_function, attribute, default):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/image_handling.py Line: 18
def _process_image_response(response: Response, url: str) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/image_handling.py Line: 50
async def async_convert_url_to_base64(url: str) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/image_handling.py Line: 67
def convert_url_to_base64(url: str) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/specialty_caches/dynamic_logging_cache.py Line: 8
class DynamicLoggingCache:
    """Prevent memory leaks caused by initializing new logging clients on each request.

    Relevant Issue: https://github.com/BerriAI/litellm/issues/5695"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py Line: 12
def _is_above_128k(tokens: float) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py Line: 18
def _generic_cost_per_character(model: str, custom_llm_provider: str, prompt_characters: float, completion_characters: float, custom_prompt_cost: Optional[float], custom_completion_cost: Optional[float]) -> Tuple[(Optional[float], Optional[float])]:
    """Calculates cost per character for aspeech/speech calls.

    Calculates the cost per character for a given model, input messages, and response object.

    Input:
        - model: str, the model name without provider prefix
        - custom_llm_provider: str, "vertex_ai-*"
        - prompt_characters: float, the number of input characters
        - completion_characters: float, the number of output characters

    Returns:
        Tuple[Optional[float], Optional[float]] - prompt_cost_in_usd, completion_cost_in_usd.
        - returns None if not able to calculate cost.

    Raises:
        Exception if 'input_cost_per_character' or 'output_cost_per_character' is missing from model_info"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py Line: 93
def _get_token_base_cost(model_info: ModelInfo, usage: Usage) -> Tuple[(float, float)]:
    """Return prompt cost for a given model and usage.

    If input_tokens > threshold and `input_cost_per_token_above_[x]k_tokens` or `input_cost_per_token_above_[x]_tokens` is set,
    then we use the corresponding threshold cost."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py Line: 134
def calculate_cost_component(model_info: ModelInfo, cost_key: str, usage_value: Optional[float]) -> float:
    """Generic cost calculator for any usage component

    Args:
        model_info: Dictionary containing cost information
        cost_key: The key for the cost multiplier in model_info (e.g., 'input_cost_per_audio_token')
        usage_value: The actual usage value (e.g., number of tokens, characters, seconds)

    Returns:
        float: The calculated cost"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py Line: 159
def generic_cost_per_token(model: str, usage: Usage, custom_llm_provider: str) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Handles context caching as well.

    Input:
        - model: str, the model name without provider prefix
        - usage: LiteLLM Usage block, containing anthropic caching information

    Returns:
        Tuple[float, float] - prompt_cost_in_usd, completion_cost_in_usd"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_cost_calc/tool_call_cost_tracking.py Line: 18
class StandardBuiltInToolCostTracking:
    """Helper class for tracking the cost of built-in tools

    Example: Web Search"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/audio_utils/utils.py Line: 10
def get_audio_file_name(file_obj: FileTypes) -> str:
    """Safely get the name of a file-like object or return its string representation.

    Args:
        file_obj (Any): A file-like object or any other object.

    Returns:
        str: The name of the file if available, otherwise a string representation of the object."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/audio_utils/utils.py Line: 28
def get_audio_file_for_health_check() -> FileTypes:
    """Get an audio file for health check

    Returns the content of `audio_health_check.wav` in the same directory as this file"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/get_formatted_prompt.py Line: 4
def get_formatted_prompt(data: dict, call_type: Literal[(?, ?, ?, ?, ?, ?)]) -> str:
    """Extracts the prompt from the input data based on the call type.

    Returns a string."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/response_metadata.py Line: 15
class ResponseMetadata:
    "Handles setting and managing `_hidden_params`, `response_time_ms`, and `litellm_overhead_time_ms` for LiteLLM responses"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/response_metadata.py Line: 105
def update_response_metadata(result: Any, logging_obj: LiteLLMLoggingObject, model: Optional[str], kwargs: dict, start_time: ?, end_time: ?) -> ?:
    "Updates response metadata including hidden params and timing metrics"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/get_headers.py Line: 4
def get_response_headers(_response_headers: Optional[dict]) -> dict:
    """Sets the Appropriate OpenAI headers for the response and forward all headers as llm_provider-{header}

    Note: _response_headers Passed here should be OpenAI compatible headers

    Args:
        _response_headers (Optional[dict], optional): _response_headers. Defaults to None.

    Returns:
        dict: _response_headers with OpenAI headers and llm_provider-{header}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/get_headers.py Line: 42
def _get_llm_provider_headers(response_headers: dict) -> dict:
    """Adds a llm_provider-{header} to all headers that are not already prefixed with llm_provider

    Forward all headers as llm_provider-{header}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py Line: 40
def convert_tool_call_to_json_mode(tool_calls: List[ChatCompletionMessageToolCall], convert_tool_call_to_json_mode: bool) -> Tuple[(Optional[Message], Optional[str])]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py Line: 59
async def convert_to_streaming_response_async(response_object: Optional[dict]):
    """Asynchronously converts a response object to a streaming response.

    Args:
        response_object (Optional[dict]): The response object to be converted. Defaults to None.

    Raises:
        Exception: If the response object is None.

    Yields:
        ModelResponse: The converted streaming response object.

    Returns:
        None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py Line: 145
def convert_to_streaming_response(response_object: Optional[dict]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py Line: 194
def _handle_invalid_parallel_tool_calls(tool_calls: List[ChatCompletionMessageToolCall]):
    """Handle hallucinated parallel tool call from openai - https://community.openai.com/t/model-tries-to-call-unknown-function-multi-tool-use-parallel/490653

    Code modified from: https://github.com/phdowling/openai_multi_tool_use_parallel_patch/blob/main/openai_multi_tool_use_parallel_patch.py"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py Line: 242
def _parse_content_for_reasoning(message_text: Optional[str]) -> Tuple[(Optional[str], Optional[str])]:
    """Parse the content for reasoning

    Returns:
    - reasoning_content: The content of the reasoning
    - content: The content of the message"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py Line: 263
def _extract_reasoning_content(message: dict) -> Tuple[(Optional[str], Optional[str])]:
    """Extract reasoning content and main content from a message.

    Args:
        message (dict): The message dictionary that may contain reasoning_content

    Returns:
        tuple[Optional[str], Optional[str]]: A tuple of (reasoning_content, content)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py Line: 281
class LiteLLMResponseObjectHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py Line: 363
def _should_convert_tool_call_to_json_mode(tool_calls: Optional[Union[(List[ChatCompletionMessageToolCall], List[DatabricksTool])]], convert_tool_call_to_json_mode: Optional[bool]) -> bool:
    "Determine if tool calls should be converted to JSON mode"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py Line: 382
def convert_to_model_response_object(response_object: Optional[dict], model_response_object: Optional[Union[(ModelResponse, EmbeddingResponse, ImageResponse, TranscriptionResponse, RerankResponse)]], response_type: Literal[(?, ?, ?, ?, ?)]="completion", stream, start_time, end_time, hidden_params: Optional[dict], _response_headers: Optional[dict], convert_tool_call_to_json_mode: Optional[bool]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/litellm_core_utils/llm_response_utils/get_api_base.py Line: 10
def get_api_base(model: str, optional_params: Union[(dict, LiteLLM_Params)]) -> Optional[str]:
    """Returns the api base used for calling the model.

    Parameters:
    - model: str - the model passed to litellm.completion()
    - optional_params - the 'litellm_params' in router.completion *OR* additional params passed to litellm.completion - eg. api_base, api_key, etc. See `LiteLLM_Params` - https://github.com/BerriAI/litellm/blob/f09e6ba98d65e035a79f73bc069145002ceafd36/litellm/router.py#L67

    Returns:
    - string (api_base) or None

    Example:
    ```
    from litellm import get_api_base

    get_api_base(model="gemini/gemini-pro")
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/anthropic_interface/messages/__init__.py Line: 23
async def acreate(max_tokens: int, messages: List[Dict], model: str, metadata: Optional[Dict], stop_sequences: Optional[List[str]], stream: Optional[bool], system: Optional[str], temperature: Optional[float]=1.0, thinking: Optional[Dict], tool_choice: Optional[Dict], tools: Optional[List[Dict]], top_k: Optional[int], top_p: Optional[float], **kwargs) -> Union[(AnthropicMessagesResponse, AsyncIterator)]:
    """Async wrapper for Anthropic's messages API

    Args:
        max_tokens (int): Maximum tokens to generate (required)
        messages (List[Dict]): List of message objects with role and content (required)
        model (str): Model name to use (required)
        metadata (Dict, optional): Request metadata
        stop_sequences (List[str], optional): Custom stop sequences
        stream (bool, optional): Whether to stream the response
        system (str, optional): System prompt
        temperature (float, optional): Sampling temperature (0.0 to 1.0)
        thinking (Dict, optional): Extended thinking configuration
        tool_choice (Dict, optional): Tool choice configuration
        tools (List[Dict], optional): List of tool definitions
        top_k (int, optional): Top K sampling parameter
        top_p (float, optional): Nucleus sampling parameter
        **kwargs: Additional arguments

    Returns:
        Dict: Response from the API"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/anthropic_interface/messages/__init__.py Line: 79
async def create(max_tokens: int, messages: List[Dict], model: str, metadata: Optional[Dict], stop_sequences: Optional[List[str]], stream: Optional[bool], system: Optional[str], temperature: Optional[float]=1.0, thinking: Optional[Dict], tool_choice: Optional[Dict], tools: Optional[List[Dict]], top_k: Optional[int], top_p: Optional[float], **kwargs) -> Union[(AnthropicMessagesResponse, Iterator)]:
    """Async wrapper for Anthropic's messages API

    Args:
        max_tokens (int): Maximum tokens to generate (required)
        messages (List[Dict]): List of message objects with role and content (required)
        model (str): Model name to use (required)
        metadata (Dict, optional): Request metadata
        stop_sequences (List[str], optional): Custom stop sequences
        stream (bool, optional): Whether to stream the response
        system (str, optional): System prompt
        temperature (float, optional): Sampling temperature (0.0 to 1.0)
        thinking (Dict, optional): Extended thinking configuration
        tool_choice (Dict, optional): Tool choice configuration
        tools (List[Dict], optional): List of tool definitions
        top_k (int, optional): Top K sampling parameter
        top_p (float, optional): Nucleus sampling parameter
        **kwargs: Additional arguments

    Returns:
        Dict: Response from the API"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/replicate/common_utils.py Line: 8
class ReplicateError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openrouter/common_utils.py Line: 4
class OpenRouterException(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deepgram/common_utils.py Line: 4
class DeepgramException(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/fireworks_ai/cost_calculator.py Line: 19
def get_base_model_for_pricing(model_name: str) -> str:
    """Helper function for calculating together ai pricing.

    Returns:
    - str: model pricing category if mapped else received model name"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/fireworks_ai/cost_calculator.py Line: 55
def cost_per_token(model: str, usage: Usage) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Input:
        - model: str, the model name without provider prefix
        - usage: LiteLLM Usage block, containing anthropic caching information

    Returns:
        Tuple[float, float] - prompt_cost_in_usd, completion_cost_in_usd"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/fireworks_ai/common_utils.py Line: 11
class FireworksAIException(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/fireworks_ai/common_utils.py Line: 15
class FireworksAIMixin:
    "Common Base Config functions across Fireworks AI Endpoints"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/oobabooga/common_utils.py Line: 8
class OobaboogaError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/sambanova/chat.py Line: 12
class SambanovaConfig(OpenAIGPTConfig):
    """Reference: https://docs.sambanova.ai/cloud/api-reference/

    Below are the parameters:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/base_model_iterator.py Line: 16
class BaseModelResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/base_model_iterator.py Line: 131
class MockResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/base_model_iterator.py Line: 184
class FakeStreamResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/base_utils.py Line: 18
class BaseLLMModelInfo(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/base_utils.py Line: 72
def _convert_tool_response_to_message(tool_calls: List[ChatCompletionToolCallChunk]) -> Optional[Message]:
    "In JSON mode, Anthropic API returns JSON schema as a tool call, we need to convert it to a message to follow the OpenAI format"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/base_utils.py Line: 98
def _dict_to_response_format_helper(response_format: dict, ref_template: Optional[str]) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/base_utils.py Line: 139
def type_to_response_format_param(response_format: Optional[Union[(Type[BaseModel], dict)]], ref_template: Optional[str]) -> Optional[dict]:
    """Re-implementation of openai's 'type_to_response_format_param' function

    Used for converting pydantic object to api schema."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/base_utils.py Line: 175
def map_developer_role_to_system_role(messages: List[AllMessageValues]) -> List[AllMessageValues]:
    "Translate `developer` role to `system` role for non-OpenAI providers."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/base_aws_llm.py Line: 24
class Boto3CredentialsInfo(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/base_aws_llm.py Line: 30
class AwsAuthError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/base_aws_llm.py Line: 43
class BaseAWSLLM:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/common_utils.py Line: 16
class BedrockError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/common_utils.py Line: 20
class AmazonBedrockGlobalConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/common_utils.py Line: 95
def add_custom_header(headers):
    "Closure to capture the headers and add them."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/common_utils.py Line: 106
def init_bedrock_client(region_name, aws_access_key_id: Optional[str], aws_secret_access_key: Optional[str], aws_region_name: Optional[str], aws_bedrock_runtime_endpoint: Optional[str], aws_session_name: Optional[str], aws_profile_name: Optional[str], aws_role_name: Optional[str], aws_web_identity_token: Optional[str], extra_headers: Optional[dict], timeout: Optional[Union[(float, ?)]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/common_utils.py Line: 288
class ModelResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/common_utils.py Line: 314
def get_bedrock_tool_name(response_tool_name: str) -> str:
    """If litellm formatted the input tool name, we need to convert it back to the original name.

    Args:
        response_tool_name (str): The name of the tool as received from the response.

    Returns:
        str: The original name of the tool."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/common_utils.py Line: 332
class BedrockModelInfo(BaseLLMModelInfo):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/topaz/common_utils.py Line: 10
class TopazException(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/topaz/common_utils.py Line: 14
class TopazModelInfo(BaseLLMModelInfo):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deprecated_providers/aleph_alpha.py Line: 12
class AlephAlphaError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deprecated_providers/aleph_alpha.py Line: 25
class AlephAlphaConfig:
    """Reference: https://docs.aleph-alpha.com/api/complete/

    The `AlephAlphaConfig` class represents the configuration for the Aleph Alpha API. Here are the properties:

    - `maximum_tokens` (integer, required): The maximum number of tokens to be generated by the completion. The sum of input tokens and maximum tokens may not exceed 2048.

    - `minimum_tokens` (integer, optional; default value: 0): Generate at least this number of tokens before an end-of-text token is generated.

    - `echo` (boolean, optional; default value: false): Whether to echo the prompt in the completion.

    - `temperature` (number, nullable; default value: 0): Adjusts how creatively the model generates outputs. Use combinations of temperature, top_k, and top_p sensibly.

    - `top_k` (integer, nullable; default value: 0): Introduces randomness into token generation by considering the top k most likely options.

    - `top_p` (number, nullable; default value: 0): Adds randomness by considering the smallest set of tokens whose cumulative probability exceeds top_p.

    - `presence_penalty`, `frequency_penalty`, `sequence_penalty` (number, nullable; default value: 0): Various penalties that can reduce repetition.

    - `sequence_penalty_min_length` (integer; default value: 2): Minimum number of tokens to be considered as a sequence.

    - `repetition_penalties_include_prompt`, `repetition_penalties_include_completion`, `use_multiplicative_presence_penalty`,`use_multiplicative_frequency_penalty`,`use_multiplicative_sequence_penalty` (boolean, nullable; default value: false): Various settings that adjust how the repetition penalties are applied.

    - `penalty_bias` (string, nullable): Text used in addition to the penalized tokens for repetition penalties.

    - `penalty_exceptions` (string[], nullable): Strings that may be generated without penalty.

    - `penalty_exceptions_include_stop_sequences` (boolean, nullable; default value: true): Include all stop_sequences in penalty_exceptions.

    - `best_of` (integer, nullable; default value: 1): The number of completions will be generated on the server side.

    - `n` (integer, nullable; default value: 1): The number of completions to return.

    - `logit_bias` (object, nullable): Adjust the logit scores before sampling.

    - `log_probs` (integer, nullable): Number of top log probabilities for each token generated.

    - `stop_sequences` (string[], nullable): List of strings that will stop generation if they're generated.

    - `tokens` (boolean, nullable; default value: false): Flag indicating whether individual tokens of the completion should be returned or not.

    - `raw_completion` (boolean; default value: false): if True, the raw completion of the model will be returned.

    - `disable_optimizations` (boolean, nullable; default value: false): Disables any applied optimizations to both your prompt and completion.

    - `completion_bias_inclusion`, `completion_bias_exclusion` (string[], default value: []): Set of strings to bias the generation of tokens.

    - `completion_bias_inclusion_first_token_only`, `completion_bias_exclusion_first_token_only` (boolean; default value: false): Consider only the first token for the completion_bias_inclusion/exclusion.

    - `contextual_control_threshold` (number, nullable): Control over how similar tokens are controlled.

    - `control_log_additive` (boolean; default value: true): Method of applying control to attention scores."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deprecated_providers/aleph_alpha.py Line: 172
def validate_environment(api_key):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deprecated_providers/aleph_alpha.py Line: 182
def completion(model: str, messages: list, api_base: str, model_response: ModelResponse, print_verbose: Callable, encoding, api_key, logging_obj, optional_params: dict, litellm_params, logger_fn, default_max_tokens_to_sample):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deprecated_providers/aleph_alpha.py Line: 305
def embedding():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deprecated_providers/palm.py Line: 13
class PalmError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deprecated_providers/palm.py Line: 27
class PalmConfig:
    """Reference: https://developers.generativeai.google/api/python/google/generativeai/chat

    The class `PalmConfig` provides configuration for the Palm's API interface. Here are the parameters:

    - `context` (string): Text that should be provided to the model first, to ground the response. This could be a prompt to guide the model's responses.

    - `examples` (list): Examples of what the model should generate. They are treated identically to conversation messages except that they take precedence over the history in messages if the total input size exceeds the model's input_token_limit.

    - `temperature` (float): Controls the randomness of the output. Must be positive. Higher values produce a more random and varied response. A temperature of zero will be deterministic.

    - `candidate_count` (int): Maximum number of generated response messages to return. This value must be between [1, 8], inclusive. Only unique candidates are returned.

    - `top_k` (int): The API uses combined nucleus and top-k sampling. `top_k` sets the maximum number of tokens to sample from on each step.

    - `top_p` (float): The API uses combined nucleus and top-k sampling. `top_p` configures the nucleus sampling. It sets the maximum cumulative probability of tokens to sample from.

    - `max_output_tokens` (int): Sets the maximum number of tokens to be returned in the output"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deprecated_providers/palm.py Line: 90
def completion(model: str, messages: list, model_response: ModelResponse, print_verbose: Callable, api_key, encoding, logging_obj, optional_params: dict, litellm_params, logger_fn):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deprecated_providers/palm.py Line: 199
def embedding():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/infinity/common_utils.py Line: 7
class InfinityError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/snowflake/common_utils.py Line: 4
class SnowflakeBase:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/common_utils.py Line: 17
class AnthropicError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/common_utils.py Line: 27
class AnthropicModelInfo(BaseLLMModelInfo):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/common_utils.py Line: 197
def process_anthropic_headers(headers: Union[(?, dict)]) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/cost_calculation.py Line: 12
def cost_per_token(model: str, usage: Usage) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Input:
        - model: str, the model name without provider prefix
        - usage: LiteLLM Usage block, containing anthropic caching information

    Returns:
        Tuple[float, float] - prompt_cost_in_usd, completion_cost_in_usd"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai_like/common_utils.py Line: 6
class OpenAILikeError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai_like/common_utils.py Line: 17
class OpenAILikeBase:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cerebras/chat.py Line: 12
class CerebrasConfig(OpenAIGPTConfig):
    """Reference: https://inference-docs.cerebras.ai/api-reference/chat-completions

    Below are the parameters:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/triton/common_utils.py Line: 8
class TritonError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/clarifai/common_utils.py Line: 4
class ClarifaiError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/azure.py Line: 41
class AzureOpenAIAssistantsAPIConfig:
    "Reference: https://learn.microsoft.com/en-us/azure/ai-services/openai/assistants-reference-messages?tabs=python#create-message"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/azure.py Line: 103
def _check_dynamic_azure_params(azure_client_params: dict, azure_client: Optional[Union[(AzureOpenAI, AsyncAzureOpenAI)]]) -> bool:
    """Returns True if user passed in client params != initialized azure client

    Currently only implemented for api version"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/azure.py Line: 124
class AzureChatCompletion(BaseAzureLLM, BaseLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/audio_transcriptions.py Line: 19
class AzureAudioTranscription(AzureChatCompletion):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/common_utils.py Line: 21
class AzureOpenAIError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/common_utils.py Line: 41
def process_azure_headers(headers: Union[(?, dict)]) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/common_utils.py Line: 64
def get_azure_ad_token_from_entra_id(tenant_id: str, client_id: str, client_secret: str, scope: str="https://cognitiveservices.azure.com/.default") -> Callable[(?, str)]:
    """Get Azure AD token provider from `client_id`, `client_secret`, and `tenant_id`

    Args:
        tenant_id: str
        client_id: str
        client_secret: str
        scope: str

    Returns:
        callable that returns a bearer token."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/common_utils.py Line: 120
def get_azure_ad_token_from_username_password(client_id: str, azure_username: str, azure_password: str, scope: str="https://cognitiveservices.azure.com/.default") -> Callable[(?, str)]:
    """Get Azure AD token provider from `client_id`, `azure_username`, and `azure_password`

    Args:
        client_id: str
        azure_username: str
        azure_password: str
        scope: str

    Returns:
        callable that returns a bearer token."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/common_utils.py Line: 161
def get_azure_ad_token_from_oidc(azure_ad_token: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/common_utils.py Line: 236
def select_azure_base_url_or_endpoint(azure_client_params: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/common_utils.py Line: 248
class BaseAzureLLM(BaseOpenAILLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/assistants.py Line: 24
class AzureAssistantsAPI(BaseAzureLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/cost_calculation.py Line: 13
def cost_per_token(model: str, usage: Usage, response_time_ms: Optional[float]) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Input:
        - model: str, the model name without provider prefix
        - usage: LiteLLM Usage block, containing anthropic caching information

    Returns:
        Tuple[float, float] - prompt_cost_in_usd, completion_cost_in_usd"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/openai.py Line: 60
class MistralEmbeddingConfig:
    "Reference: https://docs.mistral.ai/api/#operation/createEmbedding"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/openai.py Line: 103
class OpenAIConfig(BaseConfig):
    """Reference: https://platform.openai.com/docs/api-reference/chat/create

    The class `OpenAIConfig` provides configuration for the OpenAI's Chat API interface. Below are the parameters:

    - `frequency_penalty` (number or null): Defaults to 0. Allows a value between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, thereby minimizing repetition.

    - `function_call` (string or object): This optional parameter controls how the model calls functions.

    - `functions` (array): An optional parameter. It is a list of functions for which the model may generate JSON inputs.

    - `logit_bias` (map): This optional parameter modifies the likelihood of specified tokens appearing in the completion.

    - `max_tokens` (integer or null): This optional parameter helps to set the maximum number of tokens to generate in the chat completion. OpenAI has now deprecated in favor of max_completion_tokens, and is not compatible with o1 series models.

    - `max_completion_tokens` (integer or null): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.

    - `n` (integer or null): This optional parameter helps to set how many chat completion choices to generate for each input message.

    - `presence_penalty` (number or null): Defaults to 0. It penalizes new tokens based on if they appear in the text so far, hence increasing the model's likelihood to talk about new topics.

    - `stop` (string / array / null): Specifies up to 4 sequences where the API will stop generating further tokens.

    - `temperature` (number or null): Defines the sampling temperature to use, varying between 0 and 2.

    - `top_p` (number or null): An alternative to sampling with temperature, used for nucleus sampling."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/openai.py Line: 311
class OpenAIChatCompletionResponseIterator(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/openai.py Line: 322
class OpenAIChatCompletion(BaseLLM, BaseOpenAILLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/openai.py Line: 1437
class OpenAIFilesAPI(BaseLLM):
    """OpenAI methods to support for batches
    - create_file()
    - retrieve_file()
    - list_files()
    - delete_file()
    - file_content()
    - update_file()"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/openai.py Line: 1719
class OpenAIBatchesAPI(BaseLLM):
    """OpenAI methods to support for batches
    - create_batch()
    - retrieve_batch()
    - cancel_batch()
    - list_batch()"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/openai.py Line: 1945
class OpenAIAssistantsAPI(BaseLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/common_utils.py Line: 18
class OpenAIError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/common_utils.py Line: 53
def drop_params_from_unprocessable_entity_error(e: Union[(?, ?)], data: Dict[(str, Any)]) -> Dict[(str, Any)]:
    """Helper function to read OpenAI UnprocessableEntityError and drop the params that raised an error from the error message.

    Args:
    e (UnprocessableEntityError): The UnprocessableEntityError exception
    data (Dict[str, Any]): The original data dictionary containing all parameters

    Returns:
    Dict[str, Any]: A new dictionary with invalid parameters removed"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/common_utils.py Line: 101
class BaseOpenAILLM:
    "Base class for OpenAI LLMs for getting their httpx clients and SSL verification settings"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/cost_calculation.py Line: 14
def cost_router(call_type: CallTypes) -> Literal[(?, ?)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/cost_calculation.py Line: 21
def cost_per_token(model: str, usage: Usage) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Input:
        - model: str, the model name without provider prefix
        - usage: LiteLLM Usage block, containing anthropic caching information

    Returns:
        Tuple[float, float] - prompt_cost_in_usd, completion_cost_in_usd"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/cost_calculation.py Line: 81
def cost_per_second(model: str, custom_llm_provider: Optional[str], duration: float) -> Tuple[(float, float)]:
    """Calculates the cost per second for a given model, prompt tokens, and completion tokens.

    Input:
        - model: str, the model name without provider prefix
        - custom_llm_provider: str, the custom llm provider
        - duration: float, the duration of the response in seconds

    Returns:
        Tuple[float, float] - prompt_cost_in_usd, completion_cost_in_usd"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama/common_utils.py Line: 8
class OllamaError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama/common_utils.py Line: 15
def _convert_image(image):
    """Convert image to base64 encoded image if not already in base64 format

    If image is already in base64 format AND is a jpeg/png, return it

    If image is not JPEG/PNG, convert it to JPEG base64 format"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/gemini/cost_calculator.py Line: 13
def cost_per_token(model: str, usage: Usage) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Follows the same logic as Anthropic's cost per token calculation."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/gemini/common_utils.py Line: 12
class GeminiError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/gemini/common_utils.py Line: 16
class GeminiModelInfo(BaseLLMModelInfo):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/common_utils.py Line: 13
class CohereError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/common_utils.py Line: 18
def validate_environment(headers: dict, model: str, messages: List[AllMessageValues], optional_params: dict, api_key: Optional[str]) -> dict:
    """Return headers to use for cohere chat completion request

    Cohere API Ref: https://docs.cohere.com/reference/chat
    Expected headers:
    {
        "Request-Source": "unspecified:litellm",
        "accept": "application/json",
        "content-type": "application/json",
        "Authorization": "bearer $CO_API_KEY"
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/common_utils.py Line: 49
class ModelResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/petals/common_utils.py Line: 8
class PetalsError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/nlp_cloud/common_utils.py Line: 8
class NLPCloudError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/sagemaker/common_utils.py Line: 15
class SagemakerError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/sagemaker/common_utils.py Line: 25
class AWSEventStreamDecoder:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/sagemaker/common_utils.py Line: 207
def get_response_stream_shape():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deepseek/cost_calculator.py Line: 13
def cost_per_token(model: str, usage: Usage) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Follows the same logic as Anthropic's cost per token calculation."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/predibase/common_utils.py Line: 8
class PredibaseError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/together_ai/cost_calculator.py Line: 22
def get_model_params_and_category(model_name, call_type: CallTypes) -> str:
    """Helper function for calculating together ai pricing.

    Returns
    - str - model pricing category if mapped else received model name"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/together_ai/cost_calculator.py Line: 62
def get_model_params_and_category_embeddings(model_name) -> str:
    """Helper function for calculating together ai embedding pricing.

    Returns
    - str - model pricing category if mapped else received model name"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/together_ai/chat.py Line: 16
class TogetherAIConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/mistral/mistral_chat_transformation.py Line: 21
class MistralConfig(OpenAIGPTConfig):
    """Reference: https://docs.mistral.ai/api/

    The class `MistralConfig` provides configuration for the Mistral's Chat API interface. Below are the parameters:

    - `temperature` (number or null): Defines the sampling temperature to use, varying between 0 and 2. API Default - 0.7.

    - `top_p` (number or null): An alternative to sampling with temperature, used for nucleus sampling. API Default - 1.

    - `max_tokens` (integer or null): This optional parameter helps to set the maximum number of tokens to generate in the chat completion. API Default - null.

    - `tools` (list or null): A list of available tools for the model. Use this to specify functions for which the model can generate JSON inputs.

    - `tool_choice` (string - 'auto'/'any'/'none' or null): Specifies if/how functions are called. If set to none the model won't call a function and will generate a message instead. If set to auto the model can choose to either generate a message or call a function. If set to any the model is forced to call a function. Default - 'auto'.

    - `stop` (string or array of strings): Stop generation if this token is detected. Or if one of these tokens is detected when providing an array

    - `random_seed` (integer or null): The seed to use for random sampling. If set, different calls will generate deterministic results.

    - `safe_prompt` (boolean): Whether to inject a safety prompt before all conversations. API Default - 'false'.

    - `response_format` (object or null): An object specifying the format that the model must output. Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/databricks/streaming_utils.py Line: 14
class ModelResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/databricks/cost_calculator.py Line: 12
def cost_per_token(model: str, usage: Usage) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Input:
        - model: str, the model name without provider prefix
        - usage: LiteLLM Usage block, containing anthropic caching information

    Returns:
        Tuple[float, float] - prompt_cost_in_usd, completion_cost_in_usd"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/databricks/common_utils.py Line: 6
class DatabricksException(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/databricks/common_utils.py Line: 10
class DatabricksBase:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/huggingface/common_utils.py Line: 12
class HuggingFaceError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/huggingface/common_utils.py Line: 45
def output_parser(generated_text: str):
    """Parse the output text to remove any special characters. In our current approach we just check for ChatML tokens.

    Initial issue that prompted this - https://github.com/BerriAI/litellm/issues/763"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/huggingface/common_utils.py Line: 61
def _fetch_inference_provider_mapping(model: str) -> dict:
    """Fetch provider mappings for a model from the Hugging Face Hub.

    Args:
        model: The model identifier (e.g., 'meta-llama/Llama-2-7b')

    Returns:
        dict: The inference provider mapping for the model

    Raises:
        ValueError: If no provider mapping is found
        HuggingFaceError: If the API request fails"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/cost_calculator.py Line: 30
def cost_router(model: str, custom_llm_provider: str, call_type: Union[(Literal[(?, ?)], str)]) -> Literal[(?, ?)]:
    """Route the cost calc to the right place, based on model/call_type/etc.

    Returns
        - str, the specific google cost calc function it should route to."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/cost_calculator.py Line: 58
def cost_per_character(model: str, custom_llm_provider: str, usage: Usage, prompt_characters: Optional[float], completion_characters: Optional[float]) -> Tuple[(float, float)]:
    """Calculates the cost per character for a given VertexAI model, input messages, and response object.

    Input:
        - model: str, the model name without provider prefix
        - custom_llm_provider: str, "vertex_ai-*"
        - prompt_characters: float, the number of input characters
        - completion_characters: float, the number of output characters

    Returns:
        Tuple[float, float] - prompt_cost_in_usd, completion_cost_in_usd

    Raises:
        Exception if model requires >128k pricing, but model cost not mapped"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/cost_calculator.py Line: 184
def _handle_128k_pricing(model_info: ModelInfo, usage: Usage) -> Tuple[(float, float)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/cost_calculator.py Line: 222
def cost_per_token(model: str, custom_llm_provider: str, usage: Usage) -> Tuple[(float, float)]:
    """Calculates the cost per token for a given model, prompt tokens, and completion tokens.

    Input:
        - model: str, the model name without provider prefix
        - custom_llm_provider: str, either "vertex_ai-*" or "gemini"
        - prompt_tokens: float, the number of input tokens
        - completion_tokens: float, the number of output tokens

    Returns:
        Tuple[float, float] - prompt_cost_in_usd, completion_cost_in_usd

    Raises:
        Exception if model requires >128k pricing, but model cost not mapped"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py Line: 24
class VertexBase:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py Line: 16
class VertexAIError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py Line: 29
class TextStreamer:
    "Fake streaming iterator for Vertex AI Model Garden calls"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py Line: 61
def _get_client_cache_key(model: str, vertex_project: Optional[str], vertex_location: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py Line: 68
def _get_client_from_cache(client_cache_key: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py Line: 72
def _set_client_in_cache(client_cache_key: str, vertex_llm_model: Any):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py Line: 80
def completion(model: str, messages: list, model_response: ModelResponse, print_verbose: Callable, encoding, logging_obj, optional_params: dict, vertex_project, vertex_location, vertex_credentials, litellm_params, logger_fn, acompletion: bool):
    """NON-GEMINI/ANTHROPIC CALLS.

    This is the handler for OLDER PALM MODELS and VERTEX AI MODEL GARDEN

    For Vertex AI Anthropic: `vertex_anthropic.py`
    For Gemini: `vertex_httpx.py`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py Line: 486
async def async_completion(llm_model, mode: str, prompt: str, model: str, messages: list, model_response: ModelResponse, request_str: str, print_verbose: Callable, logging_obj, encoding, client_options, instances, vertex_project, vertex_location, safety_settings, **optional_params):
    "Add support for acompletion calls for gemini-pro"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py Line: 649
async def async_streaming(llm_model, mode: str, prompt: str, model: str, model_response: ModelResponse, messages: list, print_verbose: Callable, logging_obj, request_str: str, encoding, client_options, instances, vertex_project, vertex_location, safety_settings, **optional_params):
    "Add support for async streaming calls for gemini-pro"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 14
class VertexAIError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 24
def get_supports_system_message(model: str, custom_llm_provider: Literal[(?, ?, ?)]) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 49
def get_supports_response_schema(model: str, custom_llm_provider: Literal[(?, ?, ?)]) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 70
def _get_vertex_url(mode: all_gemini_url_modes, model: str, stream: Optional[bool], vertex_project: Optional[str], vertex_location: Optional[str], vertex_api_version: Literal[(?, ?)]) -> Tuple[(str, str)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 115
def _get_gemini_url(mode: all_gemini_url_modes, model: str, stream: Optional[bool], gemini_api_key: Optional[str]) -> Tuple[(str, str)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 153
def _check_text_in_content(parts: List[PartType]) -> bool:
    """check that user_content has 'text' parameter.
        - Known Vertex Error: Unable to submit request because it must have a text parameter.
        - 'text' param needs to be len > 0
        - Relevant Issue: https://github.com/BerriAI/litellm/issues/5515"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 168
def _build_vertex_schema(parameters: dict, add_property_ordering: bool):
    """This is a modified version of https://github.com/google-gemini/generative-ai-python/blob/8f77cc6ac99937cd3a81299ecf79608b91b06bbb/google/generativeai/types/content_types.py#L419

    Updates the input parameters, removing extraneous fields, adjusting types, unwinding $defs, and adding propertyOrdering if specified, returning the updated parameters.

    Parameters:
        parameters: dict - the json schema to build from
        add_property_ordering: bool - whether to add propertyOrdering to the schema. This is only applicable to schemas for structured outputs. See
          set_schema_property_ordering for more details.
    Returns:
        parameters: dict - the input parameters, modified in place"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 205
def set_schema_property_ordering(schema: Dict[(str, Any)], depth: int) -> Dict[(str, Any)]:
    """vertex ai and generativeai apis order output of fields alphabetically, unless you specify the order.
    python dicts retain order, so we just use that. Note that this field only applies to structured outputs, and not tools.
    Function tools are not afflicted by the same alphabetical ordering issue, (the order of keys returned seems to be arbitrary, up to the model)
    https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.cachedContents#Schema.FIELDS.property_ordering

    Args:
        schema: The schema dictionary to process
        depth: Current recursion depth to prevent infinite loops"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 234
def filter_schema_fields(schema_dict: Dict[(str, Any)], valid_fields: Set[str], processed) -> Dict[(str, Any)]:
    "Recursively filter a schema dictionary to keep only valid fields."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 274
def convert_anyof_null_to_nullable(schema, depth):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 311
def add_object_type(schema):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 325
def strip_field(schema, field_name: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 338
def _convert_vertex_datetime_to_openai_datetime(vertex_datetime: str) -> int:
    """Converts a Vertex AI datetime string to an OpenAI datetime integer

    vertex_datetime: str = "2024-12-04T21:53:12.120184Z"
    returns: int = 1722729192"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 353
def get_vertex_project_id_from_url(url: str) -> Optional[str]:
    """Get the vertex project id from the url

    `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:streamGenerateContent`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 363
def get_vertex_location_from_url(url: str) -> Optional[str]:
    """Get the vertex location from the url

    `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:streamGenerateContent`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 373
def replace_project_and_location_in_route(requested_route: str, vertex_project: str, vertex_location: str) -> str:
    "Replace project and location values in the route with the provided values"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/common_utils.py Line: 388
def construct_target_url(base_url: str, requested_route: str, vertex_location: Optional[str], vertex_project: Optional[str]) -> ?:
    """Allow user to specify their own project id / location.

    If missing, use defaults

    Handle cachedContent scenario - https://github.com/BerriAI/litellm/issues/5460

    Constructed Url:
    POST https://LOCATION-aiplatform.googleapis.com/{version}/projects/PROJECT_ID/locations/LOCATION/cachedContents"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vllm/common_utils.py Line: 13
class VLLMError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vllm/common_utils.py Line: 17
class VLLMModelInfo(BaseLLMModelInfo):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/common_utils.py Line: 15
class WatsonXAIError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/common_utils.py Line: 28
def get_watsonx_iam_url():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/common_utils.py Line: 34
def generate_iam_token(api_key, **params) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/common_utils.py Line: 72
def _generate_watsonx_token(api_key: Optional[str], token: Optional[str]) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/common_utils.py Line: 79
def _get_api_params(params: dict) -> WatsonXAPIParams:
    "Find watsonx.ai credentials in the params or environment variables and return the headers for authentication."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/common_utils.py Line: 130
def convert_watsonx_messages_to_prompt(model: str, messages: List[AllMessageValues], provider: str, custom_prompt_dict: Dict) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/common_utils.py Line: 161
class IBMWatsonXMixin:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/nvidia_nim/chat.py Line: 16
class NvidiaNimConfig(OpenAIGPTConfig):
    """Reference: https://docs.api.nvidia.com/nim/reference/databricks-dbrx-instruct-infer

    The class `NvidiaNimConfig` provides configuration for the Nvidia NIM's Chat Completions API interface. Below are the parameters:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/nvidia_nim/embed.py Line: 15
class NvidiaNimEmbeddingConfig:
    "Reference: https://docs.api.nvidia.com/nim/reference/nvidia-nv-embedqa-e5-v5-infer"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/xai/common_utils.py Line: 11
class XAIModelInfo(BaseLLMModelInfo):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py Line: 37
def mask_sensitive_info(error_message):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py Line: 65
class MaskedHTTPStatusError:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py Line: 91
class AsyncHTTPHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py Line: 467
class HTTPHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py Line: 715
def get_async_httpx_client(llm_provider: Union[(LlmProviders, httpxSpecialProvider)], params: Optional[dict]) -> AsyncHTTPHandler:
    """Retrieves the async HTTP client from the cache
    If not present, creates a new client

    Caches the new client and returns it."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py Line: 753
def _get_httpx_client(params: Optional[dict]) -> HTTPHandler:
    """Retrieves the HTTP client from the cache
    If not present, creates a new client

    Caches the new client and returns it."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_httpx/httpx_handler.py Line: 15
class HTTPHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_httpx/aiohttp_handler.py Line: 34
class BaseLLMAIOHTTPHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py Line: 52
class BaseLLMHTTPHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure_ai/chat/transformation.py Line: 23
class AzureFoundryErrorStrings(str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure_ai/chat/transformation.py Line: 27
class AzureAIStudioConfig(OpenAIConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure_ai/rerank/transformation.py Line: 16
class AzureAIRerankConfig(CohereRerankConfig):
    "Azure AI Rerank - Follows the same Spec as Cohere Rerank"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure_ai/embed/handler.py Line: 19
class AzureAIEmbedding(OpenAIChatCompletion):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure_ai/embed/cohere_transformation.py Line: 20
class AzureAICohereConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/replicate/chat/handler.py Line: 24
def handle_prediction_response_streaming(prediction_url, api_token, print_verbose, headers: dict, http_client: HTTPHandler):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/replicate/chat/handler.py Line: 71
async def async_handle_prediction_response_streaming(prediction_url, api_token, print_verbose, headers: dict, http_client: AsyncHTTPHandler):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/replicate/chat/handler.py Line: 122
def completion(model: str, messages: list, api_base: str, model_response: ModelResponse, print_verbose: Callable, optional_params: dict, litellm_params: dict, logging_obj, api_key, encoding, custom_prompt_dict, logger_fn, acompletion, headers) -> Union[(ModelResponse, CustomStreamWrapper)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/replicate/chat/handler.py Line: 239
async def async_completion(model_response: ModelResponse, model: str, messages: List[AllMessageValues], encoding, optional_params: dict, litellm_params: dict, version_id, input_data, api_key, api_base, logging_obj, print_verbose, headers: dict) -> Union[(ModelResponse, CustomStreamWrapper)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/replicate/chat/transformation.py Line: 29
class ReplicateConfig(BaseConfig):
    """Reference: https://replicate.com/meta/llama-2-70b-chat/api
    - `prompt` (string): The prompt to send to the model.

    - `system_prompt` (string): The system prompt to send to the model. This is prepended to the prompt and helps guide system behavior. Default value: `You are a helpful assistant`.

    - `max_new_tokens` (integer): Maximum number of tokens to generate. Typically, a word is made up of 2-3 tokens. Default value: `128`.

    - `min_new_tokens` (integer): Minimum number of tokens to generate. To disable, set to `-1`. A word is usually 2-3 tokens. Default value: `-1`.

    - `temperature` (number): Adjusts the randomness of outputs. Values greater than 1 increase randomness, 0 is deterministic, and 0.75 is a reasonable starting value. Default value: `0.75`.

    - `top_p` (number): During text decoding, it samples from the top `p` percentage of most likely tokens. Reduce this to ignore less probable tokens. Default value: `0.9`.

    - `top_k` (integer): During text decoding, samples from the top `k` most likely tokens. Reduce this to ignore less probable tokens. Default value: `50`.

    - `stop_sequences` (string): A comma-separated list of sequences to stop generation at. For example, inputting '<end>,<stop>' will cease generation at the first occurrence of either 'end' or '<stop>'.

    - `seed` (integer): This is the seed for the random generator. Leave it blank to randomize the seed.

    - `debug` (boolean): If set to `True`, it provides debugging output in logs.

    Please note that Replicate's mapping of these parameters can be inconsistent across different models, indicating that not all of these parameters may be available for use with all models."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openrouter/chat/transformation.py Line: 23
class OpenrouterConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openrouter/chat/transformation.py Line: 94
class OpenRouterChatCompletionStreamingHandler(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deepgram/audio_transcription/transformation.py Line: 25
class DeepgramAudioTranscriptionConfig(BaseAudioTranscriptionConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/hosted_vllm/chat/transformation.py Line: 23
class HostedVLLMChatConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/fireworks_ai/chat/transformation.py Line: 33
class FireworksAIConfig(OpenAIGPTConfig):
    """Reference: https://docs.fireworks.ai/api-reference/post-chatcompletions

    The class `FireworksAIConfig` provides configuration for the Fireworks's Chat Completions API interface. Below are the parameters:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/fireworks_ai/audio_transcription/transformation.py Line: 11
class FireworksAIAudioTranscriptionConfig(FireworksAIMixin, OpenAIWhisperAudioTranscriptionConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/fireworks_ai/completion/transformation.py Line: 10
class FireworksAITextCompletionConfig(FireworksAIMixin, BaseTextCompletionConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/fireworks_ai/embed/fireworks_ai_transformation.py Line: 9
class FireworksAIEmbeddingConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/oobabooga/chat/transformation.py Line: 21
class OobaboogaConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/oobabooga/chat/oobabooga.py Line: 14
def completion(model: str, messages: list, api_base: Optional[str], model_response: ModelResponse, print_verbose: Callable, encoding, api_key, logging_obj, optional_params: dict, litellm_params: dict, custom_prompt_dict, logger_fn, default_max_tokens_to_sample):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/oobabooga/chat/oobabooga.py Line: 88
def embedding(model: str, input: list, model_response: EmbeddingResponse, api_key: Optional[str], api_base: Optional[str], logging_obj: Any, optional_params: dict, encoding):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/chat/transformation.py Line: 47
class BaseLLMException(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/chat/transformation.py Line: 78
class BaseConfig(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/rerank/transformation.py Line: 19
class BaseRerankConfig(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/audio_transcription/transformation.py Line: 21
class BaseAudioTranscriptionConfig(BaseConfig, ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/anthropic_messages/transformation.py Line: 12
class BaseAnthropicMessagesConfig(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/embedding/transformation.py Line: 18
class BaseEmbeddingConfig(BaseConfig, ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/image_variations/transformation.py Line: 27
class BaseImageVariationConfig(BaseConfig, ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/completion/transformation.py Line: 18
class BaseTextCompletionConfig(BaseConfig, ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/files/transformation.py Line: 24
class BaseFilesConfig(BaseConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/base_llm/responses/transformation.py Line: 28
class BaseResponsesAPIConfig(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_handler.py Line: 81
class AmazonCohereChatConfig:
    "Reference - https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command-r-plus.html"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_handler.py Line: 182
async def make_call(client: Optional[AsyncHTTPHandler], api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj: Logging, fake_stream: bool, json_mode: Optional[bool], bedrock_invoke_provider: Optional[?]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_handler.py Line: 270
def make_sync_call(client: Optional[HTTPHandler], api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj: Logging, fake_stream: bool, json_mode: Optional[bool], bedrock_invoke_provider: Optional[?]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_handler.py Line: 350
class BedrockLLM(BaseAWSLLM):
    """Example call

    ```
    curl --location --request POST 'https://bedrock-runtime.{aws_region_name}.amazonaws.com/model/{bedrock_model_name}/invoke'         --header 'Content-Type: application/json'         --header 'Accept: application/json'         --user "$AWS_ACCESS_KEY_ID":"$AWS_SECRET_ACCESS_KEY"         --aws-sigv4 "aws:amz:us-east-1:bedrock"         --data-raw '{
        "prompt": "Hi",
        "temperature": 0,
        "p": 0.9,
        "max_tokens": 4096
        }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_handler.py Line: 1206
def get_response_stream_shape():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_handler.py Line: 1220
class AWSEventStreamDecoder:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_handler.py Line: 1540
class AmazonAnthropicClaudeStreamDecoder(AWSEventStreamDecoder):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_handler.py Line: 1563
class AmazonDeepSeekR1StreamDecoder(AWSEventStreamDecoder):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_handler.py Line: 1583
class MockResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/converse_handler.py Line: 23
def make_sync_call(client: Optional[HTTPHandler], api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj: LiteLLMLoggingObject, json_mode: Optional[bool], fake_stream: bool):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/converse_handler.py Line: 83
class BedrockConverseLLM(BaseAWSLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/converse_transformation.py Line: 43
class AmazonConverseConfig(BaseConfig):
    """Reference - https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html
    #2 - https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/rerank/handler.py Line: 28
class BedrockRerankHandler(BaseAWSLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/rerank/transformation.py Line: 31
class BedrockRerankConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/image/amazon_nova_canvas_transformation.py Line: 18
class AmazonNovaCanvasConfig:
    "Reference: https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/model-catalog/serverless/amazon.nova-canvas-v1:0"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/image/amazon_stability1_transformation.py Line: 9
class AmazonStabilityConfig:
    """Reference: https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers?model=stability.stable-diffusion-xl-v0

    Supported Params for the Amazon / Stable Diffusion models:

    - `cfg_scale` (integer): Default `7`. Between [ 0 .. 35 ]. How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)

    - `seed` (float): Default: `0`. Between [ 0 .. 4294967295 ]. Random noise seed (omit this option or use 0 for a random seed)

    - `steps` (array of strings): Default `30`. Between [ 10 .. 50 ]. Number of diffusion steps to run.

    - `width` (integer): Default: `512`. multiple of 64 >= 128. Width of the image to generate, in pixels, in an increment divible by 64.
        Engine-specific dimension validation:

        - SDXL Beta: must be between 128x128 and 512x896 (or 896x512); only one dimension can be greater than 512.
        - SDXL v0.9: must be one of 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640, 640x1536, 768x1344, 832x1216, or 896x1152
        - SDXL v1.0: same as SDXL v0.9
        - SD v1.6: must be between 320x320 and 1536x1536

    - `height` (integer): Default: `512`. multiple of 64 >= 128. Height of the image to generate, in pixels, in an increment divible by 64.
        Engine-specific dimension validation:

        - SDXL Beta: must be between 128x128 and 512x896 (or 896x512); only one dimension can be greater than 512.
        - SDXL v0.9: must be one of 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640, 640x1536, 768x1344, 832x1216, or 896x1152
        - SDXL v1.0: same as SDXL v0.9
        - SD v1.6: must be between 320x320 and 1536x1536"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/image/image_handler.py Line: 29
class BedrockImagePreparedRequest(BaseModel):
    "Internal/Helper class for preparing the request for bedrock image generation"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/image/image_handler.py Line: 40
class BedrockImageGeneration(BaseAWSLLM):
    "Bedrock Image Generation handler"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/image/cost_calculator.py Line: 7
def cost_calculator(model: str, image_response: ImageResponse, size: Optional[str], optional_params: Optional[dict]) -> float:
    """Bedrock image generation cost calculator

    Handles both Stability 1 and Stability 3 models"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/image/amazon_stability3_transformation.py Line: 13
class AmazonStability3Config:
    """Reference: https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers?model=stability.stable-diffusion-xl-v0

    Stability API Ref: https://platform.stability.ai/docs/api-reference#tag/Generate/paths/~1v2beta~1stable-image~1generate~1sd3/post"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/embed/amazon_titan_v2_transformation.py Line: 22
class AmazonTitanV2Config:
    """Reference: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-embed-text.html

    normalize: boolean - flag indicating whether or not to normalize the output embeddings. Defaults to true
    dimensions: int - The number of dimensions the output embeddings should have. The following values are accepted: 1024 (default), 512, 256."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/embed/amazon_titan_multimodal_transformation.py Line: 20
class AmazonTitanMultimodalEmbeddingG1Config:
    "Reference - https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-embed-mm.html"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/embed/amazon_titan_g1_transformation.py Line: 22
class AmazonTitanG1Config:
    "Reference: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-embed-text.html"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/embed/cohere_transformation.py Line: 13
class BedrockCohereEmbeddingConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/embed/embedding.py Line: 33
class BedrockEmbedding(BaseAWSLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/amazon_ai21_transformation.py Line: 10
class AmazonAI21Config(AmazonInvokeConfig, BaseConfig):
    """Reference: https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers?model=j2-ultra

    Supported Params for the Amazon / AI21 models:

    - `maxTokens` (int32): The maximum number of tokens to generate per result. Optional, default is 16. If no `stopSequences` are given, generation stops after producing `maxTokens`.

    - `temperature` (float): Modifies the distribution from which tokens are sampled. Optional, default is 0.7. A value of 0 essentially disables sampling and results in greedy decoding.

    - `topP` (float): Used for sampling tokens from the corresponding top percentile of probability mass. Optional, default is 1. For instance, a value of 0.9 considers only tokens comprising the top 90% probability mass.

    - `stopSequences` (array of strings): Stops decoding if any of the input strings is generated. Optional.

    - `frequencyPenalty` (object): Placeholder for frequency penalty object.

    - `presencePenalty` (object): Placeholder for presence penalty object.

    - `countPenalty` (object): Placeholder for count penalty object."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/amazon_deepseek_transformation.py Line: 28
class AmazonDeepSeekR1Config(AmazonLlamaConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/amazon_deepseek_transformation.py Line: 86
class AmazonDeepseekR1ResponseIterator(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/amazon_titan_transformation.py Line: 12
class AmazonTitanConfig(AmazonInvokeConfig, BaseConfig):
    """Reference: https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers?model=titan-text-express-v1

    Supported Params for the Amazon Titan models:

    - `maxTokenCount` (integer) max tokens,
    - `stopSequences` (string[]) list of stop sequence strings
    - `temperature` (float) temperature for model,
    - `topP` (int) top p for model"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/anthropic_claude2_transformation.py Line: 9
class AmazonAnthropicConfig(AmazonInvokeConfig):
    """Reference: https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers?model=claude

    Supported Params for the Amazon / Anthropic models:

    - `max_tokens_to_sample` (integer) max tokens,
    - `temperature` (float) model temperature,
    - `top_k` (integer) top k,
    - `top_p` (integer) top p,
    - `stop_sequences` (string[]) list of stop sequences - e.g. ["\n\nHuman:"],
    - `anthropic_version` (string) version of anthropic for bedrock - e.g. "bedrock-2023-05-31""""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/amazon_nova_transformation.py Line: 23
class AmazonInvokeNovaConfig(AmazonInvokeConfig, AmazonConverseConfig):
    "Config for sending `nova` requests to `/bedrock/invoke/`"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/amazon_cohere_transformation.py Line: 10
class AmazonCohereConfig(AmazonInvokeConfig, CohereChatConfig):
    """Reference: https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers?model=command

    Supported Params for the Amazon / Cohere models:

    - `max_tokens` (integer) max tokens,
    - `temperature` (float) model temperature,
    - `return_likelihood` (string) n/a"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/amazon_llama_transformation.py Line: 10
class AmazonLlamaConfig(AmazonInvokeConfig, BaseConfig):
    """Reference: https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers?model=meta.llama2-13b-chat-v1

    Supported Params for the Amazon / Meta Llama models:

    - `max_gen_len` (integer) max tokens,
    - `temperature` (float) temperature for model,
    - `top_p` (float) top p for model"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/base_invoke_transformation.py Line: 42
class AmazonInvokeConfig(BaseConfig, BaseAWSLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/amazon_mistral_transformation.py Line: 10
class AmazonMistralConfig(AmazonInvokeConfig, BaseConfig):
    """Reference: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral.html
    Supported Params for the Amazon / Mistral models:

    - `max_tokens` (integer) max tokens,
    - `temperature` (float) temperature for model,
    - `top_p` (float) top p for model
    - `stop` [string] A list of stop sequences that if generated by the model, stops the model from generating further output.
    - `top_k` (float) top k for model"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/bedrock/chat/invoke_transformations/anthropic_claude3_transformation.py Line: 20
class AmazonAnthropicClaude3Config(AmazonInvokeConfig, AnthropicConfig):
    """Reference:
        https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers?model=claude
        https://docs.anthropic.com/claude/docs/models-overview#model-comparison

    Supported Params for the Amazon / Anthropic Claude 3 models:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/topaz/image_variations/transformation.py Line: 25
class TopazImageVariationConfig(TopazModelInfo, BaseImageVariationConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deepinfra/chat/transformation.py Line: 9
class DeepInfraConfig(OpenAIGPTConfig):
    """Reference: https://deepinfra.com/docs/advanced/openai_api

    The class `DeepInfra` provides configuration for the DeepInfra's Chat Completions API interface. Below are the parameters:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/infinity/rerank/transformation.py Line: 28
class InfinityRerankConfig(CohereRerankConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/infinity/embedding/transformation.py Line: 15
class InfinityEmbeddingConfig(BaseEmbeddingConfig):
    "Reference: https://infinity.modal.michaelfeil.eu/docs"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/snowflake/chat/transformation.py Line: 23
class SnowflakeConfig(OpenAIGPTConfig):
    "source: https://docs.snowflake.com/en/sql-reference/functions/complete-snowflake-cortex"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/chat/handler.py Line: 50
async def make_call(client: Optional[AsyncHTTPHandler], api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj, timeout: Optional[Union[(float, ?)]], json_mode: bool) -> Tuple[(Any, ?)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/chat/handler.py Line: 101
def make_sync_call(client: Optional[HTTPHandler], api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj, timeout: Optional[Union[(float, ?)]], json_mode: bool) -> Tuple[(Any, ?)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/chat/handler.py Line: 158
class AnthropicChatCompletion(BaseLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/chat/handler.py Line: 462
class ModelResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/chat/transformation.py Line: 61
class AnthropicConfig(AnthropicModelInfo, BaseConfig):
    """Reference: https://docs.anthropic.com/claude/reference/messages_post

    to pass metadata to anthropic, it's {"user_id": "any-relevant-information"}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/completion/transformation.py Line: 35
class AnthropicTextError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/completion/transformation.py Line: 51
class AnthropicTextConfig(BaseConfig):
    """Reference: https://docs.anthropic.com/claude/reference/complete_post

    to pass metadata to anthropic, it's {"user_id": "any-relevant-information"}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/completion/transformation.py Line: 281
class AnthropicTextCompletionResponseIterator(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/experimental_pass_through/messages/handler.py Line: 30
class AnthropicMessagesHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/experimental_pass_through/messages/handler.py Line: 67
async def anthropic_messages(max_tokens: int, messages: List[Dict], model: str, metadata: Optional[Dict], stop_sequences: Optional[List[str]], stream: Optional[bool], system: Optional[str], temperature: Optional[float], thinking: Optional[Dict], tool_choice: Optional[Dict], tools: Optional[List[Dict]], top_k: Optional[int], top_p: Optional[float], api_key: Optional[str], api_base: Optional[str], client: Optional[AsyncHTTPHandler], custom_llm_provider: Optional[str], **kwargs) -> Union[(AnthropicMessagesResponse, AsyncIterator)]:
    "Makes Anthropic `/v1/messages` API calls In the Anthropic API Spec"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/anthropic/experimental_pass_through/messages/transformation.py Line: 11
class AnthropicMessagesConfig(BaseAnthropicMessagesConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py Line: 26
async def make_call(client: Optional[AsyncHTTPHandler], api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj, streaming_decoder: Optional[CustomStreamingDecoder], fake_stream: bool):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py Line: 66
def make_sync_call(client: Optional[HTTPHandler], api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj, streaming_decoder: Optional[CustomStreamingDecoder], fake_stream: bool, timeout: Optional[Union[(float, ?)]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py Line: 111
class OpenAILikeChatHandler(OpenAILikeBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai_like/chat/transformation.py Line: 23
class OpenAILikeChatConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai_like/embedding/handler.py Line: 21
class OpenAILikeEmbeddingHandler(OpenAILikeBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/jina_ai/rerank/transformation.py Line: 26
class JinaAIRerankConfig(BaseRerankConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/jina_ai/embedding/transformation.py Line: 16
class JinaAIEmbeddingConfig:
    "Reference: https://jina.ai/embeddings/"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ai21/chat/transformation.py Line: 12
class AI21ChatConfig(OpenAILikeChatConfig):
    """Reference: https://docs.ai21.com/reference/jamba-15-api-ref#request-parameters

    Below are the parameters:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/friendliai/chat/transformation.py Line: 8
class FriendliaiChatConfig(OpenAILikeChatConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/triton/embedding/transformation.py Line: 16
class TritonEmbeddingConfig(BaseEmbeddingConfig):
    "Transformations for triton /embeddings endpoint (This is a trtllm model)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/triton/completion/transformation.py Line: 31
class TritonConfig(BaseConfig):
    """Base class for Triton configurations.

    Handles routing between /infer and /generate triton completion llms"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/triton/completion/transformation.py Line: 183
class TritonGenerateConfig(TritonConfig):
    "Transformations for triton /generate endpoint (This is a trtllm model)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/triton/completion/transformation.py Line: 237
class TritonInferConfig(TritonConfig):
    "Transformations for triton /infer endpoint (his is an infer model with a custom model on triton)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/triton/completion/transformation.py Line: 319
class TritonResponseIterator(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/clarifai/chat/transformation.py Line: 33
class ClarifaiConfig(BaseConfig):
    "Reference: https://clarifai.com/meta/Llama-2/models/llama2-70b-chat"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/clarifai/chat/transformation.py Line: 223
class ClarifaiModelResponseIterator(FakeStreamResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/aiohttp_openai/chat/transformation.py Line: 26
class AiohttpOpenAIChatConfig(OpenAILikeChatConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/groq/chat/handler.py Line: 18
class GroqChatCompletion(OpenAILikeChatHandler):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/groq/chat/transformation.py Line: 20
class GroqChatConfig(OpenAILikeChatConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/groq/stt/transformation.py Line: 11
class GroqSTTConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/chat/o_series_handler.py Line: 17
class AzureOpenAIO1ChatCompletion(BaseAzureLLM, OpenAIChatCompletion):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/chat/gpt_transformation.py Line: 30
class AzureOpenAIConfig(BaseConfig):
    """Reference: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions

    The class `AzureOpenAIConfig` provides configuration for the OpenAI's Chat API interface, for use with Azure. Below are the parameters::

    - `frequency_penalty` (number or null): Defaults to 0. Allows a value between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, thereby minimizing repetition.

    - `function_call` (string or object): This optional parameter controls how the model calls functions.

    - `functions` (array): An optional parameter. It is a list of functions for which the model may generate JSON inputs.

    - `logit_bias` (map): This optional parameter modifies the likelihood of specified tokens appearing in the completion.

    - `max_tokens` (integer or null): This optional parameter helps to set the maximum number of tokens to generate in the chat completion.

    - `n` (integer or null): This optional parameter helps to set how many chat completion choices to generate for each input message.

    - `presence_penalty` (number or null): Defaults to 0. It penalizes new tokens based on if they appear in the text so far, hence increasing the model's likelihood to talk about new topics.

    - `stop` (string / array / null): Specifies up to 4 sequences where the API will stop generating further tokens.

    - `temperature` (number or null): Defines the sampling temperature to use, varying between 0 and 2.

    - `top_p` (number or null): An alternative to sampling with temperature, used for nucleus sampling."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/chat/o_series_transformation.py Line: 25
class AzureOpenAIO1Config(OpenAIOSeriesConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/fine_tuning/handler.py Line: 10
class AzureOpenAIFineTuningAPI(OpenAIFineTuningAPI, BaseAzureLLM):
    "AzureOpenAI methods to support fine tuning, inherits from OpenAIFineTuningAPI."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/batches/handler.py Line: 21
class AzureBatchesAPI(BaseAzureLLM):
    """Azure methods to support for batches
    - create_batch()
    - retrieve_batch()
    - cancel_batch()
    - list_batch()"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/completion/handler.py Line: 14
class AzureTextCompletion(BaseAzureLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/completion/transformation.py Line: 6
class AzureOpenAITextConfig(OpenAITextCompletionConfig):
    """Reference: https://platform.openai.com/docs/api-reference/chat/create

    The class `AzureOpenAIConfig` provides configuration for the OpenAI's Chat API interface, for use with Azure. It inherits from `OpenAIConfig`. Below are the parameters::

    - `frequency_penalty` (number or null): Defaults to 0. Allows a value between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, thereby minimizing repetition.

    - `function_call` (string or object): This optional parameter controls how the model calls functions.

    - `functions` (array): An optional parameter. It is a list of functions for which the model may generate JSON inputs.

    - `logit_bias` (map): This optional parameter modifies the likelihood of specified tokens appearing in the completion.

    - `max_tokens` (integer or null): This optional parameter helps to set the maximum number of tokens to generate in the chat completion.

    - `n` (integer or null): This optional parameter helps to set how many chat completion choices to generate for each input message.

    - `presence_penalty` (number or null): Defaults to 0. It penalizes new tokens based on if they appear in the text so far, hence increasing the model's likelihood to talk about new topics.

    - `stop` (string / array / null): Specifies up to 4 sequences where the API will stop generating further tokens.

    - `temperature` (number or null): Defines the sampling temperature to use, varying between 0 and 2.

    - `top_p` (number or null): An alternative to sampling with temperature, used for nucleus sampling."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/files/handler.py Line: 13
class AzureOpenAIFilesAPI(BaseAzureLLM):
    """AzureOpenAI methods to support for batches
    - create_file()
    - retrieve_file()
    - list_files()
    - delete_file()
    - file_content()
    - update_file()"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/realtime/handler.py Line: 16
async def forward_messages(client_ws: Any, backend_ws: Any):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/realtime/handler.py Line: 27
class AzureOpenAIRealtime(AzureChatCompletion):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/azure/responses/transformation.py Line: 22
class AzureOpenAIResponsesAPIConfig(OpenAIResponsesAPIConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/chat/gpt_audio_transformation.py Line: 12
class OpenAIGPTAudioConfig(OpenAIGPTConfig):
    "Reference: https://platform.openai.com/docs/guides/audio"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/chat/gpt_transformation.py Line: 43
class OpenAIGPTConfig(BaseLLMModelInfo, BaseConfig):
    """Reference: https://platform.openai.com/docs/api-reference/chat/create

    The class `OpenAIConfig` provides configuration for the OpenAI's Chat API interface. Below are the parameters:

    - `frequency_penalty` (number or null): Defaults to 0. Allows a value between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, thereby minimizing repetition.

    - `function_call` (string or object): This optional parameter controls how the model calls functions.

    - `functions` (array): An optional parameter. It is a list of functions for which the model may generate JSON inputs.

    - `logit_bias` (map): This optional parameter modifies the likelihood of specified tokens appearing in the completion.

    - `max_tokens` (integer or null): This optional parameter helps to set the maximum number of tokens to generate in the chat completion.

    - `n` (integer or null): This optional parameter helps to set how many chat completion choices to generate for each input message.

    - `presence_penalty` (number or null): Defaults to 0. It penalizes new tokens based on if they appear in the text so far, hence increasing the model's likelihood to talk about new topics.

    - `stop` (string / array / null): Specifies up to 4 sequences where the API will stop generating further tokens.

    - `temperature` (number or null): Defines the sampling temperature to use, varying between 0 and 2.

    - `top_p` (number or null): An alternative to sampling with temperature, used for nucleus sampling."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/chat/gpt_transformation.py Line: 408
class OpenAIChatCompletionStreamingHandler(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/chat/o_series_transformation.py Line: 30
class OpenAIOSeriesConfig(OpenAIGPTConfig):
    "Reference: https://platform.openai.com/docs/guides/reasoning"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/fine_tuning/handler.py Line: 10
class OpenAIFineTuningAPI:
    "OpenAI methods to support for batches"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/image_variations/handler.py Line: 19
class OpenAIImageVariationsHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/image_variations/transformation.py Line: 15
class OpenAIImageVariationConfig(BaseImageVariationConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/completion/handler.py Line: 18
class OpenAITextCompletion(BaseLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/completion/transformation.py Line: 15
class OpenAITextCompletionConfig(BaseTextCompletionConfig, OpenAIGPTConfig):
    """Reference: https://platform.openai.com/docs/api-reference/completions/create

    The class `OpenAITextCompletionConfig` provides configuration for the OpenAI's text completion API interface. Below are the parameters:

    - `best_of` (integer or null): This optional parameter generates server-side completions and returns the one with the highest log probability per token.

    - `echo` (boolean or null): This optional parameter will echo back the prompt in addition to the completion.

    - `frequency_penalty` (number or null): Defaults to 0. It is a numbers from -2.0 to 2.0, where positive values decrease the model's likelihood to repeat the same line.

    - `logit_bias` (map): This optional parameter modifies the likelihood of specified tokens appearing in the completion.

    - `logprobs` (integer or null): This optional parameter includes the log probabilities on the most likely tokens as well as the chosen tokens.

    - `max_tokens` (integer or null): This optional parameter sets the maximum number of tokens to generate in the completion.

    - `n` (integer or null): This optional parameter sets how many completions to generate for each prompt.

    - `presence_penalty` (number or null): Defaults to 0 and can be between -2.0 and 2.0. Positive values increase the model's likelihood to talk about new topics.

    - `stop` (string / array / null): Specifies up to 4 sequences where the API will stop generating further tokens.

    - `suffix` (string or null): Defines the suffix that comes after a completion of inserted text.

    - `temperature` (number or null): This optional parameter defines the sampling temperature to use.

    - `top_p` (number or null): An alternative to sampling with temperature, used for nucleus sampling."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/completion/utils.py Line: 13
def is_tokens_or_list_of_tokens(value: List):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/completion/utils.py Line: 26
def _transform_prompt(messages: Union[(List[AllMessageValues], List[OpenAITextCompletionUserMessage])]) -> AllPromptValues:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/transcriptions/handler.py Line: 23
class OpenAIAudioTranscription(OpenAIChatCompletion):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/transcriptions/gpt_transformation.py Line: 9
class OpenAIGPTAudioTranscriptionConfig(OpenAIWhisperAudioTranscriptionConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/transcriptions/whisper_transformation.py Line: 19
class OpenAIWhisperAudioTranscriptionConfig(BaseAudioTranscriptionConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/realtime/handler.py Line: 14
class OpenAIRealtime(OpenAIChatCompletion):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/openai/responses/transformation.py Line: 23
class OpenAIResponsesAPIConfig(BaseResponsesAPIConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama/completion/handler.py Line: 17
async def ollama_aembeddings(api_base: str, model: str, prompts: List[str], model_response: EmbeddingResponse, optional_params: dict, logging_obj: Any, encoding: Any):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama/completion/handler.py Line: 83
def ollama_embeddings(api_base: str, model: str, prompts: list, optional_params: dict, model_response: EmbeddingResponse, logging_obj: Any, encoding):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py Line: 38
class OllamaConfig(BaseConfig):
    """Reference: https://github.com/ollama/ollama/blob/main/docs/api.md#parameters

    The class `OllamaConfig` provides the configuration for the Ollama's API interface. Below are the parameters:

    - `mirostat` (int): Enable Mirostat sampling for controlling perplexity. Default is 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0. Example usage: mirostat 0

    - `mirostat_eta` (float): Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. Default: 0.1. Example usage: mirostat_eta 0.1

    - `mirostat_tau` (float): Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. Default: 5.0. Example usage: mirostat_tau 5.0

    - `num_ctx` (int): Sets the size of the context window used to generate the next token. Default: 2048. Example usage: num_ctx 4096

    - `num_gqa` (int): The number of GQA groups in the transformer layer. Required for some models, for example it is 8 for llama2:70b. Example usage: num_gqa 1

    - `num_gpu` (int): The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Example usage: num_gpu 0

    - `num_thread` (int): Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Example usage: num_thread 8

    - `repeat_last_n` (int): Sets how far back for the model to look back to prevent repetition. Default: 64, 0 = disabled, -1 = num_ctx. Example usage: repeat_last_n 64

    - `repeat_penalty` (float): Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. Default: 1.1. Example usage: repeat_penalty 1.1

    - `temperature` (float): The temperature of the model. Increasing the temperature will make the model answer more creatively. Default: 0.8. Example usage: temperature 0.7

    - `seed` (int): Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. Example usage: seed 42

    - `stop` (string[]): Sets the stop sequences to use. Example usage: stop "AI assistant:"

    - `tfs_z` (float): Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. Default: 1. Example usage: tfs_z 1

    - `num_predict` (int): Maximum number of tokens to predict when generating text. Default: 128, -1 = infinite generation, -2 = fill context. Example usage: num_predict 42

    - `top_k` (int): Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. Default: 40. Example usage: top_k 40

    - `top_p` (float): Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. Default: 0.9. Example usage: top_p 0.9

    - `system` (string): system prompt for model (overrides what is defined in the Modelfile)

    - `template` (string): the full prompt or prompt template (overrides what is defined in the Modelfile)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py Line: 400
class OllamaTextCompletionResponseIterator(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/gemini/chat/transformation.py Line: 16
class GoogleAIStudioGeminiConfig(VertexGeminiConfig):
    """Reference: https://ai.google.dev/api/rest/v1beta/GenerationConfig

    The class `GoogleAIStudioGeminiConfig` provides configuration for the Google AI Studio's Gemini API interface. Below are the parameters:

    - `temperature` (float): This controls the degree of randomness in token selection.

    - `max_output_tokens` (integer): This sets the limitation for the maximum amount of token in the text output. In this case, the default value is 256.

    - `top_p` (float): The tokens are selected from the most probable to the least probable until the sum of their probabilities equals the `top_p` value. Default is 0.95.

    - `top_k` (integer): The value of `top_k` determines how many of the most probable tokens are considered in the selection. For example, a `top_k` of 1 means the selected token is the most probable among all tokens. The default value is 40.

    - `response_mime_type` (str): The MIME type of the response. The default value is 'text/plain'. Other values - `application/json`.

    - `response_schema` (dict): Optional. Output response schema of the generated candidate text when response mime type can have schema. Schema can be objects, primitives or arrays and is a subset of OpenAPI schema. If set, a compatible response_mime_type must also be set. Compatible mimetypes: application/json: Schema for JSON response.

    - `candidate_count` (int): Number of generated responses to return.

    - `stop_sequences` (List[str]): The set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a stop sequence. The stop sequence will not be included as part of the response.

    Note: Please make sure to modify the default parameters as required for your use case."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/gemini/files/transformation.py Line: 28
class GoogleAIStudioFilesHandler(GeminiModelInfo, BaseFilesConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/chat/transformation.py Line: 24
class CohereError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/chat/transformation.py Line: 42
class CohereChatConfig(BaseConfig):
    """Configuration class for Cohere's API interface.

    Args:
        preamble (str, optional): When specified, the default Cohere preamble will be replaced with the provided one.
        chat_history (List[Dict[str, str]], optional): A list of previous messages between the user and the model.
        generation_id (str, optional): Unique identifier for the generated reply.
        response_id (str, optional): Unique identifier for the response.
        conversation_id (str, optional): An alternative to chat_history, creates or resumes a persisted conversation.
        prompt_truncation (str, optional): Dictates how the prompt will be constructed. Options: 'AUTO', 'AUTO_PRESERVE_ORDER', 'OFF'.
        connectors (List[Dict[str, str]], optional): List of connectors (e.g., web-search) to enrich the model's reply.
        search_queries_only (bool, optional): When true, the response will only contain a list of generated search queries.
        documents (List[Dict[str, str]], optional): A list of relevant documents that the model can cite.
        temperature (float, optional): A non-negative float that tunes the degree of randomness in generation.
        max_tokens [DEPRECATED - use max_completion_tokens] (int, optional): The maximum number of tokens the model will generate as part of the response.
        max_completion_tokens (int, optional): The maximum number of tokens the model will generate as part of the response.
        k (int, optional): Ensures only the top k most likely tokens are considered for generation at each step.
        p (float, optional): Ensures that only the most likely tokens, with total probability mass of p, are considered for generation.
        frequency_penalty (float, optional): Used to reduce repetitiveness of generated tokens.
        presence_penalty (float, optional): Used to reduce repetitiveness of generated tokens.
        tools (List[Dict[str, str]], optional): A list of available tools (functions) that the model may suggest invoking.
        tool_results (List[Dict[str, Any]], optional): A list of results from invoking tools.
        seed (int, optional): A seed to assist reproducibility of the model's response."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/chat/v2_transformation.py Line: 25
class CohereV2ChatConfig(BaseConfig):
    """Configuration class for Cohere's API interface.

    Args:
        preamble (str, optional): When specified, the default Cohere preamble will be replaced with the provided one.
        chat_history (List[Dict[str, str]], optional): A list of previous messages between the user and the model.
        generation_id (str, optional): Unique identifier for the generated reply.
        response_id (str, optional): Unique identifier for the response.
        conversation_id (str, optional): An alternative to chat_history, creates or resumes a persisted conversation.
        prompt_truncation (str, optional): Dictates how the prompt will be constructed. Options: 'AUTO', 'AUTO_PRESERVE_ORDER', 'OFF'.
        connectors (List[Dict[str, str]], optional): List of connectors (e.g., web-search) to enrich the model's reply.
        search_queries_only (bool, optional): When true, the response will only contain a list of generated search queries.
        documents (List[Dict[str, str]], optional): A list of relevant documents that the model can cite.
        temperature (float, optional): A non-negative float that tunes the degree of randomness in generation.
        max_tokens (int, optional): The maximum number of tokens the model will generate as part of the response.
        k (int, optional): Ensures only the top k most likely tokens are considered for generation at each step.
        p (float, optional): Ensures that only the most likely tokens, with total probability mass of p, are considered for generation.
        frequency_penalty (float, optional): Used to reduce repetitiveness of generated tokens.
        presence_penalty (float, optional): Used to reduce repetitiveness of generated tokens.
        tools (List[Dict[str, str]], optional): A list of available tools (functions) that the model may suggest invoking.
        tool_results (List[Dict[str, Any]], optional): A list of results from invoking tools.
        seed (int, optional): A seed to assist reproducibility of the model's response."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/rerank/transformation.py Line: 16
class CohereRerankConfig(BaseRerankConfig):
    "Reference: https://docs.cohere.com/v2/reference/rerank"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/completion/transformation.py Line: 26
class CohereTextConfig(BaseConfig):
    """Reference: https://docs.cohere.com/reference/generate

    The class `CohereConfig` provides configuration for the Cohere's API interface. Below are the parameters:

    - `num_generations` (integer): Maximum number of generations returned. Default is 1, with a minimum value of 1 and a maximum value of 5.

    - `max_tokens` (integer): Maximum number of tokens the model will generate as part of the response. Default value is 20.

    - `truncate` (string): Specifies how the API handles inputs longer than maximum token length. Options include NONE, START, END. Default is END.

    - `temperature` (number): A non-negative float controlling the randomness in generation. Lower temperatures result in less random generations. Default is 0.75.

    - `preset` (string): Identifier of a custom preset, a combination of parameters such as prompt, temperature etc.

    - `end_sequences` (array of strings): The generated text gets cut at the beginning of the earliest occurrence of an end sequence, which will be excluded from the text.

    - `stop_sequences` (array of strings): The generated text gets cut at the end of the earliest occurrence of a stop sequence, which will be included in the text.

    - `k` (integer): Limits generation at each step to top `k` most likely tokens. Default is 0.

    - `p` (number): Limits generation at each step to most likely tokens with total probability mass of `p`. Default is 0.

    - `frequency_penalty` (number): Reduces repetitiveness of generated tokens. Higher values apply stronger penalties to previously occurred tokens.

    - `presence_penalty` (number): Reduces repetitiveness of generated tokens. Similar to frequency_penalty, but this penalty applies equally to all tokens that have already appeared.

    - `return_likelihoods` (string): Specifies how and if token likelihoods are returned with the response. Options include GENERATION, ALL and NONE.

    - `logit_bias` (object): Used to prevent the model from generating unwanted tokens or to incentivize it to include desired tokens. e.g. {"hello_world": 1233}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/rerank_v2/transformation.py Line: 7
class CohereRerankV2Config(CohereRerankConfig):
    "Reference: https://docs.cohere.com/v2/reference/rerank"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/embed/handler.py Line: 19
def validate_environment(api_key, headers: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/embed/handler.py Line: 32
class CohereError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/embed/handler.py Line: 45
async def async_embedding(model: str, data: Union[(dict, CohereEmbeddingRequest)], input: list, model_response: ?, timeout: Optional[Union[(float, ?)]], logging_obj: LiteLLMLoggingObj, optional_params: dict, api_base: str, api_key: Optional[str], headers: dict, encoding: Callable, client: Optional[AsyncHTTPHandler]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/embed/handler.py Line: 111
def embedding(model: str, input: list, model_response: EmbeddingResponse, logging_obj: LiteLLMLoggingObj, optional_params: dict, headers: dict, encoding: Any, data: Optional[Union[(dict, CohereEmbeddingRequest)]], complete_api_base: Optional[str], api_key: Optional[str], aembedding: Optional[bool], timeout: Optional[Union[(float, ?)]]=..., client: Optional[Union[(HTTPHandler, AsyncHTTPHandler)]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cohere/embed/transformation.py Line: 27
class CohereEmbeddingConfig:
    "Reference: https://docs.cohere.com/v2/reference/embed"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/petals/completion/handler.py Line: 19
def completion(model: str, messages: list, api_base: Optional[str], model_response: ModelResponse, print_verbose: Callable, encoding, logging_obj, optional_params: dict, stream, litellm_params, logger_fn, client: Optional[Union[(HTTPHandler, AsyncHTTPHandler)]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/petals/completion/handler.py Line: 147
def embedding():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/petals/completion/transformation.py Line: 17
class PetalsConfig(BaseConfig):
    """Reference: https://github.com/petals-infra/chat.petals.dev#post-apiv1generate
    The `PetalsConfig` class encapsulates the configuration for the Petals API. The properties of this class are described below:

    - `max_length` (integer): This represents the maximum length of the generated text (including the prefix) in tokens.

    - `max_new_tokens` (integer): This represents the maximum number of newly generated tokens (excluding the prefix).

    The generation parameters are compatible with `.generate()` from Hugging Face's Transformers library:

    - `do_sample` (boolean, optional): If set to 0 (default), the API runs greedy generation. If set to 1, the API performs sampling using the parameters below:

    - `temperature` (float, optional): This value sets the temperature for sampling.

    - `top_k` (integer, optional): This value sets the limit for top-k sampling.

    - `top_p` (float, optional): This value sets the limit for top-p (nucleus) sampling.

    - `repetition_penalty` (float, optional): This helps apply the repetition penalty during text generation, as discussed in this paper."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cloudflare/chat/transformation.py Line: 25
class CloudflareError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cloudflare/chat/transformation.py Line: 39
class CloudflareChatConfig(BaseConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/cloudflare/chat/transformation.py Line: 190
class CloudflareChatResponseIterator(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/voyage/embedding/transformation.py Line: 13
class VoyageError(BaseLLMException):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/voyage/embedding/transformation.py Line: 33
class VoyageEmbeddingConfig(BaseEmbeddingConfig):
    "Reference: https://docs.voyageai.com/reference/embeddings-api"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/nlp_cloud/chat/handler.py Line: 17
def completion(model: str, messages: list, api_base: str, model_response: ModelResponse, print_verbose: Callable, encoding, api_key, logging_obj, optional_params: dict, litellm_params: dict, logger_fn, default_max_tokens_to_sample, client: Optional[Union[(HTTPHandler, AsyncHTTPHandler)]], headers):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/nlp_cloud/chat/handler.py Line: 111
def clean_and_iterate_chunks(response):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/nlp_cloud/chat/handler.py Line: 129
def embedding():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/nlp_cloud/chat/transformation.py Line: 24
class NLPCloudConfig(BaseConfig):
    """Reference: https://docs.nlpcloud.com/#generation

    - `max_length` (int): Optional. The maximum number of tokens that the generated text should contain.

    - `length_no_input` (boolean): Optional. Whether `min_length` and `max_length` should not include the length of the input text.

    - `end_sequence` (string): Optional. A specific token that should be the end of the generated sequence.

    - `remove_end_sequence` (boolean): Optional. Whether to remove the `end_sequence` string from the result.

    - `remove_input` (boolean): Optional. Whether to remove the input text from the result.

    - `bad_words` (list of strings): Optional. List of tokens that are not allowed to be generated.

    - `temperature` (float): Optional. Temperature sampling. It modulates the next token probabilities.

    - `top_p` (float): Optional. Top P sampling. Below 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.

    - `top_k` (int): Optional. Top K sampling. The number of highest probability vocabulary tokens to keep for top k filtering.

    - `repetition_penalty` (float): Optional. Prevents the same word from being repeated too many times.

    - `num_beams` (int): Optional. Number of beams for beam search.

    - `num_return_sequences` (int): Optional. The number of independently computed returned sequences."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/sagemaker/chat/handler.py Line: 15
class SagemakerChatHandler(BaseAWSLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/sagemaker/chat/transformation.py Line: 20
class SagemakerChatConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/sagemaker/completion/handler.py Line: 37
class SagemakerLLM(BaseAWSLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/sagemaker/completion/transformation.py Line: 34
class SagemakerConfig(BaseConfig):
    "Reference: https://d-uuwbxj1u4cnu.studio.us-west-2.sagemaker.aws/jupyter/default/lab/workspaces/auto-q/tree/DemoNotebooks/meta-textgeneration-llama-2-7b-SDK_1.ipynb"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/deepseek/chat/transformation.py Line: 16
class DeepSeekChatConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/predibase/chat/handler.py Line: 30
async def make_call(client: AsyncHTTPHandler, api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj, timeout: Optional[Union[(float, ?)]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/predibase/chat/handler.py Line: 59
class PredibaseChatCompletion:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/predibase/chat/transformation.py Line: 20
class PredibaseConfig(BaseConfig):
    "Reference:  https://docs.predibase.com/user-guide/inference/rest_api"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/together_ai/rerank/handler.py Line: 19
class TogetherAIRerank(BaseLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/together_ai/rerank/transformation.py Line: 20
class TogetherAIRerankConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/together_ai/completion/transformation.py Line: 22
class TogetherAITextCompletionConfig(OpenAITextCompletionConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/databricks/chat/transformation.py Line: 67
class DatabricksConfig(DatabricksBase, OpenAILikeChatConfig, AnthropicConfig):
    "Reference: https://docs.databricks.com/en/machine-learning/foundation-models/api-reference.html#chat-request"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/databricks/chat/transformation.py Line: 485
class DatabricksChatResponseIterator(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/databricks/embed/handler.py Line: 13
class DatabricksEmbeddingHandler(OpenAILikeEmbeddingHandler, DatabricksBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/databricks/embed/transformation.py Line: 9
class DatabricksEmbeddingConfig:
    "Reference: https://learn.microsoft.com/en-us/azure/databricks/machine-learning/foundation-models/api-reference#--embedding-task"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/huggingface/chat/transformation.py Line: 26
class HuggingFaceChatConfig(OpenAIGPTConfig):
    "Reference: https://huggingface.co/docs/huggingface_hub/guides/inference"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/huggingface/embedding/handler.py Line: 29
def get_hf_task_embedding_for_model(model: str, task_type: Optional[str], api_base: str) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/huggingface/embedding/handler.py Line: 52
async def async_get_hf_task_embedding_for_model(model: str, task_type: Optional[str], api_base: str) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/huggingface/embedding/handler.py Line: 77
class HuggingFaceEmbedding(BaseLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/huggingface/embedding/transformation.py Line: 38
class HuggingFaceEmbeddingConfig(BaseConfig):
    "Reference: https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference/compat_generate"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/image_generation/cost_calculator.py Line: 9
def cost_calculator(model: str, image_response: ImageResponse) -> float:
    "Vertex AI Image Generation Cost Calculator"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/image_generation/image_generation_handler.py Line: 18
class VertexImageGeneration(VertexLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_embeddings/embedding_handler.py Line: 21
class VertexEmbedding(VertexBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_embeddings/transformation.py Line: 11
class VertexAITextEmbeddingConfig(BaseModel):
    """Reference: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api#TextEmbeddingInput

    Args:
        auto_truncate: Optional(bool) If True, will truncate input text to fit within the model's max input length.
        task_type: Optional(str) The type of task to be performed. The default is "RETRIEVAL_QUERY".
        title: Optional(str) The title of the document to be embedded. (only valid with task_type=RETRIEVAL_DOCUMENT)."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_embeddings/types.py Line: 9
class TaskType(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_embeddings/types.py Line: 20
class TextEmbeddingInput(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_embeddings/types.py Line: 28
class TextEmbeddingFineTunedInput(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_embeddings/types.py Line: 32
class TextEmbeddingFineTunedParameters(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_embeddings/types.py Line: 39
class EmbeddingParameters(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_embeddings/types.py Line: 44
class VertexEmbeddingRequest(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/fine_tuning/handler.py Line: 25
class VertexFineTuningAPI(VertexLLM):
    "Vertex methods to support for batches"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini_embeddings/batch_embed_content_handler.py Line: 30
class GoogleBatchEmbeddings(VertexLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini_embeddings/batch_embed_content_transformation.py Line: 22
def transform_openai_input_gemini_content(input: EmbeddingInput, model: str, optional_params: dict) -> VertexAIBatchEmbeddingsRequestBody:
    "The content to embed. Only the parts.text fields will be counted."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini_embeddings/batch_embed_content_transformation.py Line: 49
def process_response(input: EmbeddingInput, model_response: EmbeddingResponse, model: str, _predictions: VertexAIBatchEmbeddingsResponseObject) -> EmbeddingResponse:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/batches/handler.py Line: 22
class VertexAIBatchPrediction(VertexLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/batches/transformation.py Line: 12
class VertexAIBatchTransformation:
    """Transforms OpenAI Batch requests to Vertex AI Batch requests

    API Ref: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/context_caching/transformation.py Line: 20
def get_first_continuous_block_idx(filtered_messages: List[Tuple[(int, AllMessageValues)]]) -> int:
    """Find the array index that ends the first continuous sequence of message blocks.

    Args:
        filtered_messages: List of tuples containing (index, message) pairs

    Returns:
        int: The array index where the first continuous sequence ends"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/context_caching/transformation.py Line: 50
def separate_cached_messages(messages: List[AllMessageValues]) -> Tuple[(List[AllMessageValues], List[AllMessageValues])]:
    """Returns separated cached and non-cached messages.

    Args:
        messages: List of messages to be separated.

    Returns:
        Tuple containing:
        - cached_messages: List of cached messages.
        - non_cached_messages: List of non-cached messages."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/context_caching/transformation.py Line: 90
def transform_openai_messages_to_gemini_context_caching(model: str, messages: List[AllMessageValues], cache_key: str) -> CachedContentRequestBody:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py Line: 31
class ContextCachingEndpoints(VertexBase):
    """Covers context caching endpoints for Vertex AI + Google AI Studio

    v0: covers Google AI Studio"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/transformation.py Line: 63
def _process_gemini_image(image_url: str, format: Optional[str]) -> PartType:
    "Given an image URL, return the appropriate PartType for Gemini"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/transformation.py Line: 105
def _gemini_convert_messages_with_history(messages: List[AllMessageValues]) -> List[ContentType]:
    """Converts given messages from OpenAI format to Gemini format

    - Parts must be iterable
    - Roles must alternate b/w 'user' and 'model' (same as anthropic -> merge consecutive roles)
    - Please ensure that function response turn comes immediately after a function call turn"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/transformation.py Line: 287
def _transform_request_body(messages: List[AllMessageValues], model: str, optional_params: dict, custom_llm_provider: Literal[(?, ?, ?)], litellm_params: dict, cached_content: Optional[str]) -> RequestBody:
    "Common transformation logic across sync + async Gemini /generateContent calls."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/transformation.py Line: 369
def sync_transform_request_body(gemini_api_key: Optional[str], messages: List[AllMessageValues], api_base: Optional[str], model: str, client: Optional[HTTPHandler], timeout: Optional[Union[(float, ?)]], extra_headers: Optional[dict], optional_params: dict, logging_obj: LiteLLMLoggingObj, custom_llm_provider: Literal[(?, ?, ?)], litellm_params: dict) -> RequestBody:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/transformation.py Line: 411
async def async_transform_request_body(gemini_api_key: Optional[str], messages: List[AllMessageValues], api_base: Optional[str], model: str, client: Optional[AsyncHTTPHandler], timeout: Optional[Union[(float, ?)]], extra_headers: Optional[dict], optional_params: dict, logging_obj: ?, custom_llm_provider: Literal[(?, ?, ?)], litellm_params: dict) -> RequestBody:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/transformation.py Line: 456
def _transform_system_message(supports_system_message: bool, messages: List[AllMessageValues]) -> Tuple[(Optional[SystemInstructions], List[AllMessageValues])]:
    """Extracts the system message from the openai message list.

    Converts the system message to Gemini format

    Returns
    - system_content_blocks: Optional[SystemInstructions] - the system message list in Gemini format.
    - messages: List[AllMessageValues] - filtered list of messages in OpenAI format (transformed separately)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py Line: 89
class VertexAIBaseConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py Line: 137
class VertexGeminiConfig(VertexAIBaseConfig, BaseConfig):
    """Reference: https://cloud.google.com/vertex-ai/docs/generative-ai/chat/test-chat-prompts
    Reference: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference

    The class `VertexAIConfig` provides configuration for the VertexAI's API interface. Below are the parameters:

    - `temperature` (float): This controls the degree of randomness in token selection.

    - `max_output_tokens` (integer): This sets the limitation for the maximum amount of token in the text output. In this case, the default value is 256.

    - `top_p` (float): The tokens are selected from the most probable to the least probable until the sum of their probabilities equals the `top_p` value. Default is 0.95.

    - `top_k` (integer): The value of `top_k` determines how many of the most probable tokens are considered in the selection. For example, a `top_k` of 1 means the selected token is the most probable among all tokens. The default value is 40.

    - `response_mime_type` (str): The MIME type of the response. The default value is 'text/plain'.

    - `candidate_count` (int): Number of generated responses to return.

    - `stop_sequences` (List[str]): The set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a stop sequence. The stop sequence will not be included as part of the response.

    - `frequency_penalty` (float): This parameter is used to penalize the model from repeating the same output. The default value is 0.0.

    - `presence_penalty` (float): This parameter is used to penalize the model from generating the same output as the input. The default value is 0.0.

    - `seed` (int): The seed value is used to help generate the same output for the same input. The default value is None.

    Note: Please make sure to modify the default parameters as required for your use case."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py Line: 1029
async def make_call(client: Optional[AsyncHTTPHandler], api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py Line: 1074
def make_sync_call(client: Optional[HTTPHandler], gemini_client: Optional[HTTPHandler], api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py Line: 1113
class VertexLLM(VertexBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py Line: 1519
class ModelResponseIterator:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/multimodal_embeddings/embedding_handler.py Line: 24
class VertexMultimodalEmbedding(VertexLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/multimodal_embeddings/transformation.py Line: 28
class VertexAIMultimodalEmbeddingConfig(BaseEmbeddingConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_model_garden/main.py Line: 29
def create_vertex_url(vertex_location: str, vertex_project: str, stream: Optional[bool], model: str, api_base: Optional[str]) -> str:
    "Return the base url for the vertex garden models"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_model_garden/main.py Line: 41
class VertexAIModelGardenModels(VertexBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_partner_models/main.py Line: 15
class VertexPartnerProvider(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_partner_models/main.py Line: 22
class VertexAIError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_partner_models/main.py Line: 35
def create_vertex_url(vertex_location: str, vertex_project: str, partner: VertexPartnerProvider, stream: Optional[bool], model: str, api_base: Optional[str]) -> str:
    "Return the base url for the vertex partner models"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_partner_models/main.py Line: 63
class VertexAIPartnerModels(VertexBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/files/handler.py Line: 20
class VertexAIFilesHandler(GCSBucketBase):
    """Handles Calling VertexAI in OpenAI Files API format v1/files/*

    This implementation uploads files on GCS Buckets"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/files/transformation.py Line: 37
class VertexAIFilesConfig(VertexBase, BaseFilesConfig):
    "Config for VertexAI Files"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/files/transformation.py Line: 333
class VertexAIJsonlFilesTransformation(VertexGeminiConfig):
    "Transforms OpenAI /v1/files/* requests to VertexAI /v1/files/* requests"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/text_to_speech/text_to_speech_handler.py Line: 15
class VertexInput(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/text_to_speech/text_to_speech_handler.py Line: 20
class VertexVoice(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/text_to_speech/text_to_speech_handler.py Line: 25
class VertexAudioConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/text_to_speech/text_to_speech_handler.py Line: 30
class VertexTextToSpeechRequest(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/text_to_speech/text_to_speech_handler.py Line: 36
class VertexTextToSpeechAPI(VertexLLM):
    "Vertex methods to support for batches"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/text_to_speech/text_to_speech_handler.py Line: 215
def validate_vertex_input(input_data: VertexInput, kwargs: dict, optional_params: dict) -> ?:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_partner_models/anthropic/transformation.py Line: 15
class VertexAIError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_partner_models/anthropic/transformation.py Line: 28
class VertexAIAnthropicConfig(AnthropicConfig):
    """Reference:https://docs.anthropic.com/claude/reference/messages_post

    Note that the API for Claude on Vertex differs from the Anthropic API documentation in the following ways:

    - `model` is not a valid parameter. The model is instead specified in the Google Cloud endpoint URL.
    - `anthropic_version` is a required parameter and must be set to "vertex-2023-10-16".

    The class `VertexAIAnthropicConfig` provides configuration for the VertexAI's Anthropic API interface. Below are the parameters:

    - `max_tokens` Required (integer) max tokens,
    - `anthropic_version` Required (string) version of anthropic for bedrock - e.g. "bedrock-2023-05-31"
    - `system` Optional (string) the system prompt, conversion from openai format to this is handled in factory.py
    - `temperature` Optional (float) The amount of randomness injected into the response
    - `top_p` Optional (float) Use nucleus sampling.
    - `top_k` Optional (int) Only sample from the top K options for each subsequent token
    - `stop_sequences` Optional (List[str]) Custom text sequences that cause the model to stop generating

    Note: Please make sure to modify the default parameters as required for your use case."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_partner_models/llama3/transformation.py Line: 7
class VertexAILlama3Config(OpenAIGPTConfig):
    """Reference:https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama#streaming

    The class `VertexAILlama3Config` provides configuration for the VertexAI's Llama API interface. Below are the parameters:

    - `max_tokens` Required (integer) max tokens,

    Note: Please make sure to modify the default parameters as required for your use case."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vertex_ai/vertex_ai_partner_models/ai21/transformation.py Line: 8
class VertexAIAi21Config(OpenAIGPTConfig):
    """Reference: https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/ai21

    The class `VertexAIAi21Config` provides configuration for the VertexAI's AI21 API interface

    -> Supports all OpenAI parameters"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/galadriel/chat/transformation.py Line: 8
class GaladrielChatConfig(OpenAILikeChatConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/empower/chat/transformation.py Line: 8
class EmpowerChatConfig(OpenAILikeChatConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/perplexity/chat/transformation.py Line: 12
class PerplexityChatConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/lm_studio/chat/transformation.py Line: 12
class LMStudioChatConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/lm_studio/embed/transformation.py Line: 13
class LmStudioEmbeddingConfig:
    "Reference: https://lmstudio.ai/docs/basics/server"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vllm/completion/handler.py Line: 15
class VLLMError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vllm/completion/handler.py Line: 27
def validate_environment(model: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vllm/completion/handler.py Line: 39
def completion(model: str, messages: list, model_response: ModelResponse, print_verbose: Callable, encoding, logging_obj, optional_params: dict, custom_prompt_dict, litellm_params, logger_fn):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vllm/completion/handler.py Line: 113
def batch_completions(model: str, messages: list, optional_params, custom_prompt_dict):
    """Example usage:
    import litellm
    import os
    from litellm import batch_completion


    responses = batch_completion(
        model="vllm/facebook/opt-125m",
        messages = [
            [
                {
                    "role": "user",
                    "content": "good morning? "
                }
            ],
            [
                {
                    "role": "user",
                    "content": "what's the time? "
                }
            ]
        ]
    )"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vllm/completion/handler.py Line: 193
def embedding():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/vllm/completion/transformation.py Line: 10
class VLLMConfig(HostedVLLMChatConfig):
    "VLLM SDK supports the same OpenAI params as hosted_vllm."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/chat/handler.py Line: 15
class WatsonXChatHandler(OpenAILikeChatHandler):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/chat/transformation.py Line: 17
class IBMWatsonXChatConfig(IBMWatsonXMixin, OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/completion/transformation.py Line: 38
class IBMWatsonXAIConfig(IBMWatsonXMixin, BaseConfig):
    """Reference: https://cloud.ibm.com/apidocs/watsonx-ai#text-generation
    (See ibm_watsonx_ai.metanames.GenTextParamsMetaNames for a list of all available params)

    Supported params for all available watsonx.ai foundational models.

    - `decoding_method` (str): One of "greedy" or "sample"

    - `temperature` (float): Sets the model temperature for sampling - not available when decoding_method='greedy'.

    - `max_new_tokens` (integer): Maximum length of the generated tokens.

    - `min_new_tokens` (integer): Maximum length of input tokens. Any more than this will be truncated.

    - `length_penalty` (dict): A dictionary with keys "decay_factor" and "start_index".

    - `stop_sequences` (string[]): list of strings to use as stop sequences.

    - `top_k` (integer): top k for sampling - not available when decoding_method='greedy'.

    - `top_p` (integer): top p for sampling - not available when decoding_method='greedy'.

    - `repetition_penalty` (float): token repetition penalty during text generation.

    - `truncate_input_tokens` (integer): Truncate input tokens to this length.

    - `include_stop_sequences` (bool): If True, the stop sequence will be included at the end of the generated text in the case of a match.

    - `return_options` (dict): A dictionary of options to return. Options include "input_text", "generated_tokens", "input_tokens", "token_ranks". Values are boolean.

    - `random_seed` (integer): Random seed for text generation.

    - `moderations` (dict): Dictionary of properties that control the moderations, for usages such as Hate and profanity (HAP) and PII filtering.

    - `stream` (bool): If True, the model will return a stream of responses."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/completion/transformation.py Line: 362
class WatsonxTextCompletionResponseIterator(BaseModelResponseIterator):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/watsonx/embed/transformation.py Line: 20
class IBMWatsonXEmbeddingConfig(IBMWatsonXMixin, BaseEmbeddingConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/litellm_proxy/chat/transformation.py Line: 12
class LiteLLMProxyChatConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/xai/chat/transformation.py Line: 16
class XAIChatConfig(OpenAIGPTConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/github/chat/transformation.py Line: 8
class GithubChatConfig(OpenAILikeChatConfig):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/codestral/completion/handler.py Line: 24
class TextCompletionCodestralError(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/codestral/completion/handler.py Line: 52
async def make_call(client: AsyncHTTPHandler, api_base: str, headers: dict, data: str, model: str, messages: list, logging_obj):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/codestral/completion/handler.py Line: 80
class CodestralTextCompletion:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/llms/codestral/completion/transformation.py Line: 9
class CodestralTextCompletionConfig(OpenAITextCompletionConfig):
    "Reference: https://docs.mistral.ai/api/#operation/createFIMCompletion"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/passthrough_endpoints/pass_through_endpoints.py Line: 5
class EndpointType(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/passthrough_endpoints/pass_through_endpoints.py Line: 11
class PassthroughStandardLoggingPayload(TypedDict, total=...):
    "Standard logging payload for all pass through endpoints"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/passthrough_endpoints/vertex_ai.py Line: 12
class VertexPassThroughCredentials(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/databricks.py Line: 18
class GenericStreamingChunk(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/databricks.py Line: 27
class DatabricksTextContent(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/databricks.py Line: 32
class DatabricksReasoningSummary(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/databricks.py Line: 38
class DatabricksReasoningContent(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/databricks.py Line: 50
class DatabricksFunction(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/databricks.py Line: 57
class DatabricksTool(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/databricks.py Line: 62
class DatabricksMessage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/databricks.py Line: 68
class DatabricksChoice(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/databricks.py Line: 75
class DatabricksResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/mistral.py Line: 4
class FunctionCall(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/mistral.py Line: 9
class MistralToolCallMessage(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/watsonx.py Line: 8
class WatsonXAPIParams(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/watsonx.py Line: 14
class WatsonXCredentials(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/watsonx.py Line: 20
class WatsonXAIEndpoint(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/custom_http.py Line: 6
class httpxSpecialProvider(str, Enum):
    """Httpx Clients can be created for these litellm internal providers

    Example:
    - langsmith logging would need a custom async httpx client
    - pass through endpoint would need a custom async httpx client"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/custom_llm.py Line: 8
class CustomLLMItem(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 79
class HttpxBinaryResponseContent(_HttpxBinaryResponseContent):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 84
class NotGiven:
    """A sentinel singleton class used to distinguish omitted keyword arguments
    from those passed in with the value None (which may have different behavior).

    For example:

    ```py
    def get(timeout: Union[int, NotGiven, None] = NotGiven()) -> Response:
        ...


    get(timeout=1)  # 1s timeout
    get(timeout=None)  # No timeout
    get()  # Default timeout behavior, which may not be statically known at the method definition.
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 113
class ToolResourcesCodeInterpreter(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 122
class ToolResourcesFileSearchVectorStore(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 139
class ToolResourcesFileSearch(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 157
class OpenAICreateThreadParamsToolResources(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 163
class FileSearchToolParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 168
class CodeInterpreterToolParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 176
class Attachment(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 184
class ImageFileObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 189
class ImageURLObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 194
class MessageContentTextObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 199
class MessageContentImageFileObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 204
class MessageContentImageURLObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 209
class MessageData(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 225
class Thread(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 258
class OpenAIFileObject(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 324
class CreateFileRequest(TypedDict, total=...):
    """CreateFileRequest
    Used by Assistants API, Batches API, and Fine-Tunes API

    Required Params:
        file: FileTypes
        purpose: Literal['assistants', 'batch', 'fine-tune']

    Optional Params:
        extra_headers: Optional[Dict[str, str]]
        extra_body: Optional[Dict[str, str]] = None
        timeout: Optional[float] = None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 346
class FileContentRequest(TypedDict, total=...):
    """FileContentRequest
    Used by Assistants API, Batches API, and Fine-Tunes API

    Required Params:
        file_id: str

    Optional Params:
        extra_headers: Optional[Dict[str, str]]
        extra_body: Optional[Dict[str, str]] = None
        timeout: Optional[float] = None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 367
class CreateBatchRequest(TypedDict, total=...):
    "CreateBatchRequest"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 381
class RetrieveBatchRequest(TypedDict, total=...):
    "RetrieveBatchRequest"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 392
class CancelBatchRequest(TypedDict, total=...):
    "CancelBatchRequest"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 403
class ListBatchRequest(TypedDict, total=...):
    """ListBatchRequest - List your organization's batches
    Calls https://api.openai.com/v1/batches"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 428
class ChatCompletionAudioDelta(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 435
class ChatCompletionToolCallFunctionChunk(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 440
class ChatCompletionAssistantToolCall(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 446
class ChatCompletionToolCallChunk(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 453
class ChatCompletionDeltaToolCallChunk(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 460
class ChatCompletionCachedContent(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 464
class ChatCompletionThinkingBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 471
class ChatCompletionRedactedThinkingBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 477
class WebSearchOptionsUserLocationApproximate(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 497
class WebSearchOptionsUserLocation(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 505
class WebSearchOptions(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 516
class FileSearchTool(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 524
class ChatCompletionAnnotationURLCitation(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 538
class ChatCompletionAnnotation(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 546
class OpenAIChatCompletionTextObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 551
class ChatCompletionTextObject(OpenAIChatCompletionTextObject, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 557
class ChatCompletionImageUrlObject(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 563
class ChatCompletionImageObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 568
class ChatCompletionVideoUrlObject(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 573
class ChatCompletionVideoObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 578
class ChatCompletionAudioObject(ChatCompletionContentPartInputAudioParam):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 582
class DocumentObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 588
class CitationsObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 592
class ChatCompletionDocumentObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 600
class ChatCompletionFileObjectFile(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 607
class ChatCompletionFileObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 630
class OpenAIChatCompletionUserMessage(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 635
class OpenAITextCompletionUserMessage(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 640
class ChatCompletionUserMessage(OpenAIChatCompletionUserMessage, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 644
class OpenAIChatCompletionAssistantMessage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 657
class ChatCompletionAssistantMessage(OpenAIChatCompletionAssistantMessage, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 662
class ChatCompletionToolMessage(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 668
class ChatCompletionFunctionMessage(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 675
class OpenAIChatCompletionSystemMessage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 681
class OpenAIChatCompletionDeveloperMessage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 687
class ChatCompletionSystemMessage(OpenAIChatCompletionSystemMessage, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 691
class ChatCompletionDeveloperMessage(OpenAIChatCompletionDeveloperMessage, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 695
class GenericChatCompletionMessage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 719
class ChatCompletionToolChoiceFunctionParam(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 723
class ChatCompletionToolChoiceObjectParam(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 735
class ChatCompletionToolParamFunctionChunk(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 742
class OpenAIChatCompletionToolParam(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 747
class ChatCompletionToolParam(OpenAIChatCompletionToolParam, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 751
class Function(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 756
class ChatCompletionNamedToolChoiceParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 763
class ChatCompletionRequest(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 789
class ChatCompletionDeltaChunk(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 800
class ChatCompletionResponseMessage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 812
class ChatCompletionUsageBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 818
class OpenAIChatCompletionChunk(ChatCompletionChunk):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 825
class Hyperparameters(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 835
class FineTuningJobCreate(BaseModel):
    """FineTuningJobCreate - Create a fine-tuning job

    Example Request
    ```
    {
        "model": "gpt-3.5-turbo",
        "training_file": "file-abc123",
        "hyperparameters": {
            "batch_size": "auto",
            "learning_rate_multiplier": 0.1,
            "n_epochs": 3
        },
        "suffix": "custom-model-name",
        "validation_file": "file-xyz789",
        "integrations": ["slack"],
        "seed": 42
    }
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 874
class LiteLLMFineTuningJobCreate(FineTuningJobCreate):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 896
class ComputerToolParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 909
class ResponsesAPIOptionalRequestParams(TypedDict, total=...):
    "TypedDict for Optional parameters supported by the responses API."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 930
class ResponsesAPIRequestParams(ResponsesAPIOptionalRequestParams, total=...):
    "TypedDict for request parameters supported by the responses API."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 937
class OutputTokensDetails(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 945
class InputTokensDetails(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 953
class ResponseAPIUsage(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 972
class ResponsesAPIResponse(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1002
class ResponsesAPIStreamEvents(str, Enum):
    """Enum representing all supported OpenAI stream event types for the Responses API.

    Inherits from str to allow direct string comparison and usage as dictionary keys."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1054
class ResponseCreatedEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1059
class ResponseInProgressEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1064
class ResponseCompletedEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1070
class ResponseFailedEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1075
class ResponseIncompleteEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1080
class OutputItemAddedEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1086
class OutputItemDoneEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1092
class ContentPartAddedEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1100
class ContentPartDoneEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1108
class OutputTextDeltaEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1116
class OutputTextAnnotationAddedEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1125
class OutputTextDoneEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1133
class RefusalDeltaEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1141
class RefusalDoneEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1149
class FunctionCallArgumentsDeltaEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1156
class FunctionCallArgumentsDoneEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1163
class FileSearchCallInProgressEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1169
class FileSearchCallSearchingEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1175
class FileSearchCallCompletedEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1181
class WebSearchCallInProgressEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1187
class WebSearchCallSearchingEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1193
class WebSearchCallCompletedEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1199
class ErrorEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1206
class GenericEvent(BaseLiteLLMOpenAIResponseObject):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1247
class OpenAIRealtimeStreamSessionEvents(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openai.py Line: 1253
class OpenAIRealtimeStreamResponseBaseObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/azure_ai.py Line: 6
class ImageEmbeddingInput(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/azure_ai.py Line: 14
class ImageEmbeddingRequest(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 16
class FunctionResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 21
class FunctionCall(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 26
class FileDataType(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 31
class BlobType(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 36
class PartType(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 45
class HttpxFunctionCall(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 50
class HttpxExecutableCode(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 55
class HttpxCodeExecutionResult(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 60
class HttpxBlobType(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 65
class HttpxPartType(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 76
class HttpxContentType(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 81
class ContentType(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 86
class SystemInstructions(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 90
class Schema(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 115
class FunctionDeclaration(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 122
class VertexAISearch(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 126
class Retrieval(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 130
class FunctionCallingConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 164
class SafetSettingsConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 171
class GeminiThinkingConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 176
class GenerationConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 194
class Tools(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 203
class ToolConfig(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 207
class TTL(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 212
class PromptTokensDetails(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 217
class UsageMetadata(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 226
class CachedContent(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 242
class RequestBody(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 252
class CachedContentRequestBody(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 262
class CachedContentListAllResponseBody(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 267
class SafetyRatings(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 275
class Date(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 281
class Citation(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 290
class CitationMetadata(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 294
class SearchEntryPoint(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 299
class GroundingMetadata(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 305
class LogprobsCandidate(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 311
class LogprobsTopCandidate(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 315
class LogprobsResult(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 320
class Candidates(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 341
class PromptFeedback(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 347
class GenerateContentResponseBody(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 353
class FineTuneHyperparameters(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 367
class FineTunesupervisedTuningSpec(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 374
class FineTuneJobCreate(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 380
class ResponseSupervisedTuningSpec(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 385
class ResponseTuningJob(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 403
class VideoSegmentConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 409
class InstanceVideo(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 414
class InstanceImage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 420
class Instance(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 426
class VertexMultimodalEmbeddingRequest(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 430
class VideoEmbedding(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 436
class MultimodalPrediction(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 442
class MultimodalPredictions(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 446
class VertexAICachedContentResponseObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 451
class TaskTypeEnum(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 462
class VertexAITextEmbeddingsRequestBody(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 469
class ContentEmbeddings(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 473
class VertexAITextEmbeddingsResponseObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 477
class EmbedContentRequest(VertexAITextEmbeddingsRequestBody):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 481
class VertexAIBatchEmbeddingsRequestBody(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 485
class VertexAIBatchEmbeddingsResponseObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 492
class GcsSource(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 496
class InputConfig(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 501
class GcsDestination(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 505
class OutputConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 510
class GcsBucketResponse(TypedDict):
    """TypedDict for GCS bucket upload response

    Attributes:
        kind: The kind of item this is. For objects, this is always storage#object
        id: The ID of the object
        selfLink: The link to this object
        mediaLink: The link to download the object
        name: The name of the object
        bucket: The name of the bucket containing this object
        generation: The content generation of this object
        metageneration: The metadata generation of this object
        contentType: The content type of the object
        storageClass: The storage class of the object
        size: The size of the object in bytes
        md5Hash: The MD5 hash of the object
        crc32c: The CRC32c checksum of the object
        etag: The ETag of the object
        timeCreated: The creation time of the object
        updated: The last update time of the object
        timeStorageClassUpdated: The time the storage class was last updated
        timeFinalized: The time the object was finalized"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 555
class VertexAIBatchPredictionJob(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/vertex_ai.py Line: 562
class VertexBatchPredictionResponse(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/rerank.py Line: 16
class InfinityRerankResult(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 9
class AnthropicMessagesToolChoice(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 15
class AnthropicInputSchema(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 21
class AnthropicMessagesTool(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 29
class AnthropicComputerTool(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 38
class AnthropicHostedTools(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 49
class AnthropicMessagesTextParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 55
class AnthropicMessagesToolUseParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 70
class AnthopicMessagesAssistantMessageParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 85
class AnthropicContentParamSource(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 91
class AnthropicMessagesImageParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 97
class CitationsObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 101
class AnthropicMessagesDocumentParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 110
class AnthropicMessagesToolResultContent(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 115
class AnthropicMessagesToolResultParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 136
class AnthropicMessagesUserMessageParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 141
class AnthropicMetadata(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 145
class AnthropicSystemMessageContent(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 156
class AnthropicMessageRequestBase(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 170
class AnthropicMessagesRequest(AnthropicMessageRequestBase, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 176
class ContentTextBlockDelta(TypedDict):
    "'delta': {'type': 'text_delta', 'text': 'Hello'}"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 185
class ContentCitationsBlockDelta(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 190
class ContentJsonBlockDelta(TypedDict):
    "\"delta\": {\"type\": \"input_json_delta\",\"partial_json\": \"{\"location\": \"San Fra\"}}"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 199
class ContentBlockDelta(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 207
class ContentBlockStop(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 212
class ToolUseBlock(TypedDict):
    "\"content_block\":{\"type\":\"tool_use\",\"id\":\"toolu_01T1x1fJ34qAmk2tNTrN7Up6\",\"name\":\"get_weather\",\"input\":{}}"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 226
class TextBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 232
class ContentBlockStart(TypedDict):
    """event: content_block_start
    data: {"type":"content_block_start","index":1,"content_block":{"type":"tool_use","id":"toolu_01T1x1fJ34qAmk2tNTrN7Up6","name":"get_weather","input":{}}}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 243
class MessageDelta(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 247
class UsageDelta(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 252
class MessageBlockDelta(TypedDict):
    """Anthropic
    chunk = {'type': 'message_delta', 'delta': {'stop_reason': 'max_tokens', 'stop_sequence': None}, 'usage': {'output_tokens': 10}}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 263
class MessageChunk(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 274
class MessageStartBlock(TypedDict):
    """    Anthropic
        chunk = {
        "type": "message_start",
        "message": {
            "id": "msg_vrtx_011PqREFEMzd3REdCoUFAmdG",
            "type": "message",
            "role": "assistant",
            "model": "claude-3-sonnet-20240229",
            "content": [],
            "stop_reason": null,
            "stop_sequence": null,
            "usage": {
                "input_tokens": 270,
                "output_tokens": 1
            }
        }
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 299
class AnthropicResponseContentBlockText(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 304
class AnthropicResponseContentBlockToolUse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 311
class AnthropicResponseUsageBlock(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 319
class AnthropicResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 350
class AnthropicChatCompletionUsageBlock(ChatCompletionUsageBlock, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic.py Line: 365
class AnthropicThinkingParam(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/openrouter.py Line: 6
class OpenRouterErrorMessage(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 18
class CachePointBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 22
class SystemContentBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 27
class SourceBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 34
class ImageBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 44
class DocumentBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 50
class ToolResultContentBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 57
class ToolResultBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 63
class ToolUseBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 69
class BedrockConverseReasoningTextBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 74
class BedrockConverseReasoningContentBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 79
class BedrockConverseReasoningContentBlockDelta(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 85
class ContentBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 95
class MessageBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 100
class ConverseMetricsBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 104
class ConverseResponseOutputBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 108
class ConverseTokenUsageBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 118
class ConverseResponseBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 128
class ToolInputSchemaBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 132
class ToolSpecBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 138
class ToolBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 142
class SpecificToolChoiceBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 146
class ToolChoiceValuesBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 152
class ToolConfigBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 157
class GuardrailConfigBlock(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 163
class InferenceConfig(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 171
class ToolBlockDeltaEvent(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 175
class ToolUseBlockStartEvent(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 180
class ContentBlockStartEvent(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 185
class ContentBlockDeltaEvent(TypedDict, total=...):
    "Either 'text' or 'toolUse' will be specified for Converse API streaming response."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 195
class PerformanceConfigBlock(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 199
class CommonRequestObject(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 211
class RequestObject(CommonRequestObject, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 215
class BedrockInvokeNovaRequest(TypedDict, total=...):
    "Request object for sending `nova` requests to `/bedrock/invoke/`"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 227
class GenericStreamingChunk(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 236
class Document(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 241
class ServerSentEvent:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 287
class CohereEmbeddingRequest(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 295
class CohereEmbeddingRequestWithModel(CohereEmbeddingRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 299
class CohereEmbeddingResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 306
class AmazonTitanV2EmbeddingRequest(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 312
class AmazonTitanV2EmbeddingResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 317
class AmazonTitanG1EmbeddingRequest(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 321
class AmazonTitanG1EmbeddingResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 326
class AmazonTitanMultimodalEmbeddingConfig(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 330
class AmazonTitanMultimodalEmbeddingRequest(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 336
class AmazonTitanMultimodalEmbeddingResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 349
class AmazonStability3TextToImageRequest(TypedDict, total=...):
    """Request for Amazon Stability 3 Text to Image API

    Ref here: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-diffusion-3-text-image.html"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 366
class AmazonStability3TextToImageResponse(TypedDict, total=...):
    """Response for Amazon Stability 3 Text to Image API

    Ref: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-diffusion-3-text-image.html"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 378
class AmazonNovaCanvasRequestBase(TypedDict, total=...):
    "Base class for Amazon Nova Canvas API requests"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 386
class AmazonNovaCanvasImageGenerationConfig(TypedDict, total=...):
    """Config for Amazon Nova Canvas Text to Image API

    Ref: https://docs.aws.amazon.com/nova/latest/userguide/image-gen-req-resp-structure.html"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 401
class AmazonNovaCanvasTextToImageParams(TypedDict, total=...):
    "Params for Amazon Nova Canvas Text to Image API"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 413
class AmazonNovaCanvasTextToImageRequest(AmazonNovaCanvasRequestBase, TypedDict, total=...):
    """Request for Amazon Nova Canvas Text to Image API

    Ref: https://docs.aws.amazon.com/nova/latest/userguide/image-gen-req-resp-structure.html"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 427
class AmazonNovaCanvasColorGuidedGenerationParams(TypedDict, total=...):
    "Params for Amazon Nova Canvas Color Guided Generation API"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 438
class AmazonNovaCanvasColorGuidedRequest(AmazonNovaCanvasRequestBase, TypedDict, total=...):
    """Request for Amazon Nova Canvas Color Guided Generation API

    Ref: https://docs.aws.amazon.com/nova/latest/userguide/image-gen-req-resp-structure.html"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 452
class AmazonNovaCanvasTextToImageResponse(TypedDict, total=...):
    """Response for Amazon Nova Canvas Text to Image API

    Ref: https://docs.aws.amazon.com/nova/latest/userguide/image-gen-req-resp-structure.html"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 470
class BedrockPreparedRequest(TypedDict):
    "Internal/Helper class for preparing the request for bedrock image generation"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 481
class BedrockRerankTextQuery(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 485
class BedrockRerankQuery(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 490
class BedrockRerankModelConfiguration(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 495
class BedrockRerankBedrockRerankingConfiguration(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 500
class BedrockRerankConfiguration(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 505
class BedrockRerankTextDocument(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 509
class BedrockRerankInlineDocumentSource(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 515
class BedrockRerankSource(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 520
class BedrockRerankRequest(TypedDict):
    "Request for Bedrock Rerank API"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/bedrock.py Line: 530
class AmazonDeepSeekR1StreamingResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 6
class CallObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 11
class ToolResultObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 16
class ChatHistoryToolResult(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 21
class ToolCallObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 26
class ChatHistoryUser(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 32
class ChatHistorySystem(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 38
class ChatHistoryChatBot(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 49
class CohereV2ChatResponseMessageToolCallFunction(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 54
class CohereV2ChatResponseMessageToolCall(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 60
class CohereV2ChatResponseMessageContent(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 66
class CohereV2ChatResponseMessage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 74
class CohereV2ChatResponseUsageBilledUnits(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 81
class CohereV2ChatResponseUsageTokens(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 86
class CohereV2ChatResponseUsage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 91
class CohereV2ChatResponseLogProbs(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/cohere.py Line: 97
class CohereV2ChatResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/gemini.py Line: 7
class GeminiFilesState(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/gemini.py Line: 14
class GeminiFilesSource(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/gemini.py Line: 20
class GeminiCreateFilesResponseObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/ollama.py Line: 16
class OllamaToolCallFunction(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/ollama.py Line: 23
class OllamaToolCall(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/ollama.py Line: 27
class OllamaVisionModelObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/base.py Line: 4
class BaseLiteLLMOpenAIResponseObject(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/mcp_server/tool_registry.py Line: 6
class MCPTool(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/mcp_server/tool_registry.py Line: 16
class ToolSchema(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/mcp_server/tool_registry.py Line: 22
class ListToolsResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/mcp_server/tool_registry.py Line: 28
class CallToolRequest(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/mcp_server/tool_registry.py Line: 33
class ContentItem(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/mcp_server/mcp_server_manager.py Line: 7
class MCPInfo(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/mcp_server/mcp_server_manager.py Line: 12
class MCPSSEServer(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/responses/main.py Line: 9
class GenericResponseOutputItemContentAnnotation(BaseLiteLLMOpenAIResponseObject):
    "Annotation for content in a message"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/responses/main.py Line: 20
class OutputText(BaseLiteLLMOpenAIResponseObject):
    "Text output content from an assistant message"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/responses/main.py Line: 28
class OutputFunctionToolCall(BaseLiteLLMOpenAIResponseObject):
    "A tool call to run a function"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/responses/main.py Line: 39
class GenericResponseOutputItem(BaseLiteLLMOpenAIResponseObject):
    "Generic response API output item"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/responses/main.py Line: 52
class DeleteResponseResult(BaseLiteLLMOpenAIResponseObject):
    """Result of a delete response request

    {
        "id": "resp_6786a1bec27481909a17d673315b29f6",
        "object": "response",
        "deleted": true
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/responses/main.py Line: 71
class DecodedResponseId(TypedDict, total=...):
    "Structure representing a decoded response ID"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/prometheus.py Line: 54
class UserAPIKeyLabelNames(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/prometheus.py Line: 95
class PrometheusMetricLabels:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/prometheus.py Line: 239
class UserAPIKeyLabelValues(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/argilla.py Line: 7
class ArgillaItem(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/argilla.py Line: 11
class ArgillaPayload(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/argilla.py Line: 15
class ArgillaCredentialsObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/slack_alerting.py Line: 14
class BaseOutageModel(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/slack_alerting.py Line: 21
class OutageModel(BaseOutageModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/slack_alerting.py Line: 25
class ProviderRegionOutageModel(BaseOutageModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/slack_alerting.py Line: 35
class SlackAlertingArgsEnum(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/slack_alerting.py Line: 46
class SlackAlertingArgs(LiteLLMPydanticObjectBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/slack_alerting.py Line: 90
class DeploymentMetrics(LiteLLMPydanticObjectBase):
    """Metrics per deployment, stored in cache

    Used for daily reporting"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/slack_alerting.py Line: 110
class SlackAlertingCacheKeys(Enum):
    "Enum for deployment daily metrics keys - {deployment_id}:{enum}"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/slack_alerting.py Line: 120
class AlertType(str, Enum):
    "Enum for alert types and management event types"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/gcs_bucket.py Line: 15
class GCSLoggingConfig(TypedDict):
    "Internal LiteLLM Config for GCS Bucket logging"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/gcs_bucket.py Line: 25
class GCSLogQueueItem(TypedDict):
    "Internal Type, used for queueing logs to be sent to GCS Bucket"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/arize_phoenix.py Line: 8
class ArizePhoenixConfig(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/arize.py Line: 11
class ArizeConfig(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/langsmith.py Line: 8
class LangsmithInputs(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/langsmith.py Line: 29
class LangsmithCredentialsObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/langsmith.py Line: 35
class LangsmithQueueObject(TypedDict):
    """Langsmith Queue Object - this is what gets stored in the internal system queue before flushing to Langsmith

    We need to store:
        - data[Dict] - data that should get logged on langsmith
        - credentials[LangsmithCredentialsObject] - credentials to use for logging to langsmith"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/langsmith.py Line: 48
class CredentialsKey(NamedTuple):
    "Immutable key for grouping credentials"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/langsmith.py Line: 57
class BatchGroup:
    "Groups credentials with their associated queue objects"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/pagerduty.py Line: 7
class LinkDict(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/pagerduty.py Line: 12
class ImageDict(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/pagerduty.py Line: 18
class PagerDutyPayload(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/pagerduty.py Line: 29
class PagerDutyRequestBody(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/pagerduty.py Line: 40
class AlertingConfig(TypedDict, total=...):
    "Config for alerting thresholds"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/pagerduty.py Line: 55
class PagerDutyInternalEvent(StandardLoggingUserAPIKeyMetadata, total=...):
    "Simple structure to hold timestamp and error info."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/base_health_check.py Line: 4
class IntegrationHealthCheckStatus(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog.py Line: 7
class DataDogStatus(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog.py Line: 13
class DatadogPayload(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog.py Line: 22
class DD_ERRORS(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog.py Line: 26
class DatadogProxyFailureHookJsonMessage(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/anthropic_cache_control_hook.py Line: 6
class CacheControlMessageInjectionPoint(TypedDict):
    "Type for message-level injection points."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog_llm_obs.py Line: 10
class InputMeta(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog_llm_obs.py Line: 16
class OutputMeta(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog_llm_obs.py Line: 20
class Meta(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog_llm_obs.py Line: 28
class LLMMetrics(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog_llm_obs.py Line: 36
class LLMObsPayload(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog_llm_obs.py Line: 48
class DDSpanAttributes(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/datadog_llm_obs.py Line: 54
class DDIntakePayload(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/integrations/langfuse.py Line: 4
class LangfuseLoggingConfig(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic_messages/anthropic_response.py Line: 6
class AnthropicResponseTextBlock(TypedDict, total=...):
    "Anthropic Response Text Block: https://docs.anthropic.com/en/api/messages"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic_messages/anthropic_response.py Line: 16
class AnthropicResponseToolUseBlock(TypedDict, total=...):
    "Anthropic Response Tool Use Block: https://docs.anthropic.com/en/api/messages"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic_messages/anthropic_response.py Line: 27
class AnthropicResponseThinkingBlock(TypedDict, total=...):
    "Anthropic Response Thinking Block: https://docs.anthropic.com/en/api/messages"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic_messages/anthropic_response.py Line: 37
class AnthropicResponseRedactedThinkingBlock(TypedDict, total=...):
    "Anthropic Response Redacted Thinking Block: https://docs.anthropic.com/en/api/messages"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic_messages/anthropic_response.py Line: 54
class AnthropicUsage(TypedDict, total=...):
    "Input and output tokens used in the request"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/llms/anthropic_messages/anthropic_response.py Line: 69
class AnthropicMessagesResponse(TypedDict, total=...):
    "Anthropic Messages API Response: https://docs.anthropic.com/en/api/messages"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/ui_sso.py Line: 8
class MicrosoftGraphAPIUserGroupDirectoryObject(TypedDict, total=...):
    "Model for Microsoft Graph API directory object"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/ui_sso.py Line: 19
class MicrosoftGraphAPIUserGroupResponse(TypedDict, total=...):
    "Model for Microsoft Graph API user groups response"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/ui_sso.py Line: 27
class MicrosoftServicePrincipalTeam(TypedDict, total=...):
    "Model for Microsoft Service Principal Team"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/ui_sso.py Line: 34
class DefaultTeamSSOParams(LiteLLMPydanticObjectBase):
    "Default parameters to apply when a new team is automatically created by LiteLLM via SSO Groups"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 7
class LiteLLM_UserScimMetadata(BaseModel):
    "Scim metadata stored in LiteLLM_UserTable.metadata"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 17
class SCIMResource(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 24
class SCIMUserName(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 33
class SCIMUserEmail(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 39
class SCIMUserGroup(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 45
class SCIMUser(SCIMResource):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 54
class SCIMMember(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 59
class SCIMGroup(SCIMResource):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 65
class SCIMListResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 74
class SCIMPatchOperation(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/scim_v2.py Line: 80
class SCIMPatchOp(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/team_endpoints.py Line: 6
class GetTeamMemberPermissionsRequest(BaseModel):
    "Request to get the team member permissions for a team"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/team_endpoints.py Line: 12
class GetTeamMemberPermissionsResponse(BaseModel):
    "Response to get the team member permissions for a team"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/team_endpoints.py Line: 31
class UpdateTeamMemberPermissionsRequest(BaseModel):
    "Request to update the team member permissions for a team"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 8
class GroupByDimension(str, Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 18
class SpendMetrics(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 30
class MetricBase(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 34
class MetricWithMetadata(MetricBase):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 38
class KeyMetadata(BaseModel):
    "Metadata for a key"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 45
class KeyMetricWithMetadata(MetricBase):
    "Base class for metrics with additional metadata"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 51
class BreakdownMetrics(BaseModel):
    "Breakdown of spend by different dimensions"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 68
class DailySpendData(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 74
class DailySpendMetadata(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 89
class SpendAnalyticsPaginatedResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 94
class LiteLLM_DailyUserSpend(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/common_daily_activity.py Line: 112
class GroupedData(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/types/proxy/management_endpoints/internal_user_endpoints.py Line: 9
class UserListResponse(BaseModel):
    "Response model for the user list endpoint"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/litellm_completion_transformation/handler.py Line: 23
class LiteLLMCompletionTransformationHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/litellm_completion_transformation/streaming_iterator.py Line: 26
class LiteLLMCompletionStreamingIterator(ResponsesAPIStreamingIterator):
    "Async iterator for processing streaming responses from the Responses API."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/litellm_completion_transformation/transformation.py Line: 55
class LiteLLMCompletionResponsesConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/litellm_completion_transformation/session_handler.py Line: 20
class ResponsesAPISessionElement(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/responses/litellm_completion_transformation/session_handler.py Line: 27
class SessionHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/azure_storage/azure_storage.py Line: 21
class AzureBlobStorageLogger(CustomBatchLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/agentops/agentops.py Line: 11
class AgentOpsConfig:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/agentops/agentops.py Line: 28
class AgentOps(OpenTelemetry):
    """AgentOps integration - built on top of OpenTelemetry

    Example usage:
        ```python
        import litellm
    
        litellm.success_callback = ["agentops"]

        response = litellm.completion(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello, how are you?"}],
        )
        ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/SlackAlerting/batching_handler.py Line: 21
def squash_payloads(queue):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/SlackAlerting/batching_handler.py Line: 43
def _print_alerting_payload_warning(payload: dict, slackAlertingInstance: SlackAlertingType):
    """Print the payload to the console when
    slackAlertingInstance.alerting_args.log_to_console is True

    Relevant issue: https://github.com/BerriAI/litellm/issues/7372"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/SlackAlerting/batching_handler.py Line: 56
async def send_to_webhook(slackAlertingInstance: SlackAlertingType, item, count):
    "Send a single slack alert to the webhook"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/SlackAlerting/utils.py Line: 19
def process_slack_alerting_variables(alert_to_webhook_url: Optional[Dict[(AlertType, Union[(List[str], str)])]]) -> Optional[Dict[(AlertType, Union[(List[str], str)])]]:
    """process alert_to_webhook_url
    - check if any urls are set as os.environ/SLACK_WEBHOOK_URL_1 read env var and set the correct value"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/SlackAlerting/utils.py Line: 61
async def _add_langfuse_trace_id_to_alert(request_data: Optional[dict]) -> Optional[str]:
    """Returns langfuse trace url

    - check:
    -> existing_trace_id
    -> trace_id
    -> litellm_call_id"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/SlackAlerting/slack_alerting.py Line: 44
class SlackAlerting(CustomBatchLogger):
    "Class for sending Slack Alerts"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prometheus_helpers/prometheus_api.py Line: 23
async def get_metric_from_prometheus(metric_name: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prometheus_helpers/prometheus_api.py Line: 43
async def get_fallback_metric_from_prometheus():
    "Gets fallback metrics from prometheus for the last 24 hours"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prometheus_helpers/prometheus_api.py Line: 78
def is_prometheus_connected() -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/prometheus_helpers/prometheus_api.py Line: 84
async def get_daily_spend_from_prometheus(api_key: Optional[str]):
    """Expected Response Format:
    [
    {
        "date": "2024-08-18T00:00:00+00:00",
        "spend": 1.001818099998933
    },
    ...]"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/arize/_utils.py Line: 16
def cast_as_primitive_value_type(value) -> Union[(str, bool, int, float)]:
    "Converts a value to an OTEL-supported primitive for Arize/Phoenix observability."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/arize/_utils.py Line: 30
def safe_set_attribute(span: Span, key: str, value: Any):
    "Sets a span attribute safely with OTEL-compliant primitive typing for Arize/Phoenix."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/arize/_utils.py Line: 38
def set_attributes(span: Span, kwargs, response_obj):
    "Populates span with OpenInference-compliant LLM attributes for Arize and Phoenix tracing."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/arize/arize_phoenix.py Line: 27
class ArizePhoenixLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/arize/arize.py Line: 28
class ArizeLogger(OpenTelemetry):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opik/utils.py Line: 9
def create_uuid7():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opik/utils.py Line: 40
def _read_opik_config_file() -> Dict[(str, str)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opik/utils.py Line: 56
def _get_env_variable(key: str) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opik/utils.py Line: 61
def get_opik_config_variable(key: str, user_value: Optional[str], default_value: Optional[str]) -> Optional[str]:
    """Get the configuration value of a variable, order priority is:
    1. user provided value
    2. environment variable
    3. Opik configuration file
    4. default value"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opik/utils.py Line: 90
def create_usage_object(usage):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opik/utils.py Line: 102
def _remove_nulls(x):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opik/utils.py Line: 107
def get_traces_and_spans_from_payload(payload: List):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/opik/opik.py Line: 26
class OpikLogger(CustomBatchLogger):
    "Opik Logger for logging events to an Opik Server"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/gcs_bucket/gcs_bucket_base.py Line: 21
class GCSBucketBase(CustomBatchLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/gcs_bucket/gcs_bucket.py Line: 23
class GCSBucketLogger(GCSBucketBase, AdditionalLoggingUtils):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/datadog/datadog.py Line: 52
class DataDogLogger(CustomBatchLogger, AdditionalLoggingUtils):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/datadog/datadog_llm_obs.py Line: 33
class DataDogLLMObsLogger(DataDogLogger, CustomBatchLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/pagerduty/pagerduty.py Line: 40
class PagerDutyAlerting(SlackAlerting):
    """Tracks failed requests and hanging requests separately.
    If threshold is crossed for either type, triggers a PagerDuty alert."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse_handler.py Line: 21
class LangFuseHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse_prompt_management.py Line: 40
def langfuse_client_init(langfuse_public_key, langfuse_secret, langfuse_secret_key, langfuse_host, flush_interval=1) -> LangfuseClass:
    """Initialize Langfuse client with caching to prevent multiple initializations.

    Args:
        langfuse_public_key (str, optional): Public key for Langfuse. Defaults to None.
        langfuse_secret (str, optional): Secret key for Langfuse. Defaults to None.
        langfuse_host (str, optional): Host URL for Langfuse. Defaults to None.
        flush_interval (int, optional): Flush interval in seconds. Defaults to 1.

    Returns:
        Langfuse: Initialized Langfuse client instance

    Raises:
        Exception: If langfuse package is not installed"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse_prompt_management.py Line: 108
class LangfusePromptManagement(LangFuseLogger, PromptManagementBase, CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse.py Line: 35
class LangFuseLogger:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse.py Line: 813
def _add_prompt_to_generation_params(generation_params: dict, clean_metadata: dict, prompt_management_metadata: Optional[StandardLoggingPromptManagementMetadata], langfuse_client: Any) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse.py Line: 902
def log_provider_specific_information_as_span(trace, clean_metadata):
    """Logs provider-specific information as spans.

    Parameters:
        trace: The tracing object used to log spans.
        clean_metadata: A dictionary containing metadata to be logged.

    Returns:
        None"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse.py Line: 946
def log_requester_metadata(clean_metadata: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 4
class SpanAttributes:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 162
class MessageAttributes:
    "Attributes for a message sent to or from an LLM"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 206
class MessageContentAttributes:
    "Attributes for the contents of user messages sent to an LLM."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 228
class ImageAttributes:
    "Attributes for images"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 239
class AudioAttributes:
    "Attributes for audio"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 258
class DocumentAttributes:
    "Attributes for a document."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 282
class RerankerAttributes:
    "Attributes for a reranker"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 309
class EmbeddingAttributes:
    "Attributes for an embedding"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 324
class ToolCallAttributes:
    "Attributes for a tool call"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 344
class ToolAttributes:
    "Attributes for a tools"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 356
class OpenInferenceSpanKindValues(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 369
class OpenInferenceMimeTypeValues(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 374
class OpenInferenceLLMSystemValues(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/_types/open_inference.py Line: 382
class OpenInferenceLLMProviderValues(Enum):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/integrations/gcs_pubsub/pub_sub.py Line: 31
class GcsPubSubLogger(CustomBatchLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/router_callbacks/track_deployment_metrics.py Line: 22
def increment_deployment_successes_for_current_minute(litellm_router_instance: LitellmRouter, deployment_id: str) -> str:
    "In-Memory: Increments the number of successes for the current minute for a deployment_id"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/router_callbacks/track_deployment_metrics.py Line: 39
def increment_deployment_failures_for_current_minute(litellm_router_instance: LitellmRouter, deployment_id: str):
    "In-Memory: Increments the number of failures for the current minute for a deployment_id"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/router_callbacks/track_deployment_metrics.py Line: 55
def get_deployment_successes_for_current_minute(litellm_router_instance: LitellmRouter, deployment_id: str) -> int:
    """Returns the number of successes for the current minute for a deployment_id

    Returns 0 if no value found"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/router_callbacks/track_deployment_metrics.py Line: 74
def get_deployment_failures_for_current_minute(litellm_router_instance: LitellmRouter, deployment_id: str) -> int:
    """Returns the number of fails for the current minute for a deployment_id

    Returns 0 if no value found"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/pre_call_checks/responses_api_deployment_check.py Line: 20
class ResponsesApiDeploymentCheck(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/router_utils/pre_call_checks/prompt_caching_deployment_check.py Line: 19
class PromptCachingDeploymentCheck(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/admin_ui_utils.py Line: 1
def show_missing_vars_in_env():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/admin_ui_utils.py Line: 26
def missing_keys_form(missing_key_names: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/admin_ui_utils.py Line: 99
def admin_ui_disabled():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/swagger_utils.py Line: 8
class ErrorResponse(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/swagger_utils.py Line: 23
def get_status_code(exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/encrypt_decrypt_utils.py Line: 8
def _get_salt_key():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/encrypt_decrypt_utils.py Line: 23
def encrypt_value_helper(value: str, new_encryption_key: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/encrypt_decrypt_utils.py Line: 42
def decrypt_value_helper(value: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/encrypt_decrypt_utils.py Line: 61
def encrypt_value(value: str, signing_key: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/encrypt_decrypt_utils.py Line: 82
def decrypt_value(value: bytes, signing_key: str) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/proxy_state.py Line: 12
class ProxyState:
    "Proxy state class has get/set methods for Proxy state variables."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/load_config_utils.py Line: 6
def get_file_contents_from_s3(bucket_name, object_key):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/load_config_utils.py Line: 53
async def get_config_file_contents_from_gcs(bucket_name, object_key):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/callback_utils.py Line: 13
def initialize_callbacks_on_proxy(value: Any, premium_user: bool, config_file_path: str, litellm_settings: dict, callback_specific_params: dict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/callback_utils.py Line: 242
def get_model_group_from_litellm_kwargs(kwargs: dict) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/callback_utils.py Line: 252
def get_model_group_from_request_data(data: dict) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/callback_utils.py Line: 261
def get_remaining_tokens_and_requests_from_request_data(data: Dict) -> Dict[(str, str)]:
    """Helper function to return x-litellm-key-remaining-tokens-{model_group} and x-litellm-key-remaining-requests-{model_group}

    Returns {} when api_key + model rpm/tpm limit is not set"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/callback_utils.py Line: 287
def get_logging_caching_headers(request_data: Dict) -> Optional[Dict]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/callback_utils.py Line: 301
def add_guardrail_to_applied_guardrails_header(request_data: Dict, guardrail_name: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/reset_budget_job.py Line: 18
class ResetBudgetJob:
    "Resets the budget for all the keys, users, and teams that need it"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/http_parsing_utils.py Line: 11
async def _read_request_body(request: Optional[Request]) -> Dict:
    """Safely read the request body and parse it as JSON.

    Parameters:
    - request: The request object to read the body from

    Returns:
    - dict: Parsed request data as a dictionary or an empty dictionary if parsing fails"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/http_parsing_utils.py Line: 81
def _safe_get_request_parsed_body(request: Optional[Request]) -> Optional[dict]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/http_parsing_utils.py Line: 94
def _safe_set_request_parsed_body(request: Optional[Request], parsed_body: dict) -> ?:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/http_parsing_utils.py Line: 108
def _safe_get_request_headers(request: Optional[Request]) -> dict:
    "[Non-Blocking] Safely get the request headers"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/http_parsing_utils.py Line: 123
def check_file_size_under_limit(request_data: dict, file: UploadFile, router_model_names: List[str]) -> bool:
    """Check if any files passed in request are under max_file_size_mb

    Returns True -> when file size is under max_file_size_mb limit
    Raises ProxyException -> when file size is over max_file_size_mb limit or not a premium_user"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/debug_utils.py Line: 48
async def memory_usage_in_mem_cache():
    """1. user_api_key_cache
    2. router_cache
    3. proxy_logging_cache
    4. internal_usage_cache"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/debug_utils.py Line: 85
async def memory_usage_in_mem_cache_items():
    """1. user_api_key_cache
    2. router_cache
    3. proxy_logging_cache
    4. internal_usage_cache"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/debug_utils.py Line: 117
async def get_otel_spans():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/debug_utils.py Line: 160
def init_verbose_loggers():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/openai_endpoint_utils.py Line: 12
def remove_sensitive_info_from_deployment(deployment_dict: dict) -> dict:
    """Removes sensitive information from a deployment dictionary.

    Args:
        deployment_dict (dict): The deployment dictionary to remove sensitive information from.

    Returns:
        dict: The modified deployment dictionary with sensitive information removed."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/common_utils/openai_endpoint_utils.py Line: 30
async def get_custom_llm_provider_from_request_body(request: Request) -> Optional[str]:
    """Get the `custom_llm_provider` from the request body

    Safely reads the request body"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/utils.py Line: 33
def get_new_internal_user_defaults(user_id: str, user_email: Optional[str]) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/utils.py Line: 56
async def add_new_member(new_member: Member, max_budget_in_team: Optional[float], prisma_client: PrismaClient, team_id: str, user_api_key_dict: UserAPIKeyAuth, litellm_proxy_admin_name: str) -> Tuple[(LiteLLM_UserTable, Optional[LiteLLM_TeamMembership])]:
    """Add a new member to a team

    - add team id to user table
    - add team member w/ budget to team member table

    Returns created/existing user + team membership w/ budget id"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/utils.py Line: 158
def _delete_user_id_from_cache(kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/utils.py Line: 173
def _delete_api_key_from_cache(kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/utils.py Line: 188
def _delete_team_id_from_cache(kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/utils.py Line: 203
def _delete_customer_id_from_cache(kwargs):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/utils.py Line: 218
async def send_management_endpoint_alert(request_kwargs: dict, user_api_key_dict: UserAPIKeyAuth, function_name: str):
    """Sends a slack alert when:
    - A virtual key is created, updated, or deleted
    - An internal user is created, updated, or deleted
    - A team is created, updated, or deleted"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/utils.py Line: 274
def management_endpoint_wrapper(func):
    """This wrapper does the following:

    1. Log I/O, Exceptions to OTEL
    2. Create an Audit log for success calls"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/team_member_permission_checks.py Line: 25
class TeamMemberPermissionChecks:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/audit_logs.py Line: 20
async def create_object_audit_log(object_id: str, action: AUDIT_ACTIONS, litellm_changed_by: Optional[str], user_api_key_dict: UserAPIKeyAuth, litellm_proxy_admin_name: Optional[str], table_name: LitellmTableNames, before_value: Optional[str], after_value: Optional[str]):
    """Create an audit log for an internal user.

    Parameters:
    - user_id: str - The id of the user to create the audit log for.
    - action: AUDIT_ACTIONS - The action to create the audit log for.
    - user_row: LiteLLM_UserTable - The user row to create the audit log for.
    - litellm_changed_by: Optional[str] - The user id of the user who is changing the user.
    - user_api_key_dict: UserAPIKeyAuth - The user api key dictionary.
    - litellm_proxy_admin_name: Optional[str] - The name of the proxy admin."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_helpers/audit_logs.py Line: 61
async def create_audit_log_for_update(request_data: LiteLLM_AuditLogs):
    "Create an audit log for an object."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/vertex_ai_endpoints/langfuse_endpoints.py Line: 31
def create_request_copy(request: Request):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/vertex_ai_endpoints/langfuse_endpoints.py Line: 46
async def langfuse_proxy_route(endpoint: str, request: Request, fastapi_response: Response):
    """Call Langfuse via LiteLLM proxy. Works with Langfuse SDK.

    [Docs](https://docs.litellm.ai/docs/pass_through/langfuse)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/analytics_endpoints/analytics_endpoints.py Line: 23
async def get_global_activity(start_date: Optional[str]=..., end_date: Optional[str]=...):
    """Get number of cache hits, vs misses

    {
        "daily_data": [
                const chartdata = [
                {
                    date: 'Jan 22',
                    cache_hits: 10,
                    llm_api_calls: 2000
                },
                {
                    date: 'Jan 23',
                    cache_hits: 10,
                    llm_api_calls: 12
                },
        ],
        "sum_cache_hits": 20,
        "sum_llm_api_calls": 2012
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_experimental/post_call_rules.py Line: 1
def my_custom_rule(input):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 48
def set_files_config(config):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 65
def get_files_provider_config(custom_llm_provider: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 79
def get_first_json_object(file_content_bytes: bytes) -> Optional[dict]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 92
def get_model_from_json_obj(json_object: dict) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 99
def is_known_model(model: Optional[str], llm_router: Optional[Router]) -> bool:
    "Returns True if the model is in the llm_router model names"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 114
async def _deprecated_loadbalanced_create_file(llm_router: Optional[Router], router_model: str, _create_file_request: CreateFileRequest) -> OpenAIFileObject:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 131
async def route_create_file(llm_router: Optional[Router], _create_file_request: CreateFileRequest, purpose: OpenAIFilesPurpose, proxy_logging_obj: ProxyLogging, user_api_key_dict: UserAPIKeyAuth, target_model_names_list: List[str], is_router_model: bool, router_model: Optional[str], custom_llm_provider: str) -> OpenAIFileObject:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 207
async def create_file(request: Request, fastapi_response: Response, purpose: str=..., target_model_names: str=..., provider: Optional[str], custom_llm_provider: str=..., file: UploadFile=..., user_api_key_dict: UserAPIKeyAuth=...):
    """Upload a file that can be used across - Assistants API, Batch API 
    This is the equivalent of POST https://api.openai.com/v1/files

    Supports Identical Params as: https://platform.openai.com/docs/api-reference/files/create

    Example Curl
    ```
    curl http://localhost:4000/v1/files         -H "Authorization: Bearer sk-1234"         -F purpose="batch"         -F file="@mydata.jsonl"

    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 389
async def get_file_content(request: Request, fastapi_response: Response, file_id: str, provider: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Returns information about a specific file. that can be used across - Assistants API, Batch API 
    This is the equivalent of GET https://api.openai.com/v1/files/{file_id}/content

    Supports Identical Params as: https://platform.openai.com/docs/api-reference/files/retrieve-contents

    Example Curl
    ```
    curl http://localhost:4000/v1/files/file-abc123/content         -H "Authorization: Bearer sk-1234"

    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 547
async def get_file(request: Request, fastapi_response: Response, file_id: str, provider: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Returns information about a specific file. that can be used across - Assistants API, Batch API 
    This is the equivalent of GET https://api.openai.com/v1/files/{file_id}

    Supports Identical Params as: https://platform.openai.com/docs/api-reference/files/retrieve

    Example Curl
    ```
    curl http://localhost:4000/v1/files/file-abc123         -H "Authorization: Bearer sk-1234"

    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 685
async def delete_file(request: Request, fastapi_response: Response, file_id: str, provider: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Deletes a specified file. that can be used across - Assistants API, Batch API 
    This is the equivalent of DELETE https://api.openai.com/v1/files/{file_id}

    Supports Identical Params as: https://platform.openai.com/docs/api-reference/files/delete

    Example Curl
    ```
    curl http://localhost:4000/v1/files/file-abc123     -X DELETE     -H "Authorization: Bearer $OPENAI_API_KEY"

    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/openai_files_endpoints/files_endpoints.py Line: 834
async def list_files(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=..., provider: Optional[str], purpose: Optional[str]):
    """Returns information about a specific file. that can be used across - Assistants API, Batch API 
    This is the equivalent of GET https://api.openai.com/v1/files/

    Supports Identical Params as: https://platform.openai.com/docs/api-reference/files/list

    Example Curl
    ```
    curl http://localhost:4000/v1/files        -H "Authorization: Bearer sk-1234"

    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/response_api_endpoints/endpoints.py Line: 20
async def responses_api(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Follows the OpenAI Responses API spec: https://platform.openai.com/docs/api-reference/responses

    ```bash
    curl -X POST http://localhost:4000/v1/responses     -H "Content-Type: application/json"     -H "Authorization: Bearer sk-1234"     -d '{
        "model": "gpt-4o",
        "input": "Tell me about AI"
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/response_api_endpoints/endpoints.py Line: 93
async def get_response(response_id: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Get a response by ID.

    Follows the OpenAI Responses API spec: https://platform.openai.com/docs/api-reference/responses/get

    ```bash
    curl -X GET http://localhost:4000/v1/responses/resp_abc123     -H "Authorization: Bearer sk-1234"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/response_api_endpoints/endpoints.py Line: 123
async def delete_response(response_id: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Delete a response by ID.

    Follows the OpenAI Responses API spec: https://platform.openai.com/docs/api-reference/responses/delete

    ```bash
    curl -X DELETE http://localhost:4000/v1/responses/resp_abc123     -H "Authorization: Bearer sk-1234"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/response_api_endpoints/endpoints.py Line: 153
async def get_response_input_items(response_id: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Get input items for a response.

    Follows the OpenAI Responses API spec: https://platform.openai.com/docs/api-reference/responses/input-items

    ```bash
    curl -X GET http://localhost:4000/v1/responses/resp_abc123/input_items     -H "Authorization: Bearer sk-1234"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/init_guardrails.py Line: 24
def initialize_guardrails(guardrails_config: List[Dict[(str, GuardrailItemSpec)]], premium_user: bool, config_file_path: str, litellm_settings: dict) -> Dict[(str, GuardrailItem)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/init_guardrails.py Line: 87
def init_guardrails_v2(all_guardrails: List[Dict], config_file_path: Optional[str]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_endpoints.py Line: 17
def _get_guardrails_list_response(guardrails_config: List[Dict]) -> ListGuardrailsResponse:
    "Helper function to get the guardrails list response"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_endpoints.py Line: 41
async def list_guardrails():
    """List the guardrails that are available on the proxy server

     [Guardrail docs](https://docs.litellm.ai/docs/proxy/guardrails/quick_start)

    Example Request:
    ```bash
    curl -X GET "http://localhost:4000/guardrails/list" -H "Authorization: Bearer <your_api_key>"
    ```

    Example Response:
    ```json
    {
        "guardrails": [
            {
            "guardrail_name": "bedrock-pre-guard",
            "guardrail_info": {
                "params": [
                {
                    "name": "toxicity_score",
                    "type": "float",
                    "description": "Score between 0-1 indicating content toxicity level"
                },
                {
                    "name": "pii_detection",
                    "type": "boolean"
                }
                ]
            }
            }
        ]
    }
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_initializers.py Line: 6
def initialize_aporia(litellm_params, guardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_initializers.py Line: 19
def initialize_bedrock(litellm_params, guardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_initializers.py Line: 34
def initialize_lakera(litellm_params, guardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_initializers.py Line: 48
def initialize_aim(litellm_params, guardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_initializers.py Line: 61
def initialize_presidio(litellm_params, guardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_initializers.py Line: 87
def initialize_hide_secrets(litellm_params, guardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_initializers.py Line: 99
def initialize_guardrails_ai(litellm_params, guardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_helpers.py Line: 15
def can_modify_guardrails(team_obj: Optional[LiteLLM_TeamTable]) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_helpers.py Line: 30
async def should_proceed_based_on_metadata(data: dict, guardrail_name: str) -> bool:
    "checks if this guardrail should be applied to this call"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_helpers.py Line: 78
async def should_proceed_based_on_api_key(user_api_key_dict: UserAPIKeyAuth, guardrail_name: str) -> bool:
    "checks if this guardrail should be applied to this call"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/batches_endpoints/endpoints.py Line: 47
async def create_batch(request: Request, fastapi_response: Response, provider: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Create large batches of API requests for asynchronous processing.
    This is the equivalent of POST https://api.openai.com/v1/batch
    Supports Identical Params as: https://platform.openai.com/docs/api-reference/batch

    Example Curl
    ```
    curl http://localhost:4000/v1/batches         -H "Authorization: Bearer sk-1234"         -H "Content-Type: application/json"         -d '{
            "input_file_id": "file-abc123",
            "endpoint": "/v1/chat/completions",
            "completion_window": "24h"
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/batches_endpoints/endpoints.py Line: 179
async def retrieve_batch(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=..., provider: Optional[str], batch_id: str=...):
    """Retrieves a batch.
    This is the equivalent of GET https://api.openai.com/v1/batches/{batch_id}
    Supports Identical Params as: https://platform.openai.com/docs/api-reference/batch/retrieve

    Example Curl
    ```
    curl http://localhost:4000/v1/batches/batch_abc123     -H "Authorization: Bearer sk-1234"     -H "Content-Type: application/json" 
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/batches_endpoints/endpoints.py Line: 307
async def list_batches(request: Request, fastapi_response: Response, provider: Optional[str], limit: Optional[int], after: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Lists 
    This is the equivalent of GET https://api.openai.com/v1/batches/
    Supports Identical Params as: https://platform.openai.com/docs/api-reference/batch/list

    Example Curl
    ```
    curl http://localhost:4000/v1/batches?limit=2     -H "Authorization: Bearer sk-1234"     -H "Content-Type: application/json" 
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/batches_endpoints/endpoints.py Line: 390
async def cancel_batch(request: Request, batch_id: str, fastapi_response: Response, provider: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Cancel a batch.
    This is the equivalent of POST https://api.openai.com/v1/batches/{batch_id}/cancel

    Supports Identical Params as: https://platform.openai.com/docs/api-reference/batch/cancel

    Example Curl
    ```
    curl http://localhost:4000/v1/batches/batch_abc123/cancel         -H "Authorization: Bearer sk-1234"         -H "Content-Type: application/json"         -X POST

    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/anthropic_endpoints/endpoints.py Line: 25
async def async_data_generator_anthropic(response, user_api_key_dict: UserAPIKeyAuth, request_data: dict, proxy_logging_obj: ProxyLogging):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/anthropic_endpoints/endpoints.py Line: 81
async def anthropic_response(fastapi_response: Response, request: Request, user_api_key_dict: UserAPIKeyAuth=...):
    """Use `{PROXY_BASE_URL}/anthropic/v1/messages` instead - [Docs](https://docs.litellm.ai/docs/anthropic_completion).

    This was a BETA endpoint that calls 100+ LLMs in the anthropic format."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/model_max_budget_limiter.py Line: 20
class _PROXY_VirtualKeyModelMaxBudgetLimiter(RouterBudgetLimiting):
    """Handles budgets for model + virtual key

    Example: key=sk-1234567890, model=gpt-4o, max_budget=100, time_period=1d"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/max_budget_limiter.py Line: 10
class _PROXY_MaxBudgetLimiter(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/prompt_injection_detection.py Line: 28
class _OPTIONAL_PromptInjectionDetection(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/azure_content_safety.py Line: 13
class _PROXY_AzureContentSafety(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/key_management_event_hooks.py Line: 30
class KeyManagementEventHooks:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/batch_redis_get.py Line: 18
class _PROXY_BatchRedisRequests(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/__init__.py Line: 18
def get_proxy_hook(hook_name: Union[(Literal[(?, ?, ?, ?)], str)]):
    "Factory method to get a proxy hook instance by name"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/parallel_request_limiter.py Line: 32
class CacheObject(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/parallel_request_limiter.py Line: 41
class _PROXY_MaxParallelRequestsHandler(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/managed_files.py Line: 39
class BaseFileEndpoints(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/managed_files.py Line: 61
class _PROXY_LiteLLMManagedFiles(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/dynamic_rate_limiter.py Line: 21
class DynamicRateLimiterCache:
    """Thin wrapper on DualCache for this file.

    Track number of active projects calling a model."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/dynamic_rate_limiter.py Line: 73
class _PROXY_DynamicRateLimitHandler(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/proxy_track_cost_callback.py Line: 25
class _ProxyDBLogger(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/proxy_track_cost_callback.py Line: 236
def _should_track_cost_callback(user_api_key: Optional[str], user_id: Optional[str], team_id: Optional[str], end_user_id: Optional[str]) -> bool:
    "Determine if the cost callback should be tracked based on the kwargs"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/hooks/cache_control_check.py Line: 14
class _PROXY_CacheControlCheck(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/types_utils/utils.py Line: 6
def get_instance_fn(value: str, config_file_path: Optional[str]) -> Any:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/types_utils/utils.py Line: 53
def security_checks(module_name: str):
    """This function checks if the module name contains any dangerous modules that can execute arbitrary code.

    Reference: https://huntr.com/bounties/1d98bebb-6cf4-46c9-87c3-d3b1972973b5"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/types_utils/utils.py Line: 82
def validate_custom_validate_return_type(fn: Optional[Callable[(?, Any)]]) -> Optional[Callable[(?, Literal[?])]]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/passthrough_endpoint_router.py Line: 9
class PassthroughEndpointRouter:
    "Use this class to Set/Get credentials for pass-through endpoints"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/success_handler.py Line: 32
class PassThroughEndpointLogging:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 56
def get_response_body(response: ?) -> Optional[dict]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 63
async def set_env_variables_in_header(custom_headers: Optional[dict]) -> Optional[dict]:
    """checks if any headers on config.yaml are defined as os.environ/COHERE_API_KEY etc

    only runs for headers defined on config.yaml

    example header can be

    {"Authorization": "bearer os.environ/COHERE_API_KEY"}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 116
async def chat_completion_pass_through_endpoint(fastapi_response: Response, request: Request, adapter_id: str, user_api_key_dict: UserAPIKeyAuth):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 279
class HttpPassThroughEndpointHelpers:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 462
async def pass_through_request(request: Request, target: str, custom_headers: dict, user_api_key_dict: UserAPIKeyAuth, custom_body: Optional[dict], forward_headers: Optional[bool], merge_query_params: Optional[bool], query_params: Optional[dict], stream: Optional[bool]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 746
def _init_kwargs_for_pass_through_endpoint(request: Request, user_api_key_dict: UserAPIKeyAuth, passthrough_logging_payload: PassthroughStandardLoggingPayload, logging_obj: LiteLLMLoggingObj, _parsed_body: Optional[dict], litellm_call_id: Optional[str]) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 793
def _update_metadata_with_tags_in_header(request: Request, metadata: dict) -> dict:
    """If tags are in the request headers, add them to the metadata

    Used for google and vertex JS SDKs"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 805
def create_pass_through_route(endpoint, target: str, custom_headers: Optional[dict], _forward_headers: Optional[bool], _merge_query_params: Optional[bool], dependencies: Optional[List]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 866
def _is_streaming_response(response: ?) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 873
async def initialize_pass_through_endpoints(pass_through_endpoints: list):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 927
async def get_pass_through_endpoints(endpoint_id: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """GET configured pass through endpoint.

    If no endpoint_id given, return all configured endpoints."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 975
async def update_pass_through_endpoints(request: Request, endpoint_id: str):
    "Update a pass-through endpoint"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 986
async def create_pass_through_endpoints(data: PassThroughGenericEndpoint, user_api_key_dict: UserAPIKeyAuth=...):
    "Create new pass-through endpoint"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/pass_through_endpoints.py Line: 1032
async def delete_pass_through_endpoints(endpoint_id: str, user_api_key_dict: UserAPIKeyAuth=...):
    """Delete a pass-through endpoint

    Returns - the deleted endpoint"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 37
def create_request_copy(request: Request):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 47
async def llm_passthrough_factory_proxy_route(custom_llm_provider: str, endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    "Factory function for creating pass-through endpoints for LLM providers."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 130
async def gemini_proxy_route(endpoint: str, request: Request, fastapi_response: Response):
    "[Docs](https://docs.litellm.ai/docs/pass_through/google_ai_studio)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 197
async def cohere_proxy_route(endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    "[Docs](https://docs.litellm.ai/docs/pass_through/cohere)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 249
async def vllm_proxy_route(endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    "[Docs](https://docs.litellm.ai/docs/pass_through/vllm)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 272
async def mistral_proxy_route(endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    "[Docs](https://docs.litellm.ai/docs/anthropic_completion)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 327
async def anthropic_proxy_route(endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    "[Docs](https://docs.litellm.ai/docs/anthropic_completion)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 383
async def bedrock_proxy_route(endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    "[Docs](https://docs.litellm.ai/docs/pass_through/bedrock)"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 458
def _is_bedrock_agent_runtime_route(endpoint: str) -> bool:
    "Return True, if the endpoint should be routed to the `bedrock-agent-runtime` endpoint."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 478
async def assemblyai_proxy_route(endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 544
async def azure_proxy_route(endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Call any azure endpoint using the proxy.

    Just use `{PROXY_BASE_URL}/azure/{endpoint:path}`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 584
class BaseVertexAIPassThroughHandler(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 598
class VertexAIDiscoveryPassThroughHandler(BaseVertexAIPassThroughHandler):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 610
class VertexAIPassThroughHandler(BaseVertexAIPassThroughHandler):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 622
def get_vertex_pass_through_handler(call_type: Literal[(?, ?)]) -> BaseVertexAIPassThroughHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 633
async def _base_vertex_proxy_route(endpoint: str, request: Request, fastapi_response: Response, get_vertex_pass_through_handler: BaseVertexAIPassThroughHandler, user_api_key_dict: Optional[UserAPIKeyAuth]):
    """Base function for Vertex AI passthrough routes.
    Handles common logic for all Vertex AI services.

    Default base_target_url is `https://{vertex_location}-aiplatform.googleapis.com/`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 774
async def vertex_discovery_proxy_route(endpoint: str, request: Request, fastapi_response: Response):
    """Call any vertex discovery endpoint using the proxy.

    Just use `{PROXY_BASE_URL}/vertex_ai/discovery/{endpoint:path}`

    Target url: `https://discoveryengine.googleapis.com`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 807
async def vertex_proxy_route(endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    """Call LiteLLM proxy via Vertex AI SDK.

    [Docs](https://docs.litellm.ai/docs/pass_through/vertex_ai)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 834
async def openai_proxy_route(endpoint: str, request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    "Simple pass-through for OpenAI. Use this if you want to directly send a request to OpenAI."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_passthrough_endpoints.py Line: 867
class BaseOpenAIPassThroughHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/streaming_handler.py Line: 23
class PassThroughStreamingHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/common_utils.py Line: 4
def get_litellm_virtual_key(request: Request) -> str:
    """Extract and format API key from request headers.
    Prioritizes x-litellm-api-key over Authorization header.


    Vertex JS SDK uses `Authorization` header, we use `x-litellm-api-key` to pass litellm virtual key"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/db_spend_update_writer.py Line: 47
class DBSpendUpdateWriter:
    """Module responsible for

    1. Writing spend increments to either in memory list of transactions or to redis
    2. Reading increments from redis or in memory list of transactions and committing them to db"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/log_db_metrics.py Line: 19
def log_db_metrics(func):
    """Decorator to log the duration of a DB related function to ServiceLogger()

    Handles logging DB success/failure to ServiceLogger(), which logs to Prometheus, OTEL, Datadog

    When logging Failure it checks if the Exception is a PrismaError, httpx.ConnectError or httpx.TimeoutException and then logs that as a DB Service Failure

    Args:
        func: The function to be decorated

    Returns:
        Result from the decorated function

    Raises:
        Exception: If the decorated function raises an exception"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/log_db_metrics.py Line: 104
def _is_exception_related_to_db(e: Exception) -> bool:
    "Returns True if the exception is related to the DB"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/log_db_metrics.py Line: 115
async def _handle_logging_db_exception(e: Exception, func: Callable, kwargs: Dict, args: Tuple, start_time: datetime, end_time: datetime) -> ?:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/check_migration.py Line: 10
def extract_sql_commands(diff_output: str) -> List[str]:
    """Extract SQL commands from the Prisma migrate diff output.
    Args:
        diff_output (str): The full output from prisma migrate diff.
    Returns:
        List[str]: A list of SQL commands extracted from the diff output."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/check_migration.py Line: 47
def check_prisma_schema_diff_helper(db_url: str) -> Tuple[(bool, List[str])]:
    """Checks for differences between current database and Prisma schema.
    Returns:
        A tuple containing:
        - A boolean indicating if differences were found (True) or not (False).
        - A string with the diff output or error message.
    Raises:
        subprocess.CalledProcessError: If the Prisma command fails.
        Exception: For any other errors during execution."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/check_migration.py Line: 92
def check_prisma_schema_diff(db_url: Optional[str]) -> ?:
    "Main function to run the Prisma schema diff check."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/base_client.py Line: 4
class CustomDB:
    "Implements a base class that we expect any custom db implementation (e.g. DynamoDB) to follow"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/dynamo_db.py Line: 10
class DynamoDBWrapper(CustomDB):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/create_views.py Line: 8
async def create_missing_views(db: _db):
    """--------------------------------------------------
    NOTE: Copy of `litellm/db_scripts/create_views.py`.
    --------------------------------------------------
    Checks if the LiteLLM_VerificationTokenView and MonthlyGlobalSpend exists in the user's db.

    LiteLLM_VerificationTokenView: This view is used for getting the token + team data in user_api_key_auth

    MonthlyGlobalSpend: This view is used for the admin view to see global spend for this month

    If the view doesn't exist, one will be created."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/create_views.py Line: 199
async def should_create_missing_views(db: _db) -> bool:
    """Run only on first time startup.

    If SpendLogs table already has values, then don't create views on startup."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/prisma_client.py Line: 19
class PrismaWrapper:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/prisma_client.py Line: 119
class PrismaManager:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/prisma_client.py Line: 185
def should_update_prisma_schema(disable_updates: Optional[Union[(bool, str)]]) -> bool:
    """Determines if Prisma Schema updates should be applied during startup.

    Args:
        disable_updates: Controls whether schema updates are disabled.
            Accepts boolean or string ('true'/'false'). Defaults to checking DISABLE_SCHEMA_UPDATE env var.

    Returns:
        bool: True if schema updates should be applied, False if updates are disabled.

    Examples:
        >>> should_update_prisma_schema()  # Checks DISABLE_SCHEMA_UPDATE env var
        >>> should_update_prisma_schema(True)  # Explicitly disable updates
        >>> should_update_prisma_schema("false")  # Enable updates using string"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/exception_handler.py Line: 11
class PrismaDBExceptionHandler:
    "Class to handle DB Exceptions or Connection Errors"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 73
async def google_login(request: Request):
    """Create Proxy API Keys using Google Workspace SSO. Requires setting PROXY_BASE_URL in .env
    PROXY_BASE_URL should be the your deployed proxy endpoint, e.g. PROXY_BASE_URL="https://litellm-production-7002.up.railway.app/"
    Example:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 145
def generic_response_convertor(response, jwt_handler: JWTHandler):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 182
async def get_generic_sso_response(request: Request, jwt_handler: JWTHandler, generic_client_id: str, redirect_url: str) -> Union[(OpenID, dict)]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 267
async def create_team_member_add_task(team_id, user_info):
    "Create a task for adding a member to a team."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 286
async def add_missing_team_member(user_info: Union[(NewUserResponse, LiteLLM_UserTable)], sso_teams: List[str]):
    """- Get missing teams (diff b/w user_info.team_ids and sso_teams)
    - Add missing user to missing teams"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 311
def get_disabled_non_admin_personal_key_creation():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 323
async def auth_callback(request: Request):
    "Verify login"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 562
async def insert_sso_user(result_openid: Optional[Union[(OpenID, dict)]], user_defined_values: Optional[SSOUserDefinedValues]) -> NewUserResponse:
    """Helper function to create a New User in LiteLLM DB after a successful SSO login

    Args:
        result_openid (OpenID): User information in OpenID format if the login was successful.
        user_defined_values (Optional[SSOUserDefinedValues], optional): LiteLLM SSOValues / fields that were read

    Returns:
        Tuple[str, str]: User ID and User Role"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 624
async def get_ui_settings(request: Request):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 651
class SSOAuthenticationHandler:
    "Handler for SSO Authentication across all SSO providers"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 969
class MicrosoftSSOHandler:
    "Handles Microsoft SSO callback response and returns a CustomOpenID object"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 1269
class GoogleSSOHandler:
    "Handles Google SSO callback response and returns a CustomOpenID object"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 1318
async def debug_sso_login(request: Request):
    """Create Proxy API Keys using Google Workspace SSO. Requires setting PROXY_BASE_URL in .env
    PROXY_BASE_URL should be the your deployed proxy endpoint, e.g. PROXY_BASE_URL="https://litellm-production-7002.up.railway.app/"
    Example:"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/ui_sso.py Line: 1368
async def debug_sso_callback(request: Request):
    "Returns the OpenID object returned by the SSO provider"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 39
async def _get_model_names(prisma_client, model_ids: list) -> Dict[(str, str)]:
    "Helper function to get model names from model IDs"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 51
async def _get_tags_config(prisma_client) -> Dict[(str, TagConfig)]:
    "Helper function to get tags config from db"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 76
async def _save_tags_config(prisma_client, tags_config: Dict[(str, TagConfig)]):
    "Helper function to save tags config to db"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 119
async def new_tag(tag: TagNewRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Create a new tag.

    Parameters:
    - name: str - The name of the tag
    - description: Optional[str] - Description of what this tag represents
    - models: List[str] - List of LLM models allowed for this tag"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 182
async def _add_tag_to_deployment(model_id: str, tag: str):
    "Helper function to add tag to deployment"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 210
async def update_tag(tag: TagUpdateRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Update an existing tag.

    Parameters:
    - name: str - The name of the tag to update
    - description: Optional[str] - Updated description
    - models: List[str] - Updated list of allowed LLM models"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 268
async def info_tag(data: TagInfoRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Get information about specific tags.

    Parameters:
    - names: List[str] - List of tag names to get information for"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 307
async def list_tags(user_api_key_dict: UserAPIKeyAuth=...):
    "List all available tags."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 355
async def delete_tag(data: TagDeleteRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Delete a tag.

    Parameters:
    - name: str - The name of the tag to delete"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/tag_management_endpoints.py Line: 395
async def get_tag_daily_activity(tags: Optional[str], start_date: Optional[str], end_date: Optional[str], model: Optional[str], api_key: Optional[str], page: int=1, page_size: int=10):
    """Get daily activity for specific tags or all tags.

    Args:
        tags (Optional[str]): Comma-separated list of tags to filter by. If not provided, returns data for all tags.
        start_date (Optional[str]): Start date for the activity period (YYYY-MM-DD).
        end_date (Optional[str]): End date for the activity period (YYYY-MM-DD).
        model (Optional[str]): Filter by model name.
        api_key (Optional[str]): Filter by API key.
        page (int): Page number for pagination.
        page_size (int): Number of items per page.

    Returns:
        SpendAnalyticsPaginatedResponse: Paginated response containing daily activity data."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/sso_helper_utils.py Line: 4
def check_is_admin_only_access(ui_access_mode: str) -> bool:
    "Checks ui access mode is admin_only"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/sso_helper_utils.py Line: 9
def has_admin_ui_access(user_role: str) -> bool:
    """Check if the user has admin access to the UI.

    Returns:
        bool: True if user is 'proxy_admin' or 'proxy_admin_view_only', False otherwise."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_callback_endpoints.py Line: 33
async def add_team_callbacks(data: AddTeamCallback, http_request: Request, team_id: str, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...):
    """Add a success/failure callback to a team

    Use this if if you want different teams to have different success/failure callbacks

    Parameters:
    - callback_name (Literal["langfuse", "langsmith", "gcs"], required): The name of the callback to add
    - callback_type (Literal["success", "failure", "success_and_failure"], required): The type of callback to add. One of:
        - "success": Callback for successful LLM calls
        - "failure": Callback for failed LLM calls
        - "success_and_failure": Callback for both successful and failed LLM calls
    - callback_vars (StandardCallbackDynamicParams, required): A dictionary of variables to pass to the callback
        - langfuse_public_key: The public key for the Langfuse callback
        - langfuse_secret_key: The secret key for the Langfuse callback
        - langfuse_secret: The secret for the Langfuse callback
        - langfuse_host: The host for the Langfuse callback
        - gcs_bucket_name: The name of the GCS bucket
        - gcs_path_service_account: The path to the GCS service account
        - langsmith_api_key: The API key for the Langsmith callback
        - langsmith_project: The project for the Langsmith callback
        - langsmith_base_url: The base URL for the Langsmith callback

    Example curl:
    ```
    curl -X POST 'http:/localhost:4000/team/dbe2f686-a686-4896-864a-4c3924458709/callback'         -H 'Content-Type: application/json'         -H 'Authorization: Bearer sk-1234'         -d '{
        "callback_name": "langfuse",
        "callback_type": "success",
        "callback_vars": {"langfuse_public_key": "pk-lf-xxxx1", "langfuse_secret_key": "sk-xxxxx"}
    
    }'
    ```

    This means for the team where team_id = dbe2f686-a686-4896-864a-4c3924458709, all LLM calls will be logged to langfuse using the public key pk-lf-xxxx1 and the secret key sk-xxxxx"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_callback_endpoints.py Line: 201
async def disable_team_logging(http_request: Request, team_id: str, user_api_key_dict: UserAPIKeyAuth=...):
    """Disable all logging callbacks for a team

    Parameters:
    - team_id (str, required): The unique identifier for the team

    Example curl:
    ```
    curl -X POST 'http://localhost:4000/team/dbe2f686-a686-4896-864a-4c3924458709/disable_logging'         -H 'Authorization: Bearer sk-1234'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_callback_endpoints.py Line: 300
async def get_team_callbacks(http_request: Request, team_id: str, user_api_key_dict: UserAPIKeyAuth=...):
    """Get the success/failure callbacks and variables for a team

    Parameters:
    - team_id (str, required): The unique identifier for the team

    Example curl:
    ```
    curl -X GET 'http://localhost:4000/team/dbe2f686-a686-4896-864a-4c3924458709/callback'         -H 'Authorization: Bearer sk-1234'
    ```

    This will return the callback settings for the team with id dbe2f686-a686-4896-864a-4c3924458709

    Returns {
            "status": "success",
            "data": {
                "team_id": team_id,
                "success_callbacks": team_callback_settings_obj.success_callback,
                "failure_callbacks": team_callback_settings_obj.failure_callback,
                "callback_vars": team_callback_settings_obj.callback_vars,
            },
        }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 41
async def new_organization(data: NewOrganizationRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Allow orgs to own teams

    Set org level budgets + model access.

    Only admins can create orgs.

    # Parameters

    - organization_alias: *str* - The name of the organization.
    - models: *List* - The models the organization has access to.
    - budget_id: *Optional[str]* - The id for a budget (tpm/rpm/max budget) for the organization.
    ### IF NO BUDGET ID - CREATE ONE WITH THESE PARAMS ###
    - max_budget: *Optional[float]* - Max budget for org
    - tpm_limit: *Optional[int]* - Max tpm limit for org
    - rpm_limit: *Optional[int]* - Max rpm limit for org
    - max_parallel_requests: *Optional[int]* - [Not Implemented Yet] Max parallel requests for org
    - soft_budget: *Optional[float]* - [Not Implemented Yet] Get a slack alert when this soft budget is reached. Don't block requests.
    - model_max_budget: *Optional[dict]* - Max budget for a specific model
    - budget_duration: *Optional[str]* - Frequency of reseting org budget
    - metadata: *Optional[dict]* - Metadata for organization, store information for organization. Example metadata - {"extra_info": "some info"}
    - blocked: *bool* - Flag indicating if the org is blocked or not - will stop all calls from keys with this org_id.
    - tags: *Optional[List[str]]* - Tags for [tracking spend](https://litellm.vercel.app/docs/proxy/enterprise#tracking-spend-for-custom-tags) and/or doing [tag-based routing](https://litellm.vercel.app/docs/proxy/tag_routing).
    - organization_id: *Optional[str]* - The organization id of the team. Default is None. Create via `/organization/new`.
    - model_aliases: Optional[dict] - Model aliases for the team. [Docs](https://docs.litellm.ai/docs/proxy/team_based_routing#create-team-with-model-alias)

    Case 1: Create new org **without** a budget_id

    ```bash
    curl --location 'http://0.0.0.0:4000/organization/new' 
    --header 'Authorization: Bearer sk-1234' 
    --header 'Content-Type: application/json' 
    --data '{
        "organization_alias": "my-secret-org",
        "models": ["model1", "model2"],
        "max_budget": 100
    }'


    ```

    Case 2: Create new org **with** a budget_id

    ```bash
    curl --location 'http://0.0.0.0:4000/organization/new' 
    --header 'Authorization: Bearer sk-1234' 
    --header 'Content-Type: application/json' 
    --data '{
        "organization_alias": "my-secret-org",
        "models": ["model1", "model2"],
        "budget_id": "428eeaa8-f3ac-4e85-a8fb-7dc8d7aa8689"
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 195
async def update_organization(data: LiteLLM_OrganizationTableUpdate, user_api_key_dict: UserAPIKeyAuth=...):
    "Update an organization"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 240
async def delete_organization(data: DeleteOrganizationRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Delete an organization

    # Parameters:

    - organization_ids: List[str] - The organization ids to delete."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 300
async def list_organization(user_api_key_dict: UserAPIKeyAuth=...):
    """```
    curl --location --request GET 'http://0.0.0.0:4000/organization/list'         --header 'Authorization: Bearer sk-1234'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 352
async def info_organization(organization_id: str):
    "Get the org specific information"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 383
async def deprecated_info_organization(data: OrganizationRequest):
    "DEPRECATED: Use GET /organization/info instead"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 414
async def organization_member_add(data: OrganizationMemberAddRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=...) -> OrganizationAddMemberResponse:
    """[BETA]

    Add new members (either via user_email or user_id) to an organization

    If user doesn't exist, new user row will also be added to User Table

    Only proxy_admin or org_admin of organization, allowed to access this endpoint.

    # Parameters:

    - organization_id: str (required)
    - member: Union[List[Member], Member] (required)
        - role: Literal[LitellmUserRoles] (required)
        - user_id: Optional[str]
        - user_email: Optional[str]

    Note: Either user_id or user_email must be provided for each member.

    Example:
    ```
    curl -X POST 'http://0.0.0.0:4000/organization/member_add'     -H 'Authorization: Bearer sk-1234'     -H 'Content-Type: application/json'     -d '{
        "organization_id": "45e3e396-ee08-4a61-a88e-16b3ce7e0849",
        "member": {
            "role": "internal_user",
            "user_id": "krrish247652@berri.ai"
        },
        "max_budget_in_organization": 100.0
    }'
    ```

    The following is executed in this function:

    1. Check if organization exists
    2. Creates a new Internal User if the user_id or user_email is not found in LiteLLM_UserTable
    3. Add Internal User to the `LiteLLM_OrganizationMembership` table"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 525
async def find_member_if_email(user_email: str, prisma_client: PrismaClient) -> LiteLLM_UserTable:
    "Find a member if the user_email is in LiteLLM_UserTable"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 558
async def organization_member_update(data: OrganizationMemberUpdateRequest, user_api_key_dict: UserAPIKeyAuth=...):
    "Update a member's role in an organization"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 695
async def organization_member_delete(data: OrganizationMemberDeleteRequest, user_api_key_dict: UserAPIKeyAuth=...):
    "Delete a member from an organization"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/organization_endpoints.py Line: 732
async def add_member_to_organization(member: OrgMember, organization_id: str, prisma_client: PrismaClient) -> Tuple[(LiteLLM_UserTable, LiteLLM_OrganizationMembershipTable)]:
    """Add a member to an organization

    - Checks if member.user_id or member.user_email is in LiteLLM_UserTable
    - If not found, create a new user in LiteLLM_UserTable
    - Add user to organization in LiteLLM_OrganizationMembership"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/types.py Line: 12
class CustomOpenID(OpenID):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/customer_endpoints.py Line: 38
async def block_user(data: BlockUsers):
    """[BETA] Reject calls with this end-user id

    Parameters:
    - user_ids (List[str], required): The unique `user_id`s for the users to block

        (any /chat/completion call with this user={end-user-id} param, will be rejected.)

        ```
        curl -X POST "http://0.0.0.0:8000/user/block"
        -H "Authorization: Bearer sk-1234"
        -d '{
        "user_ids": [<user_id>, ...]
        }'
        ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/customer_endpoints.py Line: 92
async def unblock_user(data: BlockUsers):
    """[BETA] Unblock calls with this user id

    Example
    ```
    curl -X POST "http://0.0.0.0:8000/user/unblock"
    -H "Authorization: Bearer sk-1234"
    -d '{
    "user_ids": [<user_id>, ...]
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/customer_endpoints.py Line: 134
def new_budget_request(data: NewCustomerRequest) -> Optional[BudgetNewRequest]:
    "Return a new budget object if new budget params are passed."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/customer_endpoints.py Line: 165
async def new_end_user(data: NewCustomerRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Allow creating a new Customer 


    Parameters:
    - user_id: str - The unique identifier for the user.
    - alias: Optional[str] - A human-friendly alias for the user.
    - blocked: bool - Flag to allow or disallow requests for this end-user. Default is False.
    - max_budget: Optional[float] - The maximum budget allocated to the user. Either 'max_budget' or 'budget_id' should be provided, not both.
    - budget_id: Optional[str] - The identifier for an existing budget allocated to the user. Either 'max_budget' or 'budget_id' should be provided, not both.
    - allowed_model_region: Optional[Union[Literal["eu"], Literal["us"]]] - Require all user requests to use models in this specific region.
    - default_model: Optional[str] - If no equivalent model in the allowed region, default all requests to this model.
    - metadata: Optional[dict] = Metadata for customer, store information for customer. Example metadata = {"data_training_opt_out": True}
    - budget_duration: Optional[str] - Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").
    - tpm_limit: Optional[int] - [Not Implemented Yet] Specify tpm limit for a given customer (Tokens per minute)
    - rpm_limit: Optional[int] - [Not Implemented Yet] Specify rpm limit for a given customer (Requests per minute)
    - model_max_budget: Optional[dict] - [Not Implemented Yet] Specify max budget for a given model. Example: {"openai/gpt-4o-mini": {"max_budget": 100.0, "budget_duration": "1d"}}
    - max_parallel_requests: Optional[int] - [Not Implemented Yet] Specify max parallel requests for a given customer.
    - soft_budget: Optional[float] - [Not Implemented Yet] Get alerts when customer crosses given budget, doesn't block requests.


    - Allow specifying allowed regions 
    - Allow specifying default model

    Example curl:
    ```
    curl --location 'http://0.0.0.0:4000/customer/new'         --header 'Authorization: Bearer sk-1234'         --header 'Content-Type: application/json'         --data '{
            "user_id" : "ishaan-jaff-3",
            "allowed_region": "eu",
            "budget_id": "free_tier",
            "default_model": "azure/gpt-3.5-turbo-eu" <- all calls from this user, use this model? 
        }'

        # return end-user object
    ```

    NOTE: This used to be called `/end_user/new`, we will still be maintaining compatibility for /end_user/XXX for these endpoints"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/customer_endpoints.py Line: 327
async def end_user_info(end_user_id: str=...):
    """Get information about an end-user. An `end_user` is a customer (external user) of the proxy.

    Parameters:
    - end_user_id (str, required): The unique identifier for the end-user

    Example curl:
    ```
    curl -X GET 'http://localhost:4000/customer/info?end_user_id=test-litellm-user-4'         -H 'Authorization: Bearer sk-1234'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/customer_endpoints.py Line: 375
async def update_end_user(data: UpdateCustomerRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Example curl 

    Parameters:
    - user_id: str
    - alias: Optional[str] = None  # human-friendly alias
    - blocked: bool = False  # allow/disallow requests for this end-user
    - max_budget: Optional[float] = None
    - budget_id: Optional[str] = None  # give either a budget_id or max_budget
    - allowed_model_region: Optional[AllowedModelRegion] = (
        None  # require all user requests to use models in this specific region
    )
    - default_model: Optional[str] = (
        None  # if no equivalent model in allowed region - default all requests to this model
    )

    Example curl:
    ```
    curl --location 'http://0.0.0.0:4000/customer/update'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "user_id": "test-litellm-user-4",
        "budget_id": "paid_tier"
    }'

    See below for all params 
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/customer_endpoints.py Line: 483
async def delete_end_user(data: DeleteCustomerRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Delete multiple end-users.

    Parameters:
    - user_ids (List[str], required): The unique `user_id`s for the users to delete

    Example curl:
    ```
    curl --location 'http://0.0.0.0:4000/customer/delete'         --header 'Authorization: Bearer sk-1234'         --header 'Content-Type: application/json'         --data '{
            "user_ids" :["ishaan-jaff-5"]
    }'

    See below for all params 
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/customer_endpoints.py Line: 577
async def list_end_user(http_request: Request, user_api_key_dict: UserAPIKeyAuth=...):
    """[Admin-only] List all available customers

    Example curl:
    ```
    curl --location --request GET 'http://0.0.0.0:4000/customer/list'         --header 'Authorization: Bearer sk-1234'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/common_utils.py Line: 13
def _user_has_admin_view(user_api_key_dict: UserAPIKeyAuth) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/common_utils.py Line: 20
def _is_user_team_admin(user_api_key_dict: UserAPIKeyAuth, team_obj: LiteLLM_TeamTable) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/common_utils.py Line: 32
def _set_object_metadata_field(object_data: Union[(LiteLLM_TeamTable, GenerateKeyRequest)], field_name: str, value: Any) -> ?:
    """Helper function to set metadata fields that require premium user checks

    Args:
        object_data: The team data object to modify
        field_name: Name of the metadata field to set
        value: Value to set for the field"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 66
def _is_team_key(data: Union[(GenerateKeyRequest, LiteLLM_VerificationToken)]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 70
def _get_user_in_team(team_table: LiteLLM_TeamTableCachedObj, user_id: Optional[str]) -> Optional[Member]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 82
def _is_allowed_to_make_key_request(user_api_key_dict: UserAPIKeyAuth, user_id: Optional[str], team_id: Optional[str]) -> bool:
    """Assert user only creates keys for themselves

    Relevant issue: https://github.com/BerriAI/litellm/issues/7336"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 114
def _team_key_operation_team_member_check(assigned_user_id: Optional[str], team_table: LiteLLM_TeamTableCachedObj, user_api_key_dict: UserAPIKeyAuth, team_key_generation: TeamUIKeyGenerationConfig, route: KeyManagementRoutes):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 166
def _key_generation_required_param_check(data: GenerateKeyRequest, required_params: Optional[List[str]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 182
def _team_key_generation_check(team_table: LiteLLM_TeamTableCachedObj, user_api_key_dict: UserAPIKeyAuth, data: GenerateKeyRequest, route: KeyManagementRoutes):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 215
def _personal_key_membership_check(user_api_key_dict: UserAPIKeyAuth, personal_key_generation: Optional[PersonalUIKeyGenerationConfig]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 234
def _personal_key_generation_check(user_api_key_dict: UserAPIKeyAuth, data: GenerateKeyRequest):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 258
def key_generation_check(team_table: Optional[LiteLLM_TeamTableCachedObj], user_api_key_dict: UserAPIKeyAuth, data: GenerateKeyRequest, route: KeyManagementRoutes) -> bool:
    "Check if admin has restricted key creation to certain roles for teams or individuals"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 290
def common_key_access_checks(user_api_key_dict: UserAPIKeyAuth, data: Union[(GenerateKeyRequest, UpdateKeyRequest)], llm_router: Optional[Router], premium_user: bool) -> Literal[?]:
    "Check if user is allowed to make a key request, for this key"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 334
async def generate_key_fn(data: GenerateKeyRequest, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...):
    """Generate an API key based on the provided data.

    Docs: https://docs.litellm.ai/docs/proxy/virtual_keys

    Parameters:
    - duration: Optional[str] - Specify the length of time the token is valid for. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").
    - key_alias: Optional[str] - User defined key alias
    - key: Optional[str] - User defined key value. If not set, a 16-digit unique sk-key is created for you.
    - team_id: Optional[str] - The team id of the key
    - user_id: Optional[str] - The user id of the key
    - budget_id: Optional[str] - The budget id associated with the key. Created by calling `/budget/new`.
    - models: Optional[list] - Model_name's a user is allowed to call. (if empty, key is allowed to call all models)
    - aliases: Optional[dict] - Any alias mappings, on top of anything in the config.yaml model list. - https://docs.litellm.ai/docs/proxy/virtual_keys#managing-auth---upgradedowngrade-models
    - config: Optional[dict] - any key-specific configs, overrides config in config.yaml
    - spend: Optional[int] - Amount spent by key. Default is 0. Will be updated by proxy whenever key is used. https://docs.litellm.ai/docs/proxy/virtual_keys#managing-auth---tracking-spend
    - send_invite_email: Optional[bool] - Whether to send an invite email to the user_id, with the generate key
    - max_budget: Optional[float] - Specify max budget for a given key.
    - budget_duration: Optional[str] - Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").
    - max_parallel_requests: Optional[int] - Rate limit a user based on the number of parallel requests. Raises 429 error, if user's parallel requests > x.
    - metadata: Optional[dict] - Metadata for key, store information for key. Example metadata = {"team": "core-infra", "app": "app2", "email": "ishaan@berri.ai" }
    - guardrails: Optional[List[str]] - List of active guardrails for the key
    - permissions: Optional[dict] - key-specific permissions. Currently just used for turning off pii masking (if connected). Example - {"pii": false}
    - model_max_budget: Optional[Dict[str, BudgetConfig]] - Model-specific budgets {"gpt-4": {"budget_limit": 0.0005, "time_period": "30d"}}}. IF null or {} then no model specific budget.
    - model_rpm_limit: Optional[dict] - key-specific model rpm limit. Example - {"text-davinci-002": 1000, "gpt-3.5-turbo": 1000}. IF null or {} then no model specific rpm limit.
    - model_tpm_limit: Optional[dict] - key-specific model tpm limit. Example - {"text-davinci-002": 1000, "gpt-3.5-turbo": 1000}. IF null or {} then no model specific tpm limit.
    - allowed_cache_controls: Optional[list] - List of allowed cache control values. Example - ["no-cache", "no-store"]. See all values - https://docs.litellm.ai/docs/proxy/caching#turn-on--off-caching-per-request
    - blocked: Optional[bool] - Whether the key is blocked.
    - rpm_limit: Optional[int] - Specify rpm limit for a given key (Requests per minute)
    - tpm_limit: Optional[int] - Specify tpm limit for a given key (Tokens per minute)
    - soft_budget: Optional[float] - Specify soft budget for a given key. Will trigger a slack alert when this soft budget is reached.
    - tags: Optional[List[str]] - Tags for [tracking spend](https://litellm.vercel.app/docs/proxy/enterprise#tracking-spend-for-custom-tags) and/or doing [tag-based routing](https://litellm.vercel.app/docs/proxy/tag_routing).
    - enforced_params: Optional[List[str]] - List of enforced params for the key (Enterprise only). [Docs](https://docs.litellm.ai/docs/proxy/enterprise#enforce-required-params-for-llm-requests)
    - allowed_routes: Optional[list] - List of allowed routes for the key. Store the actual route or store a wildcard pattern for a set of routes. Example - ["/chat/completions", "/embeddings", "/keys/*"]
    Examples:

    1. Allow users to turn on/off pii masking

    ```bash
    curl --location 'http://0.0.0.0:4000/key/generate'         --header 'Authorization: Bearer sk-1234'         --header 'Content-Type: application/json'         --data '{
            "permissions": {"allow_pii_controls": true}
    }'
    ```

    Returns:
    - key: (str) The generated api key
    - expires: (datetime) Datetime object for when key expires.
    - user_id: (str) Unique user id - used for tracking spend across multiple keys for same user id."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 609
def prepare_metadata_fields(data: BaseModel, non_default_values: dict, existing_metadata: dict) -> dict:
    "Check LiteLLM_ManagementEndpoint_MetadataFields (proxy/_types.py) for fields that are allowed to be updated"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 641
def prepare_key_update_data(data: Union[(UpdateKeyRequest, RegenerateKeyRequest)], existing_key_row):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 688
async def update_key_fn(request: Request, data: UpdateKeyRequest, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...):
    """Update an existing API key's parameters.

    Parameters:
    - key: str - The key to update
    - key_alias: Optional[str] - User-friendly key alias
    - user_id: Optional[str] - User ID associated with key
    - team_id: Optional[str] - Team ID associated with key
    - budget_id: Optional[str] - The budget id associated with the key. Created by calling `/budget/new`.
    - models: Optional[list] - Model_name's a user is allowed to call
    - tags: Optional[List[str]] - Tags for organizing keys (Enterprise only)
    - enforced_params: Optional[List[str]] - List of enforced params for the key (Enterprise only). [Docs](https://docs.litellm.ai/docs/proxy/enterprise#enforce-required-params-for-llm-requests)
    - spend: Optional[float] - Amount spent by key
    - max_budget: Optional[float] - Max budget for key
    - model_max_budget: Optional[Dict[str, BudgetConfig]] - Model-specific budgets {"gpt-4": {"budget_limit": 0.0005, "time_period": "30d"}}
    - budget_duration: Optional[str] - Budget reset period ("30d", "1h", etc.)
    - soft_budget: Optional[float] - [TODO] Soft budget limit (warning vs. hard stop). Will trigger a slack alert when this soft budget is reached.
    - max_parallel_requests: Optional[int] - Rate limit for parallel requests
    - metadata: Optional[dict] - Metadata for key. Example {"team": "core-infra", "app": "app2"}
    - tpm_limit: Optional[int] - Tokens per minute limit
    - rpm_limit: Optional[int] - Requests per minute limit
    - model_rpm_limit: Optional[dict] - Model-specific RPM limits {"gpt-4": 100, "claude-v1": 200}
    - model_tpm_limit: Optional[dict] - Model-specific TPM limits {"gpt-4": 100000, "claude-v1": 200000}
    - allowed_cache_controls: Optional[list] - List of allowed cache control values
    - duration: Optional[str] - Key validity duration ("30d", "1h", etc.)
    - permissions: Optional[dict] - Key-specific permissions
    - send_invite_email: Optional[bool] - Send invite email to user_id
    - guardrails: Optional[List[str]] - List of active guardrails for the key
    - blocked: Optional[bool] - Whether the key is blocked
    - aliases: Optional[dict] - Model aliases for the key - [Docs](https://litellm.vercel.app/docs/proxy/virtual_keys#model-aliases)
    - config: Optional[dict] - [DEPRECATED PARAM] Key-specific config.
    - temp_budget_increase: Optional[float] - Temporary budget increase for the key (Enterprise only).
    - temp_budget_expiry: Optional[str] - Expiry time for the temporary budget increase (Enterprise only).
    - allowed_routes: Optional[list] - List of allowed routes for the key. Store the actual route or store a wildcard pattern for a set of routes. Example - ["/chat/completions", "/embeddings", "/keys/*"]

    Example:
    ```bash
    curl --location 'http://0.0.0.0:4000/key/update'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "key": "sk-1234",
        "key_alias": "my-key",
        "user_id": "user-1234",
        "team_id": "team-1234",
        "max_budget": 100,
        "metadata": {"any_key": "any-val"},
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 852
async def delete_key_fn(data: KeyRequest, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...):
    """Delete a key from the key management system.

    Parameters::
    - keys (List[str]): A list of keys or hashed keys to delete. Example {"keys": ["sk-QWrxEynunsNpV1zT48HIrw", "837e17519f44683334df5291321d97b8bf1098cd490e49e215f6fea935aa28be"]}
    - key_aliases (List[str]): A list of key aliases to delete. Can be passed instead of `keys`.Example {"key_aliases": ["alias1", "alias2"]}

    Returns:
    - deleted_keys (List[str]): A list of deleted keys. Example {"deleted_keys": ["sk-QWrxEynunsNpV1zT48HIrw", "837e17519f44683334df5291321d97b8bf1098cd490e49e215f6fea935aa28be"]}

    Example:
    ```bash
    curl --location 'http://0.0.0.0:4000/key/delete'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "keys": ["sk-QWrxEynunsNpV1zT48HIrw"]
    }'
    ```

    Raises:
        HTTPException: If an error occurs during key deletion."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 965
async def info_key_fn_v2(data: Optional[KeyRequest], user_api_key_dict: UserAPIKeyAuth=...):
    """Retrieve information about a list of keys.

    **New endpoint**. Currently admin only.
    Parameters:
        keys: Optional[list] = body parameter representing the key(s) in the request
        user_api_key_dict: UserAPIKeyAuth = Dependency representing the user's API key
    Returns:
        Dict containing the key and its associated information

    Example Curl:
    ```
    curl -X GET "http://0.0.0.0:4000/key/info"     -H "Authorization: Bearer sk-1234"     -d {"keys": ["sk-1", "sk-2", "sk-3"]}
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1024
async def info_key_fn(key: Optional[str]=..., user_api_key_dict: UserAPIKeyAuth=...):
    """Retrieve information about a key.
    Parameters:
        key: Optional[str] = Query parameter representing the key in the request
        user_api_key_dict: UserAPIKeyAuth = Dependency representing the user's API key
    Returns:
        Dict containing the key and its associated information

    Example Curl:
    ```
    curl -X GET "http://0.0.0.0:4000/key/info?key=sk-02Wr4IAlN3NvPXvL5JVvDA" -H "Authorization: Bearer sk-1234"
    ```

    Example Curl - if no key is passed, it will use the Key Passed in Authorization Header
    ```
    curl -X GET "http://0.0.0.0:4000/key/info" -H "Authorization: Bearer sk-02Wr4IAlN3NvPXvL5JVvDA"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1101
def _check_model_access_group(models: Optional[List[str]], llm_router: Optional[Router], premium_user: bool) -> Literal[?]:
    """if is_model_access_group is True + is_wildcard_route is True, check if user is a premium user

    Return True if user is a premium user, False otherwise"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1129
async def generate_key_helper_fn(request_type: Literal[(?, ?)], duration: Optional[str], models: list, aliases: dict, config: dict, spend: float, key_max_budget: Optional[float], key_budget_duration: Optional[str], budget_id: Optional[float], soft_budget: Optional[float], max_budget: Optional[float], blocked: Optional[bool], budget_duration: Optional[str], token: Optional[str], key: Optional[str], user_id: Optional[str], user_alias: Optional[str], team_id: Optional[str], user_email: Optional[str], user_role: Optional[str], max_parallel_requests: Optional[int], metadata: Optional[dict], tpm_limit: Optional[int], rpm_limit: Optional[int], query_type: Literal[(?, ?)]="insert_data", update_key_values: Optional[dict], key_alias: Optional[str], allowed_cache_controls: Optional[list], permissions: Optional[dict], model_max_budget: Optional[dict], model_rpm_limit: Optional[dict], model_tpm_limit: Optional[dict], guardrails: Optional[list], teams: Optional[list], organization_id: Optional[str], table_name: Optional[Literal[(?, ?)]], send_invite_email: Optional[bool], created_by: Optional[str], updated_by: Optional[str], allowed_routes: Optional[list]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1380
async def _team_key_deletion_check(user_api_key_dict: UserAPIKeyAuth, key_info: LiteLLM_VerificationToken, prisma_client: PrismaClient, user_api_key_cache: DualCache):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1425
async def can_delete_verification_token(key_info: LiteLLM_VerificationToken, user_api_key_cache: DualCache, user_api_key_dict: UserAPIKeyAuth, prisma_client: PrismaClient) -> bool:
    """- check if user is proxy admin
    - check if user is team admin and key is a team key
    - check if key is personal key"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1452
async def delete_verification_tokens(tokens: List, user_api_key_cache: DualCache, user_api_key_dict: UserAPIKeyAuth) -> Tuple[(Optional[Dict], List[LiteLLM_VerificationToken])]:
    """Helper that deletes the list of tokens from the database

    - check if user is proxy admin
    - check if user is team admin and key is a team key

    Args:
        tokens: List of tokens to delete
        user_id: Optional user_id to filter by

    Returns:
        Tuple[Optional[Dict], List[LiteLLM_VerificationToken]]:
            Optional[Dict]:
                - Number of deleted tokens
            List[LiteLLM_VerificationToken]:
                - List of keys being deleted, this contains information about the key_alias, token, and user_id being deleted,
                this is passed down to the KeyManagementEventHooks to delete the keys from the secret manager and handle audit logs"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1545
async def delete_key_aliases(key_aliases: List[str], user_api_key_cache: DualCache, prisma_client: PrismaClient, user_api_key_dict: UserAPIKeyAuth) -> Tuple[(Optional[Dict], List[LiteLLM_VerificationToken])]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1563
async def _rotate_master_key(prisma_client: PrismaClient, user_api_key_dict: UserAPIKeyAuth, current_master_key: str, new_master_key: str) -> ?:
    """Rotate the master key

    1. Get the values from the DB
        - Get models from DB
        - Get config from DB
    2. Decrypt the values
        - ModelTable
            - [{"model_name": "str", "litellm_params": {}}]
        - ConfigTable
    3. Encrypt the values with the new master key
    4. Update the values in the DB"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1653
async def regenerate_key_fn(key: Optional[str], data: Optional[RegenerateKeyRequest], user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...) -> Optional[GenerateKeyResponse]:
    """Regenerate an existing API key while optionally updating its parameters.

    Parameters:
    - key: str (path parameter) - The key to regenerate
    - data: Optional[RegenerateKeyRequest] - Request body containing optional parameters to update
        - key_alias: Optional[str] - User-friendly key alias
        - user_id: Optional[str] - User ID associated with key
        - team_id: Optional[str] - Team ID associated with key
        - models: Optional[list] - Model_name's a user is allowed to call
        - tags: Optional[List[str]] - Tags for organizing keys (Enterprise only)
        - spend: Optional[float] - Amount spent by key
        - max_budget: Optional[float] - Max budget for key
        - model_max_budget: Optional[Dict[str, BudgetConfig]] - Model-specific budgets {"gpt-4": {"budget_limit": 0.0005, "time_period": "30d"}}
        - budget_duration: Optional[str] - Budget reset period ("30d", "1h", etc.)
        - soft_budget: Optional[float] - Soft budget limit (warning vs. hard stop). Will trigger a slack alert when this soft budget is reached.
        - max_parallel_requests: Optional[int] - Rate limit for parallel requests
        - metadata: Optional[dict] - Metadata for key. Example {"team": "core-infra", "app": "app2"}
        - tpm_limit: Optional[int] - Tokens per minute limit
        - rpm_limit: Optional[int] - Requests per minute limit
        - model_rpm_limit: Optional[dict] - Model-specific RPM limits {"gpt-4": 100, "claude-v1": 200}
        - model_tpm_limit: Optional[dict] - Model-specific TPM limits {"gpt-4": 100000, "claude-v1": 200000}
        - allowed_cache_controls: Optional[list] - List of allowed cache control values
        - duration: Optional[str] - Key validity duration ("30d", "1h", etc.)
        - permissions: Optional[dict] - Key-specific permissions
        - guardrails: Optional[List[str]] - List of active guardrails for the key
        - blocked: Optional[bool] - Whether the key is blocked


    Returns:
    - GenerateKeyResponse containing the new key and its updated parameters

    Example:
    ```bash
    curl --location --request POST 'http://localhost:4000/key/sk-1234/regenerate'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data-raw '{
        "max_budget": 100,
        "metadata": {"team": "core-infra"},
        "models": ["gpt-4", "gpt-3.5-turbo"]
    }'
    ```

    Note: This is an Enterprise feature. It requires a premium license to use."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1856
async def validate_key_list_check(user_api_key_dict: UserAPIKeyAuth, user_id: Optional[str], team_id: Optional[str], organization_id: Optional[str], key_alias: Optional[str], prisma_client: PrismaClient) -> Optional[LiteLLM_UserTable]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1928
async def get_admin_team_ids(complete_user_info: Optional[LiteLLM_UserTable], user_api_key_dict: UserAPIKeyAuth, prisma_client: PrismaClient) -> List[str]:
    "Get all team IDs where the user is an admin."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 1963
async def list_keys(request: Request, user_api_key_dict: UserAPIKeyAuth=..., page: int=..., size: int=..., user_id: Optional[str]=..., team_id: Optional[str]=..., organization_id: Optional[str]=..., key_alias: Optional[str]=..., return_full_object: bool=..., include_team_keys: bool=...) -> KeyListResponseObject:
    """List all keys for a given user / team / organization.

    Returns:
        {
            "keys": List[str] or List[UserAPIKeyAuth],
            "total_count": int,
            "current_page": int,
            "total_pages": int,
        }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 2058
async def _list_key_helper(prisma_client: PrismaClient, page: int, size: int, user_id: Optional[str], team_id: Optional[str], organization_id: Optional[str], key_alias: Optional[str], exclude_team_id: Optional[str], return_full_object: bool, admin_team_ids: Optional[List[str]]) -> KeyListResponseObject:
    """Helper function to list keys
    Args:
        page: int
        size: int
        user_id: Optional[str]
        team_id: Optional[str]
        key_alias: Optional[str]
        exclude_team_id: Optional[str] # exclude a specific team_id
        return_full_object: bool # when true, will return UserAPIKeyAuth objects instead of just the token
        admin_team_ids: Optional[List[str]] # list of team IDs where the user is an admin

    Returns:
        KeyListResponseObject
        {
            "keys": List[str] or List[UserAPIKeyAuth],  # Updated to reflect possible return types
            "total_count": int,
            "current_page": int,
            "total_pages": int,
        }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 2173
def _get_condition_to_filter_out_ui_session_tokens() -> Dict[(str, Any)]:
    "Condition to filter out UI session tokens"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 2191
async def block_key(data: BlockKeyRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...) -> Optional[LiteLLM_VerificationToken]:
    """Block an Virtual key from making any requests.

    Parameters:
    - key: str - The key to block. Can be either the unhashed key (sk-...) or the hashed key value

     Example:
    ```bash
    curl --location 'http://0.0.0.0:4000/key/block'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "key": "sk-Fn8Ej39NxjAXrvpUGKghGw"
    }'
    ```

    Note: This is an admin-only endpoint. Only proxy admins can block keys."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 2298
async def unblock_key(data: BlockKeyRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...):
    """Unblock a Virtual key to allow it to make requests again.

    Parameters:
    - key: str - The key to unblock. Can be either the unhashed key (sk-...) or the hashed key value

    Example:
    ```bash
    curl --location 'http://0.0.0.0:4000/key/unblock'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "key": "sk-Fn8Ej39NxjAXrvpUGKghGw"
    }'
    ```

    Note: This is an admin-only endpoint. Only proxy admins can unblock keys."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 2408
async def key_health(request: Request, user_api_key_dict: UserAPIKeyAuth=...):
    """Check the health of the key

    Checks:
    - If key based logging is configured correctly - sends a test log

    Usage 

    Pass the key in the request header

    ```bash
    curl -X POST "http://localhost:4000/key/health"      -H "Authorization: Bearer sk-1234"      -H "Content-Type: application/json"
    ```

    Response when logging callbacks are setup correctly:

    ```json
    {
      "key": "healthy",
      "logging_callbacks": {
        "callbacks": [
          "gcs_bucket"
        ],
        "status": "healthy",
        "details": "No logger exceptions triggered, system is healthy. Manually check if logs were sent to ['gcs_bucket']"
      }
    }
    ```


    Response when logging callbacks are not setup correctly:
    ```json
    {
      "key": "unhealthy",
      "logging_callbacks": {
        "callbacks": [
          "gcs_bucket"
        ],
        "status": "unhealthy",
        "details": "Logger exceptions triggered, system is unhealthy: Failed to load vertex credentials. Check to see if credentials containing partial/invalid information."
      }
    }
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 2491
async def _can_user_query_key_info(user_api_key_dict: UserAPIKeyAuth, key: Optional[str], key_info: LiteLLM_VerificationToken) -> bool:
    "Helper to check if the user has access to the key's info"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 2517
async def test_key_logging(user_api_key_dict: UserAPIKeyAuth, request: Request, key_logging: List[Dict[(str, Any)]]) -> LoggingCallbackStatus:
    """Test the key-based logging

    - Test that key logging is correctly formatted and all args are passed correctly
    - Make a mock completion call -> user can check if it's correctly logged
    - Check if any logger.exceptions were triggered -> if they were then returns it to the user client side"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 2597
async def _enforce_unique_key_alias(key_alias: Optional[str], prisma_client: Any, existing_key_token: Optional[str]) -> ?:
    """Helper to enforce unique key aliases across all keys.

    Args:
        key_alias (Optional[str]): The key alias to check
        prisma_client (Any): Prisma client instance
        existing_key_token (Optional[str]): ID of existing key being updated, to exclude from uniqueness check
            (The Admin UI passes key_alias, in all Edit key requests. So we need to be sure that if we find a key with the same alias, it's not the same key we're updating)

    Raises:
        ProxyException: If key alias already exists on a different key"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py Line: 2632
def validate_model_max_budget(model_max_budget: Optional[Dict]) -> ?:
    """Validate the model_max_budget is GenericBudgetConfigType + enforce user has an enterprise license

    Raises:
        Exception: If model_max_budget is not a valid GenericBudgetConfigType"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/budget_management_endpoints.py Line: 29
async def new_budget(budget_obj: BudgetNewRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Create a new budget object. Can apply this to teams, orgs, end-users, keys.

    Parameters:
    - budget_duration: Optional[str] - Budget reset period ("30d", "1h", etc.)
    - budget_id: Optional[str] - The id of the budget. If not provided, a new id will be generated.
    - max_budget: Optional[float] - The max budget for the budget.
    - soft_budget: Optional[float] - The soft budget for the budget.
    - max_parallel_requests: Optional[int] - The max number of parallel requests for the budget.
    - tpm_limit: Optional[int] - The tokens per minute limit for the budget.
    - rpm_limit: Optional[int] - The requests per minute limit for the budget.
    - model_max_budget: Optional[dict] - Specify max budget for a given model. Example: {"openai/gpt-4o-mini": {"max_budget": 100.0, "budget_duration": "1d", "tpm_limit": 100000, "rpm_limit": 100000}}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/budget_management_endpoints.py Line: 72
async def update_budget(budget_obj: BudgetNewRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Update an existing budget object.

    Parameters:
    - budget_duration: Optional[str] - Budget reset period ("30d", "1h", etc.)
    - budget_id: Optional[str] - The id of the budget. If not provided, a new id will be generated.
    - max_budget: Optional[float] - The max budget for the budget.
    - soft_budget: Optional[float] - The soft budget for the budget.
    - max_parallel_requests: Optional[int] - The max number of parallel requests for the budget.
    - tpm_limit: Optional[int] - The tokens per minute limit for the budget.
    - rpm_limit: Optional[int] - The requests per minute limit for the budget.
    - model_max_budget: Optional[dict] - Specify max budget for a given model. Example: {"openai/gpt-4o-mini": {"max_budget": 100.0, "budget_duration": "1d", "tpm_limit": 100000, "rpm_limit": 100000}}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/budget_management_endpoints.py Line: 115
async def info_budget(data: BudgetRequest):
    """Get the budget id specific information

    Parameters:
    - budgets: List[str] - The list of budget ids to get information for"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/budget_management_endpoints.py Line: 146
async def budget_settings(budget_id: str, user_api_key_dict: UserAPIKeyAuth=...):
    """Get list of configurable params + current value for a budget item + description of each field

    Used on Admin UI.

    Query Parameters:
    - budget_id: str - The budget id to get information for"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/budget_management_endpoints.py Line: 220
async def list_budget(user_api_key_dict: UserAPIKeyAuth=...):
    "List all the created budgets in proxy db. Used on Admin UI."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/budget_management_endpoints.py Line: 253
async def delete_budget(data: BudgetDeleteRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Delete budget

    Parameters:
    - id: str - The budget id to delete"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 94
def _is_available_team(team_id: str, user_api_key_dict: UserAPIKeyAuth) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 102
async def get_all_team_memberships(prisma_client: PrismaClient, team_id: List[str], user_id: Optional[str]) -> List[LiteLLM_TeamMembership]:
    "Get all team memberships for a given user"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 134
async def new_team(data: NewTeamRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...):
    """Allow users to create a new team. Apply user permissions to their team.

     [Detailed Doc on setting team budgets](https://docs.litellm.ai/docs/proxy/team_budgets)


    Parameters:
    - team_alias: Optional[str] - User defined team alias
    - team_id: Optional[str] - The team id of the user. If none passed, we'll generate it.
    - members_with_roles: List[{"role": "admin" or "user", "user_id": "<user-id>"}] - A list of users and their roles in the team. Get user_id when making a new user via `/user/new`.
    - team_member_permissions: Optional[List[str]] - A list of routes that non-admin team members can access. example: ["/key/generate", "/key/update", "/key/delete"]
    - metadata: Optional[dict] - Metadata for team, store information for team. Example metadata = {"extra_info": "some info"}
    - tpm_limit: Optional[int] - The TPM (Tokens Per Minute) limit for this team - all keys with this team_id will have at max this TPM limit
    - rpm_limit: Optional[int] - The RPM (Requests Per Minute) limit for this team - all keys associated with this team_id will have at max this RPM limit
    - max_budget: Optional[float] - The maximum budget allocated to the team - all keys for this team_id will have at max this max_budget
    - budget_duration: Optional[str] - The duration of the budget for the team. Doc [here](https://docs.litellm.ai/docs/proxy/team_budgets)
    - models: Optional[list] - A list of models associated with the team - all keys for this team_id will have at most, these models. If empty, assumes all models are allowed.
    - blocked: bool - Flag indicating if the team is blocked or not - will stop all calls from keys with this team_id.
    - members: Optional[List] - Control team members via `/team/member/add` and `/team/member/delete`. 
    - tags: Optional[List[str]] - Tags for [tracking spend](https://litellm.vercel.app/docs/proxy/enterprise#tracking-spend-for-custom-tags) and/or doing [tag-based routing](https://litellm.vercel.app/docs/proxy/tag_routing).
    - organization_id: Optional[str] - The organization id of the team. Default is None. Create via `/organization/new`.
    - model_aliases: Optional[dict] - Model aliases for the team. [Docs](https://docs.litellm.ai/docs/proxy/team_based_routing#create-team-with-model-alias)
    - guardrails: Optional[List[str]] - Guardrails for the team. [Docs](https://docs.litellm.ai/docs/proxy/guardrails)
    Returns:
    - team_id: (str) Unique team id - used for tracking spend across multiple keys for same team id.

    _deprecated_params:
    - admins: list - A list of user_id's for the admin role
    - users: list - A list of user_id's for the user role

    Example Request:
    ```
    curl --location 'http://0.0.0.0:4000/team/new'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
      "team_alias": "my-new-team_2",
      "members_with_roles": [{"role": "admin", "user_id": "user-1234"},
        {"role": "user", "user_id": "user-2434"}]
    }'

    ```

     ```
    curl --location 'http://0.0.0.0:4000/team/new'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
                "team_alias": "QA Prod Bot", 
                "max_budget": 0.000000001, 
                "budget_duration": "1d"
            }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 375
async def _update_model_table(data: UpdateTeamRequest, model_id: Optional[str], prisma_client: PrismaClient, user_api_key_dict: UserAPIKeyAuth, litellm_proxy_admin_name: str) -> Optional[str]:
    "Upsert model table and return the model id"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 415
async def update_team(data: UpdateTeamRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...):
    """Use `/team/member_add` AND `/team/member/delete` to add/remove new team members  

    You can now update team budget / rate limits via /team/update

    Parameters:
    - team_id: str - The team id of the user. Required param.
    - team_alias: Optional[str] - User defined team alias
    - team_member_permissions: Optional[List[str]] - A list of routes that non-admin team members can access. example: ["/key/generate", "/key/update", "/key/delete"]
    - metadata: Optional[dict] - Metadata for team, store information for team. Example metadata = {"team": "core-infra", "app": "app2", "email": "ishaan@berri.ai" }
    - tpm_limit: Optional[int] - The TPM (Tokens Per Minute) limit for this team - all keys with this team_id will have at max this TPM limit
    - rpm_limit: Optional[int] - The RPM (Requests Per Minute) limit for this team - all keys associated with this team_id will have at max this RPM limit
    - max_budget: Optional[float] - The maximum budget allocated to the team - all keys for this team_id will have at max this max_budget
    - budget_duration: Optional[str] - The duration of the budget for the team. Doc [here](https://docs.litellm.ai/docs/proxy/team_budgets)
    - models: Optional[list] - A list of models associated with the team - all keys for this team_id will have at most, these models. If empty, assumes all models are allowed.
    - blocked: bool - Flag indicating if the team is blocked or not - will stop all calls from keys with this team_id.
    - tags: Optional[List[str]] - Tags for [tracking spend](https://litellm.vercel.app/docs/proxy/enterprise#tracking-spend-for-custom-tags) and/or doing [tag-based routing](https://litellm.vercel.app/docs/proxy/tag_routing).
    - organization_id: Optional[str] - The organization id of the team. Default is None. Create via `/organization/new`.
    - model_aliases: Optional[dict] - Model aliases for the team. [Docs](https://docs.litellm.ai/docs/proxy/team_based_routing#create-team-with-model-alias)
    - guardrails: Optional[List[str]] - Guardrails for the team. [Docs](https://docs.litellm.ai/docs/proxy/guardrails)
    Example - update team TPM Limit

    ```
    curl --location 'http://0.0.0.0:4000/team/update'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data-raw '{
        "team_id": "8d916b1c-510d-4894-a334-1c16a93344f5",
        "tpm_limit": 100
    }'
    ```

    Example - Update Team `max_budget` budget
    ```
    curl --location 'http://0.0.0.0:4000/team/update'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data-raw '{
        "team_id": "8d916b1c-510d-4894-a334-1c16a93344f5",
        "max_budget": 10
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 574
def _check_team_member_admin_add(member: Union[(Member, List[Member])], premium_user: bool):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 592
def team_call_validation_checks(prisma_client: Optional[PrismaClient], data: TeamMemberAddRequest, premium_user: bool):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 617
def team_member_add_duplication_check(data: TeamMemberAddRequest, existing_team_row: LiteLLM_TeamTable):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 644
async def team_member_add(data: TeamMemberAddRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=...):
    """[BETA]

    Add new members (either via user_email or user_id) to a team

    If user doesn't exist, new user row will also be added to User Table

    Only proxy_admin or admin of team, allowed to access this endpoint.
    ```

    curl -X POST 'http://0.0.0.0:4000/team/member_add'     -H 'Authorization: Bearer sk-1234'     -H 'Content-Type: application/json'     -d '{"team_id": "45e3e396-ee08-4a61-a88e-16b3ce7e0849", "member": {"role": "user", "user_id": "krrish247652@berri.ai"}}'

    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 839
async def team_member_delete(data: TeamMemberDeleteRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """[BETA]

    delete members (either via user_email or user_id) from a team

    If user doesn't exist, an exception will be raised
    ```
    curl -X POST 'http://0.0.0.0:8000/team/member_delete' 
    -H 'Authorization: Bearer sk-1234' 
    -H 'Content-Type: application/json' 
    -d '{
        "team_id": "45e3e396-ee08-4a61-a88e-16b3ce7e0849",
        "user_id": "krrish247652@berri.ai"
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 974
async def team_member_update(data: TeamMemberUpdateRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=...):
    """[BETA]

    Update team member budgets and team member role"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1117
async def delete_team(data: DeleteTeamRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...):
    """delete team and associated team keys

    Parameters:
    - team_ids: List[str] - Required. List of team IDs to delete. Example: ["team-1234", "team-5678"]

    ```
    curl --location 'http://0.0.0.0:4000/team/delete'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data-raw '{
        "team_ids": ["8d916b1c-510d-4894-a334-1c16a93344f5"]
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1235
def validate_membership(user_api_key_dict: UserAPIKeyAuth, team_table: LiteLLM_TeamTable):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1262
def _unfurl_all_proxy_models(team_info: LiteLLM_TeamTable, llm_router: Router) -> LiteLLM_TeamTable:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1283
async def team_info(http_request: Request, team_id: str=..., user_api_key_dict: UserAPIKeyAuth=...):
    """get info on team + related keys

    Parameters:
    - team_id: str - Required. The unique identifier of the team to get info on.

    ```
    curl --location 'http://localhost:4000/team/info?team_id=your_team_id_here'     --header 'Authorization: Bearer your_api_key_here'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1412
async def block_team(data: BlockTeamRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=...):
    """Blocks all calls from keys with this team id.

    Parameters:
    - team_id: str - Required. The unique identifier of the team to block.

    Example:
    ```
    curl --location 'http://0.0.0.0:4000/team/block'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "team_id": "team-1234"
    }'
    ```

    Returns:
    - The updated team record with blocked=True"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1461
async def unblock_team(data: BlockTeamRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=...):
    """Blocks all calls from keys with this team id.

    Parameters:
    - team_id: str - Required. The unique identifier of the team to unblock.

    Example:
    ```
    curl --location 'http://0.0.0.0:4000/team/unblock'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "team_id": "team-1234"
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1501
async def list_available_teams(http_request: Request, user_api_key_dict: UserAPIKeyAuth=..., response_model=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1560
async def list_team(http_request: Request, user_id: Optional[str]=..., organization_id: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """```
    curl --location --request GET 'http://0.0.0.0:4000/team/list'         --header 'Authorization: Bearer sk-1234'
    ```

    Parameters:
    - user_id: str - Optional. If passed will only return teams that the user_id is a member of.
    - organization_id: str - Optional. If passed will only return teams that belong to the organization_id. Pass 'default_organization' to get all teams without organization_id."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1670
async def get_paginated_teams(prisma_client: PrismaClient, page_size: int=10, page: int=1) -> Tuple[(List[LiteLLM_TeamTable], int)]:
    """Get paginated list of teams from team table

    Parameters:
        prisma_client: PrismaClient - The database client
        page_size: int - Number of teams per page
        page: int - Page number (1-based)

    Returns:
        Tuple[List[LiteLLM_TeamTable], int] - (list of teams, total count)"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1704
def _update_team_metadata_field(updated_kv: dict, field_name: str) -> ?:
    """Helper function to update metadata fields that require premium user checks in the update endpoint

    Args:
        updated_kv: The key-value dict being used for the update
        field_name: Name of the metadata field being updated"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1733
async def ui_view_teams(team_id: Optional[str]=..., team_alias: Optional[str]=..., page: int=..., page_size: int=..., user_api_key_dict: UserAPIKeyAuth=...):
    """[PROXY-ADMIN ONLY] Filter teams based on partial match of team_id or team_alias with pagination.

    Args:
        user_id (Optional[str]): Partial user ID to search for
        user_email (Optional[str]): Partial email to search for
        page (int): Page number for pagination (starts at 1)
        page_size (int): Number of items per page (max 100)
        user_api_key_dict (UserAPIKeyAuth): User authentication information

    Returns:
        List[LiteLLM_SpendLogs]: Paginated list of matching user records"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1808
async def team_model_add(data: TeamModelAddRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=...):
    """Add models to a team's allowed model list. Only proxy admin or team admin can add models.

    Parameters:
    - team_id: str - Required. The team to add models to
    - models: List[str] - Required. List of models to add to the team

    Example Request:
    ```
    curl --location 'http://0.0.0.0:4000/team/model/add'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "team_id": "team-1234",
        "models": ["gpt-4", "claude-2"]
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1881
async def team_model_delete(data: TeamModelDeleteRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=...):
    """Remove models from a team's allowed model list. Only proxy admin or team admin can remove models.

    Parameters:
    - team_id: str - Required. The team to remove models from
    - models: List[str] - Required. List of models to remove from the team

    Example Request:
    ```
    curl --location 'http://0.0.0.0:4000/team/model/delete'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "team_id": "team-1234",
        "models": ["gpt-4"]
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 1954
async def team_member_permissions(team_id: str=..., user_api_key_dict: UserAPIKeyAuth=...) -> GetTeamMemberPermissionsResponse:
    "Get the team member permissions for a team"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 2028
async def update_team_member_permissions(data: UpdateTeamMemberPermissionsRequest, http_request: Request, user_api_key_dict: UserAPIKeyAuth=...) -> LiteLLM_TeamTable:
    "Update the team member permissions for a team"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/team_endpoints.py Line: 2097
async def get_team_daily_activity(team_ids: Optional[str], start_date: Optional[str], end_date: Optional[str], model: Optional[str], api_key: Optional[str], page: int=1, page_size: int=10, exclude_team_ids: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    """Get daily activity for specific teams or all teams.

    Args:
        team_ids (Optional[str]): Comma-separated list of team IDs to filter by. If not provided, returns data for all teams.
        start_date (Optional[str]): Start date for the activity period (YYYY-MM-DD).
        end_date (Optional[str]): End date for the activity period (YYYY-MM-DD).
        model (Optional[str]): Filter by model name.
        api_key (Optional[str]): Filter by API key.
        page (int): Page number for pagination.
        page_size (int): Number of items per page.
        exclude_team_ids (Optional[str]): Comma-separated list of team IDs to exclude.
    Returns:
        SpendAnalyticsPaginatedResponse: Paginated response containing daily activity data."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/common_daily_activity.py Line: 21
def update_metrics(existing_metrics: SpendMetrics, record: Any) -> SpendMetrics:
    "Update metrics with new record data."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/common_daily_activity.py Line: 35
def update_breakdown_metrics(breakdown: BreakdownMetrics, record: Any, model_metadata: Dict[(str, Dict[(str, Any)])], provider_metadata: Dict[(str, Dict[(str, Any)])], api_key_metadata: Dict[(str, Dict[(str, Any)])], entity_id_field: Optional[str], entity_metadata_field: Optional[Dict[(str, dict)]]) -> BreakdownMetrics:
    "Updates breakdown metrics for a single record using the existing update_metrics function"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/common_daily_activity.py Line: 104
async def get_api_key_metadata(prisma_client: PrismaClient, api_keys: Set[str]) -> Dict[(str, Dict[(str, Any)])]:
    "Update api key metadata for a single record."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/common_daily_activity.py Line: 117
async def get_daily_activity(prisma_client: Optional[PrismaClient], table_name: str, entity_id_field: str, entity_id: Optional[Union[(str, List[str])]], entity_metadata_field: Optional[Dict[(str, dict)]], start_date: Optional[str], end_date: Optional[str], model: Optional[str], api_key: Optional[str], page: int, page_size: int, exclude_entity_ids: Optional[List[str]]) -> SpendAnalyticsPaginatedResponse:
    "Common function to get daily activity for any entity type."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 57
async def get_db_model(model_id: str, prisma_client: PrismaClient) -> Optional[Deployment]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 74
def update_db_model(db_model: Deployment, updated_patch: updateDeployment) -> PrismaCompatibleUpdateDBModel:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 132
async def patch_model(model_id: str, patch_data: updateDeployment, user_api_key_dict: UserAPIKeyAuth=...):
    """PATCH Endpoint for partial model updates.

    Only updates the fields specified in the request while preserving other existing values.
    Follows proper PATCH semantics by only modifying provided fields.

    Args:
        model_id: The ID of the model to update
        patch_data: The fields to update and their new values
        user_api_key_dict: User authentication information

    Returns:
        Updated model information

    Raises:
        ProxyException: For various error conditions including authentication and database errors"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 233
async def _add_model_to_db(model_params: Deployment, user_api_key_dict: UserAPIKeyAuth, prisma_client: PrismaClient, new_encryption_key: Optional[str], should_create_model_in_db: bool=True) -> Optional[LiteLLM_ProxyModelTable]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 269
async def _add_team_model_to_db(model_params: Deployment, user_api_key_dict: UserAPIKeyAuth, prisma_client: PrismaClient) -> Optional[LiteLLM_ProxyModelTable]:
    """If 'team_id' is provided,

    - generate a unique 'model_name' for the model (e.g. 'model_name_{team_id}_{uuid})
    - store the model in the db with the unique 'model_name'
    - store a team model alias mapping {"model_name": "model_name_{team_id}_{uuid}"}"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 322
class ModelManagementAuthChecks:
    "Common auth checks for model management endpoints"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 450
async def delete_model(model_info: ModelInfoDelete, user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 567
async def add_new_model(model_params: Deployment, user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 700
async def update_model(model_params: updateDeployment, user_api_key_dict: UserAPIKeyAuth=...):
    """Old endpoint for model update. Makes a PUT request.

    Use `/model/{model_id}/update` to PATCH the stored model in db."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/model_management_endpoints.py Line: 851
def _deduplicate_litellm_router_models(models: List[Dict]) -> List[Dict]:
    """Deduplicate models based on their model_info.id field.
    Returns a list of unique models keeping only the first occurrence of each model ID.

    Args:
        models: List of model dictionaries containing model_info

    Returns:
        List of deduplicated model dictionaries"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 53
async def create_internal_user_audit_log(user_id: str, action: AUDIT_ACTIONS, litellm_changed_by: Optional[str], user_api_key_dict: UserAPIKeyAuth, litellm_proxy_admin_name: Optional[str], before_value: Optional[str], after_value: Optional[str]):
    """Create an audit log for an internal user.

    Parameters:
    - user_id: str - The id of the user to create the audit log for.
    - action: AUDIT_ACTIONS - The action to create the audit log for.
    - user_row: LiteLLM_UserTable - The user row to create the audit log for.
    - litellm_changed_by: Optional[str] - The user id of the user who is changing the user.
    - user_api_key_dict: UserAPIKeyAuth - The user api key dictionary.
    - litellm_proxy_admin_name: Optional[str] - The name of the proxy admin."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 93
def _update_internal_new_user_params(data_json: dict, data: NewUserRequest) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 129
async def _check_duplicate_user_email(user_email: Optional[str], prisma_client: Any) -> ?:
    """Helper function to check if a user email already exists in the database.

    Args:
        user_email (Optional[str]): Email to check
        prisma_client (Any): Database client instance

    Raises:
        Exception: If database is not connected
        HTTPException: If user with email already exists"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 165
async def new_user(data: NewUserRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Use this to create a new INTERNAL user with a budget.
    Internal Users can access LiteLLM Admin UI to make keys, request access to models.
    This creates a new user and generates a new api key for the new user. The new api key is returned.

    Returns user id, budget + new key.

    Parameters:
    - user_id: Optional[str] - Specify a user id. If not set, a unique id will be generated.
    - user_alias: Optional[str] - A descriptive name for you to know who this user id refers to.
    - teams: Optional[list] - specify a list of team id's a user belongs to.
    - user_email: Optional[str] - Specify a user email.
    - send_invite_email: Optional[bool] - Specify if an invite email should be sent.
    - user_role: Optional[str] - Specify a user role - "proxy_admin", "proxy_admin_viewer", "internal_user", "internal_user_viewer", "team", "customer". Info about each role here: `https://github.com/BerriAI/litellm/litellm/proxy/_types.py#L20`
    - max_budget: Optional[float] - Specify max budget for a given user.
    - budget_duration: Optional[str] - Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").
    - models: Optional[list] - Model_name's a user is allowed to call. (if empty, key is allowed to call all models). Set to ['no-default-models'] to block all model access. Restricting user to only team-based model access.
    - tpm_limit: Optional[int] - Specify tpm limit for a given user (Tokens per minute)
    - rpm_limit: Optional[int] - Specify rpm limit for a given user (Requests per minute)
    - auto_create_key: bool - Default=True. Flag used for returning a key as part of the /user/new response
    - aliases: Optional[dict] - Model aliases for the user - [Docs](https://litellm.vercel.app/docs/proxy/virtual_keys#model-aliases)
    - config: Optional[dict] - [DEPRECATED PARAM] User-specific config.
    - allowed_cache_controls: Optional[list] - List of allowed cache control values. Example - ["no-cache", "no-store"]. See all values - https://docs.litellm.ai/docs/proxy/caching#turn-on--off-caching-per-request-
    - blocked: Optional[bool] - [Not Implemented Yet] Whether the user is blocked.
    - guardrails: Optional[List[str]] - [Not Implemented Yet] List of active guardrails for the user
    - permissions: Optional[dict] - [Not Implemented Yet] User-specific permissions, eg. turning off pii masking.
    - metadata: Optional[dict] - Metadata for user, store information for user. Example metadata = {"team": "core-infra", "app": "app2", "email": "ishaan@berri.ai" }
    - max_parallel_requests: Optional[int] - Rate limit a user based on the number of parallel requests. Raises 429 error, if user's parallel requests > x.
    - soft_budget: Optional[float] - Get alerts when user crosses given budget, doesn't block requests.
    - model_max_budget: Optional[dict] - Model-specific max budget for user. [Docs](https://docs.litellm.ai/docs/proxy/users#add-model-specific-budgets-to-keys)
    - model_rpm_limit: Optional[float] - Model-specific rpm limit for user. [Docs](https://docs.litellm.ai/docs/proxy/users#add-model-specific-limits-to-keys)
    - model_tpm_limit: Optional[float] - Model-specific tpm limit for user. [Docs](https://docs.litellm.ai/docs/proxy/users#add-model-specific-limits-to-keys)
    - spend: Optional[float] - Amount spent by user. Default is 0. Will be updated by proxy whenever user is used. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").
    - team_id: Optional[str] - [DEPRECATED PARAM] The team id of the user. Default is None. 
    - duration: Optional[str] - Duration for the key auto-created on `/user/new`. Default is None.
    - key_alias: Optional[str] - Alias for the key auto-created on `/user/new`. Default is None.

    Returns:
    - key: (str) The generated api key for the user
    - expires: (datetime) Datetime object for when key expires.
    - user_id: (str) Unique user id - used for tracking spend across multiple keys for same user id.
    - max_budget: (float|None) Max budget for given user.

    Usage Example 

    ```shell
     curl -X POST "http://localhost:4000/user/new"      -H "Content-Type: application/json"      -H "Authorization: Bearer sk-1234"      -d '{
         "username": "new_user",
         "email": "new_user@example.com"
     }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 371
async def ui_get_available_role(user_api_key_dict: UserAPIKeyAuth=...):
    """Endpoint used by Admin UI to show all available roles to assign a user
    return {
        "proxy_admin": {
            "description": "Proxy Admin role",
            "ui_label": "Admin"
        }
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 400
def get_team_from_list(team_list: Optional[Union[(List[LiteLLM_TeamTable], List[TeamListResponseObject])]], team_id: str) -> Optional[Union[(LiteLLM_TeamTable, LiteLLM_TeamMembership)]]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 420
async def user_info(user_id: Optional[str]=..., user_api_key_dict: UserAPIKeyAuth=...):
    """[10/07/2024]
    Note: To get all users (+pagination), use `/user/list` endpoint.


    Use this to get user information. (user row + all user key info)

    Example request
    ```
    curl -X GET 'http://localhost:4000/user/info?user_id=krrish7%40berri.ai'     --header 'Authorization: Bearer sk-1234'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 543
async def _get_user_info_for_proxy_admin():
    """Admin UI Endpoint - Returns All Teams and Keys when Proxy Admin is querying

    - get all teams in LiteLLM_TeamTable
    - get all keys in LiteLLM_VerificationToken table

    Why separate helper for proxy admin ?
        - To get Faster UI load times, get all teams and virtual keys in 1 query"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 591
def _process_keys_for_user_info(keys: Optional[List[LiteLLM_VerificationToken]], all_teams: Optional[Union[(List[LiteLLM_TeamTable], List[TeamListResponseObject])]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 633
def _update_internal_user_params(data_json: dict, data: UpdateUserRequest) -> dict:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 685
async def user_update(data: UpdateUserRequest, user_api_key_dict: UserAPIKeyAuth=...):
    """Example curl 

    ```
    curl --location 'http://0.0.0.0:4000/user/update'     --header 'Authorization: Bearer sk-1234'     --header 'Content-Type: application/json'     --data '{
        "user_id": "test-litellm-user-4",
        "user_role": "proxy_admin_viewer"
    }'
    ```

    Parameters:
        - user_id: Optional[str] - Specify a user id. If not set, a unique id will be generated.
        - user_email: Optional[str] - Specify a user email.
        - password: Optional[str] - Specify a user password.
        - user_alias: Optional[str] - A descriptive name for you to know who this user id refers to.
        - teams: Optional[list] - specify a list of team id's a user belongs to.
        - send_invite_email: Optional[bool] - Specify if an invite email should be sent.
        - user_role: Optional[str] - Specify a user role - "proxy_admin", "proxy_admin_viewer", "internal_user", "internal_user_viewer", "team", "customer". Info about each role here: `https://github.com/BerriAI/litellm/litellm/proxy/_types.py#L20`
        - max_budget: Optional[float] - Specify max budget for a given user.
        - budget_duration: Optional[str] - Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").
        - models: Optional[list] - Model_name's a user is allowed to call. (if empty, key is allowed to call all models)
        - tpm_limit: Optional[int] - Specify tpm limit for a given user (Tokens per minute)
        - rpm_limit: Optional[int] - Specify rpm limit for a given user (Requests per minute)
        - auto_create_key: bool - Default=True. Flag used for returning a key as part of the /user/new response
        - aliases: Optional[dict] - Model aliases for the user - [Docs](https://litellm.vercel.app/docs/proxy/virtual_keys#model-aliases)
        - config: Optional[dict] - [DEPRECATED PARAM] User-specific config.
        - allowed_cache_controls: Optional[list] - List of allowed cache control values. Example - ["no-cache", "no-store"]. See all values - https://docs.litellm.ai/docs/proxy/caching#turn-on--off-caching-per-request-
        - blocked: Optional[bool] - [Not Implemented Yet] Whether the user is blocked.
        - guardrails: Optional[List[str]] - [Not Implemented Yet] List of active guardrails for the user
        - permissions: Optional[dict] - [Not Implemented Yet] User-specific permissions, eg. turning off pii masking.
        - metadata: Optional[dict] - Metadata for user, store information for user. Example metadata = {"team": "core-infra", "app": "app2", "email": "ishaan@berri.ai" }
        - max_parallel_requests: Optional[int] - Rate limit a user based on the number of parallel requests. Raises 429 error, if user's parallel requests > x.
        - soft_budget: Optional[float] - Get alerts when user crosses given budget, doesn't block requests.
        - model_max_budget: Optional[dict] - Model-specific max budget for user. [Docs](https://docs.litellm.ai/docs/proxy/users#add-model-specific-budgets-to-keys)
        - model_rpm_limit: Optional[float] - Model-specific rpm limit for user. [Docs](https://docs.litellm.ai/docs/proxy/users#add-model-specific-limits-to-keys)
        - model_tpm_limit: Optional[float] - Model-specific tpm limit for user. [Docs](https://docs.litellm.ai/docs/proxy/users#add-model-specific-limits-to-keys)
        - spend: Optional[float] - Amount spent by user. Default is 0. Will be updated by proxy whenever user is used. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").
        - team_id: Optional[str] - [DEPRECATED PARAM] The team id of the user. Default is None. 
        - duration: Optional[str] - [NOT IMPLEMENTED].
        - key_alias: Optional[str] - [NOT IMPLEMENTED].
            """

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 868
async def get_user_key_counts(prisma_client, user_ids: Optional[List[str]]):
    """Helper function to get the count of keys for each user using Prisma's count method.

    Args:
        prisma_client: The Prisma client instance
        user_ids: List of user IDs to get key counts for

    Returns:
        Dictionary mapping user_id to key count"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 905
def _validate_sort_params(sort_by: Optional[str], sort_order: str) -> Optional[Dict[(str, str)]]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 947
async def get_users(role: Optional[str]=..., user_ids: Optional[str]=..., sso_user_ids: Optional[str]=..., user_email: Optional[str]=..., team: Optional[str]=..., page: int=..., page_size: int=..., sort_by: Optional[str]=..., sort_order: str=...):
    """Get a paginated list of users with filtering and sorting options.

    Parameters:
        role: Optional[str]
            Filter users by role. Can be one of:
            - proxy_admin
            - proxy_admin_viewer
            - internal_user
            - internal_user_viewer
        user_ids: Optional[str]
            Get list of users by user_ids. Comma separated list of user_ids.
        sso_ids: Optional[str]
            Get list of users by sso_ids. Comma separated list of sso_ids.
        user_email: Optional[str]
            Filter users by partial email match
        team: Optional[str]
            Filter users by team id. Will match if user has this team in their teams array.
        page: int
            The page number to return
        page_size: int
            The number of items per page
        sort_by: Optional[str]
            Column to sort by (e.g. 'user_id', 'user_email', 'created_at', 'spend')
        sort_order: Optional[str]
            Sort order ('asc' or 'desc')"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 1105
async def delete_user(data: DeleteUserRequest, user_api_key_dict: UserAPIKeyAuth=..., litellm_changed_by: Optional[str]=...):
    """delete user and associated user keys

    ```
    curl --location 'http://0.0.0.0:4000/user/delete' 
    --header 'Authorization: Bearer sk-1234' 
    --header 'Content-Type: application/json' 
    --data-raw '{
        "user_ids": ["45e3e396-ee08-4a61-a88e-16b3ce7e0849"]
    }'
    ```

    Parameters:
    - user_ids: List[str] - The list of user id's to be deleted."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 1204
async def add_internal_user_to_organization(user_id: str, organization_id: str, user_role: LitellmUserRoles):
    """Helper function to add an internal user to an organization

    Adds the user to LiteLLM_OrganizationMembership table

    - Checks if organization_id exists

    Raises:
    - Exception if database not connected
    - Exception if user_id or organization_id not found"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 1259
async def ui_view_users(user_id: Optional[str]=..., user_email: Optional[str]=..., page: int=..., page_size: int=..., user_api_key_dict: UserAPIKeyAuth=...):
    """[PROXY-ADMIN ONLY]Filter users based on partial match of user_id or email with pagination.

    Args:
        user_id (Optional[str]): Partial user ID to search for
        user_email (Optional[str]): Partial email to search for
        page (int): Page number for pagination (starts at 1)
        page_size (int): Number of items per page (max 100)
        user_api_key_dict (UserAPIKeyAuth): User authentication information

    Returns:
        List[LiteLLM_SpendLogs]: Paginated list of matching user records"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 1331
def update_metrics(group_metrics: SpendMetrics, record: LiteLLM_DailyUserSpend) -> SpendMetrics:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 1346
def update_breakdown_metrics(breakdown: BreakdownMetrics, record: LiteLLM_DailyUserSpend, model_metadata: Dict[(str, Dict[(str, Any)])], provider_metadata: Dict[(str, Dict[(str, Any)])], api_key_metadata: Dict[(str, Dict[(str, Any)])]) -> BreakdownMetrics:
    "Updates breakdown metrics for a single record using the existing update_metrics function"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py Line: 1403
async def get_user_daily_activity(start_date: Optional[str]=..., end_date: Optional[str]=..., model: Optional[str]=..., api_key: Optional[str]=..., page: int=..., page_size: int=..., user_api_key_dict: UserAPIKeyAuth=...) -> SpendAnalyticsPaginatedResponse:
    """[BETA] This is a beta endpoint. It will change.

    Meant to optimize querying spend data for analytics for a user.

    Returns:
    (by date)
    - spend
    - prompt_tokens
    - completion_tokens
    - cache_read_input_tokens
    - cache_creation_input_tokens
    - total_tokens
    - api_requests
    - breakdown by model, api_key, provider"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/fine_tuning_endpoints/endpoints.py Line: 28
def set_fine_tuning_config(config):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/fine_tuning_endpoints/endpoints.py Line: 46
def get_fine_tuning_provider_config(custom_llm_provider: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/fine_tuning_endpoints/endpoints.py Line: 72
async def create_fine_tuning_job(request: Request, fastapi_response: Response, fine_tuning_request: LiteLLMFineTuningJobCreate, user_api_key_dict: UserAPIKeyAuth=...):
    """Creates a fine-tuning job which begins the process of creating a new model from a given dataset.
    This is the equivalent of POST https://api.openai.com/v1/fine_tuning/jobs

    Supports Identical Params as: https://platform.openai.com/docs/api-reference/fine-tuning/create

    Example Curl:
    ```
    curl http://localhost:4000/v1/fine_tuning/jobs       -H "Content-Type: application/json"       -H "Authorization: Bearer sk-1234"       -d '{
        "model": "gpt-3.5-turbo",
        "training_file": "file-abc123",
        "hyperparameters": {
          "n_epochs": 4
        }
      }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/fine_tuning_endpoints/endpoints.py Line: 190
async def retrieve_fine_tuning_job(request: Request, fastapi_response: Response, fine_tuning_job_id: str, custom_llm_provider: Literal[(?, ?)], user_api_key_dict: UserAPIKeyAuth=...):
    """Retrieves a fine-tuning job.
    This is the equivalent of GET https://api.openai.com/v1/fine_tuning/jobs/{fine_tuning_job_id}

    Supported Query Params:
    - `custom_llm_provider`: Name of the LiteLLM provider
    - `fine_tuning_job_id`: The ID of the fine-tuning job to retrieve."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/fine_tuning_endpoints/endpoints.py Line: 287
async def list_fine_tuning_jobs(request: Request, fastapi_response: Response, custom_llm_provider: Literal[(?, ?)], after: Optional[str], limit: Optional[int], user_api_key_dict: UserAPIKeyAuth=...):
    """Lists fine-tuning jobs for the organization.
    This is the equivalent of GET https://api.openai.com/v1/fine_tuning/jobs

    Supported Query Params:
    - `custom_llm_provider`: Name of the LiteLLM provider
    - `after`: Identifier for the last job from the previous pagination request.
    - `limit`: Number of fine-tuning jobs to retrieve (default is 20)."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/fine_tuning_endpoints/endpoints.py Line: 387
async def cancel_fine_tuning_job(request: Request, fastapi_response: Response, fine_tuning_job_id: str, user_api_key_dict: UserAPIKeyAuth=...):
    """Cancel a fine-tuning job.

    This is the equivalent of POST https://api.openai.com/v1/fine_tuning/jobs/{fine_tuning_job_id}/cancel

    Supported Query Params:
    - `custom_llm_provider`: Name of the LiteLLM provider
    - `fine_tuning_job_id`: The ID of the fine-tuning job to cancel."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 24
def _is_master_key(api_key: str, _master_key: Optional[str]) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 41
def _get_spend_logs_metadata(metadata: Optional[dict], applied_guardrails: Optional[List[str]], batch_models: Optional[List[str]], mcp_tool_call_metadata: Optional[StandardLoggingMCPToolCall], usage_object: Optional[dict], model_map_information: Optional[StandardLoggingModelInformation]) -> SpendLogsMetadata:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 90
def generate_hash_from_response(response_obj: Any) -> str:
    """Generate a stable hash from a response object.

    Args:
        response_obj: The response object to hash (can be dict, list, etc.)

    Returns:
        A hex string representation of the MD5 hash"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 113
def get_spend_logs_id(call_type: str, response_obj: dict, kwargs: dict) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 126
def get_logging_payload(kwargs, response_obj, start_time, end_time) -> SpendLogsPayload:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 308
def _ensure_datetime_utc(timestamp: datetime) -> datetime:
    "Helper to ensure datetime is in UTC"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 314
async def get_spend_by_team_and_customer(start_date: dt, end_date: dt, team_id: str, customer_id: str, prisma_client: PrismaClient):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 390
def _get_messages_for_spend_logs_payload(standard_logging_payload: Optional[StandardLoggingPayload], metadata: Optional[dict]) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 397
def _sanitize_request_body_for_spend_logs_payload(request_body: dict, visited: Optional[set]) -> dict:
    """Recursively sanitize request body to prevent logging large base64 strings or other large values.
    Truncates strings longer than 1000 characters and handles nested dictionaries."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 430
def _add_proxy_server_request_to_metadata(metadata: dict, litellm_params: dict) -> dict:
    "Only store if _should_store_prompts_and_responses_in_spend_logs() is True"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 449
def _get_response_for_spend_logs_payload(payload: Optional[StandardLoggingPayload]) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py Line: 459
def _should_store_prompts_and_responses_in_spend_logs() -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 35
async def spend_key_fn():
    """View all keys created, ordered by spend

    Example Request:
    ```
    curl -X GET "http://0.0.0.0:8000/spend/keys" -H "Authorization: Bearer sk-1234"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 70
async def spend_user_fn(user_id: Optional[str]=...):
    """View all users created, ordered by spend

    Example Request:
    ```
    curl -X GET "http://0.0.0.0:8000/spend/users" -H "Authorization: Bearer sk-1234"
    ```

    View User Table row for user_id
    ```
    curl -X GET "http://0.0.0.0:8000/spend/users?user_id=1234" -H "Authorization: Bearer sk-1234"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 126
async def view_spend_tags(start_date: Optional[str]=..., end_date: Optional[str]=...):
    """LiteLLM Enterprise - View Spend Per Request Tag

    Example Request:
    ```
    curl -X GET "http://0.0.0.0:8000/spend/tags" -H "Authorization: Bearer sk-1234"
    ```

    Spend with Start Date and End Date
    ```
    curl -X GET "http://0.0.0.0:8000/spend/tags?start_date=2022-01-01&end_date=2022-02-01" -H "Authorization: Bearer sk-1234"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 193
async def get_global_activity_internal_user(user_api_key_dict: UserAPIKeyAuth, start_date: datetime, end_date: datetime):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 231
async def get_global_activity(start_date: Optional[str]=..., end_date: Optional[str]=..., user_api_key_dict: UserAPIKeyAuth=...):
    """Get number of API Requests, total tokens through proxy

    {
        "daily_data": [
                const chartdata = [
                {
                date: 'Jan 22',
                api_requests: 10,
                total_tokens: 2000
                },
                {
                date: 'Jan 23',
                api_requests: 10,
                total_tokens: 12
                },
        ],
        "sum_api_requests": 20,
        "sum_total_tokens": 2012
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 335
async def get_global_activity_model_internal_user(user_api_key_dict: UserAPIKeyAuth, start_date: datetime, end_date: datetime):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 374
async def get_global_activity_model(start_date: Optional[str]=..., end_date: Optional[str]=..., user_api_key_dict: UserAPIKeyAuth=...):
    """Get number of API Requests, total tokens through proxy - Grouped by MODEL

    [
        {
            "model": "gpt-4",
            "daily_data": [
                    const chartdata = [
                    {
                    date: 'Jan 22',
                    api_requests: 10,
                    total_tokens: 2000
                    },
                    {
                    date: 'Jan 23',
                    api_requests: 10,
                    total_tokens: 12
                    },
            ],
            "sum_api_requests": 20,
            "sum_total_tokens": 2012

        },
        {
            "model": "azure/gpt-4-turbo",
            "daily_data": [
                    const chartdata = [
                    {
                    date: 'Jan 22',
                    api_requests: 10,
                    total_tokens: 2000
                    },
                    {
                    date: 'Jan 23',
                    api_requests: 10,
                    total_tokens: 12
                    },
            ],
            "sum_api_requests": 20,
            "sum_total_tokens": 2012

        },
    ]"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 530
async def get_global_activity_exceptions_per_deployment(model_group: str=..., start_date: Optional[str]=..., end_date: Optional[str]=...):
    """Get number of 429 errors - Grouped by deployment

    [
        {
            "deployment": "https://azure-us-east-1.openai.azure.com/",
            "daily_data": [
                    const chartdata = [
                    {
                    date: 'Jan 22',
                    num_rate_limit_exceptions: 10
                    },
                    {
                    date: 'Jan 23',
                    num_rate_limit_exceptions: 12
                    },
            ],
            "sum_num_rate_limit_exceptions": 20,

        },
        {
            "deployment": "https://azure-us-east-1.openai.azure.com/",
            "daily_data": [
                    const chartdata = [
                    {
                    date: 'Jan 22',
                    num_rate_limit_exceptions: 10,
                    },
                    {
                    date: 'Jan 23',
                    num_rate_limit_exceptions: 12
                    },
            ],
            "sum_num_rate_limit_exceptions": 20,

        },
    ]"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 683
async def get_global_activity_exceptions(model_group: str=..., start_date: Optional[str]=..., end_date: Optional[str]=...):
    """Get number of API Requests, total tokens through proxy

    {
        "daily_data": [
                const chartdata = [
                {
                date: 'Jan 22',
                num_rate_limit_exceptions: 10,
                },
                {
                date: 'Jan 23',
                num_rate_limit_exceptions: 10,
                },
        ],
        "sum_api_exceptions": 20,
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 791
async def get_global_spend_provider(start_date: Optional[str]=..., end_date: Optional[str]=..., user_api_key_dict: UserAPIKeyAuth=...):
    """Get breakdown of spend per provider
    [
        {
            "provider": "Azure OpenAI",
            "spend": 20
        },
        {
            "provider": "OpenAI",
            "spend": 10
        },
        {
            "provider": "VertexAI",
            "spend": 30
        }
    ]"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 921
async def get_global_spend_report(start_date: Optional[str]=..., end_date: Optional[str]=..., group_by: Optional[Literal[(?, ?, ?)]]=..., api_key: Optional[str]=..., internal_user_id: Optional[str]=..., team_id: Optional[str]=..., customer_id: Optional[str]=...):
    """Get Daily Spend per Team, based on specific startTime and endTime. Per team, view usage by each key, model
    [
        {
            "group-by-day": "2024-05-10",
            "teams": [
                {
                    "team_name": "team-1"
                    "spend": 10,
                    "keys": [
                        "key": "1213",
                        "usage": {
                            "model-1": {
                                    "cost": 12.50,
                                    "input_tokens": 1000,
                                    "output_tokens": 5000,
                                    "requests": 100
                                },
                                "audio-modelname1": {
                                "cost": 25.50,
                                "seconds": 25,
                                "requests": 50
                        },
                        }
                    }
            ]
        ]
    }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 1279
async def global_get_all_tag_names():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 1330
async def global_view_spend_tags(start_date: Optional[str]=..., end_date: Optional[str]=..., tags: Optional[str]=...):
    """LiteLLM Enterprise - View Spend Per Request Tag. Used by LiteLLM UI

    Example Request:
    ```
    curl -X GET "http://0.0.0.0:4000/spend/tags" -H "Authorization: Bearer sk-1234"
    ```

    Spend with Start Date and End Date
    ```
    curl -X GET "http://0.0.0.0:4000/spend/tags?start_date=2022-01-01&end_date=2022-02-01" -H "Authorization: Bearer sk-1234"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 1404
async def _get_spend_report_for_time_range(start_date: str, end_date: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 1470
async def calculate_spend(request: SpendCalculateRequest):
    """Accepts all the params of completion_cost.

    Calculate spend **before** making call:

    Note: If you see a spend of $0.0 you need to set custom_pricing for your model: https://docs.litellm.ai/docs/proxy/custom_pricing

    ```
    curl --location 'http://localhost:4000/spend/calculate'
    --header 'Authorization: Bearer sk-1234'
    --header 'Content-Type: application/json'
    --data '{
        "model": "anthropic.claude-v2",
        "messages": [{"role": "user", "content": "Hey, how'''s it going?"}]
    }'
    ```

    Calculate spend **after** making call:

    ```
    curl --location 'http://localhost:4000/spend/calculate'
    --header 'Authorization: Bearer sk-1234'
    --header 'Content-Type: application/json'
    --data '{
        "completion_response": {
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-3.5-turbo-0125",
            "system_fingerprint": "fp_44709d6fcb",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "Hello there, how may I assist you today?"
                },
                "logprobs": null,
                "finish_reason": "stop"
            }]
            "usage": {
                "prompt_tokens": 9,
                "completion_tokens": 12,
                "total_tokens": 21
            }
        }
    }'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 1617
async def ui_view_spend_logs(api_key: Optional[str]=..., user_id: Optional[str]=..., request_id: Optional[str]=..., team_id: Optional[str]=..., min_spend: Optional[float]=..., max_spend: Optional[float]=..., start_date: Optional[str]=..., end_date: Optional[str]=..., page: int=..., page_size: int=..., user_api_key_dict: UserAPIKeyAuth=...):
    """View spend logs for UI with pagination support

    Returns:
        {
            "data": List[LiteLLM_SpendLogs],  # Paginated spend logs
            "total": int,                      # Total number of records
            "page": int,                       # Current page number
            "page_size": int,                  # Number of items per page
            "total_pages": int                 # Total number of pages
        }"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 1766
async def ui_view_request_response_for_request_id(request_id: str, start_date: Optional[str]=..., end_date: Optional[str]=...):
    """View request / response for a specific request_id

    - goes through all callbacks, checks if any of them have a @property -> has_request_response_payload
    - if so, it will return the request and response payload"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 1817
async def view_spend_logs(api_key: Optional[str]=..., user_id: Optional[str]=..., request_id: Optional[str]=..., start_date: Optional[str]=..., end_date: Optional[str]=..., user_api_key_dict: UserAPIKeyAuth=...):
    """View all spend logs, if request_id is provided, only logs for that request_id will be returned

    Example Request for all logs
    ```
    curl -X GET "http://0.0.0.0:8000/spend/logs" -H "Authorization: Bearer sk-1234"
    ```

    Example Request for specific request_id
    ```
    curl -X GET "http://0.0.0.0:8000/spend/logs?request_id=chatcmpl-6dcb2540-d3d7-4e49-bb27-291f863f112e" -H "Authorization: Bearer sk-1234"
    ```

    Example Request for specific api_key
    ```
    curl -X GET "http://0.0.0.0:8000/spend/logs?api_key=sk-Fn8Ej39NkBQmUagFEoUWPQ" -H "Authorization: Bearer sk-1234"
    ```

    Example Request for specific user_id
    ```
    curl -X GET "http://0.0.0.0:8000/spend/logs?user_id=ishaan@berri.ai" -H "Authorization: Bearer sk-1234"
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2029
async def global_spend_reset():
    """ADMIN ONLY / MASTER KEY Only Endpoint

    Globally reset spend for All API Keys and Teams, maintain LiteLLM_SpendLogs

    1. LiteLLM_SpendLogs will maintain the logs on spend, no data gets deleted from there
    2. LiteLLM_VerificationTokens spend will be set = 0
    3. LiteLLM_TeamTable spend will be set = 0"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2067
async def global_spend_refresh():
    """ADMIN ONLY / MASTER KEY Only Endpoint

    Globally refresh spend MonthlyGlobalSpend view"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2142
async def global_spend_for_internal_user(api_key: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2186
async def global_spend_logs(api_key: Optional[str]=..., user_api_key_dict: UserAPIKeyAuth=...):
    """[BETA] This is a beta endpoint. It will change.

    Use this to get global spend (spend per day for last 30d). Admin-only endpoint

    More efficient implementation of /spend/logs, by creating a view over the spend logs table."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2277
async def global_spend():
    """[BETA] This is a beta endpoint. It will change.

    View total spend across all proxy keys"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2319
async def global_spend_key_internal_user(user_api_key_dict: UserAPIKeyAuth, limit: int=10):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2371
async def global_spend_keys(limit: int=..., user_api_key_dict: UserAPIKeyAuth=...):
    """[BETA] This is a beta endpoint. It will change.

    Use this to get the top 'n' keys with the highest spend, ordered by spend."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2421
async def global_spend_per_team():
    """[BETA] This is a beta endpoint. It will change.

    Use this to get daily spend, grouped by `team_id` and `date`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2514
async def global_view_all_end_users():
    """[BETA] This is a beta endpoint. It will change.

    Use this to just get all the unique `end_users`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2546
async def global_spend_end_users(data: Optional[GlobalEndUsersSpend]):
    """[BETA] This is a beta endpoint. It will change.

    Use this to get the top 'n' keys with the highest spend, ordered by spend."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2593
async def global_spend_models_internal_user(user_api_key_dict: UserAPIKeyAuth, limit: int=10):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2632
async def global_spend_models(limit: int=..., user_api_key_dict: UserAPIKeyAuth=...):
    """[BETA] This is a beta endpoint. It will change.

    Use this to get the top 'n' models with the highest spend, ordered by spend."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2666
async def provider_budgets() -> ProviderBudgetResponse:
    """Provider Budget Routing - Get Budget, Spend Details https://docs.litellm.ai/docs/proxy/provider_budget_routing

    Use this endpoint to check current budget, spend and budget reset time for a provider

    Example Request

    ```bash
    curl -X GET http://localhost:4000/provider/budgets     -H "Content-Type: application/json"     -H "Authorization: Bearer sk-1234"
    ```

    Example Response

    ```json
    {
        "providers": {
            "openai": {
                "budget_limit": 1e-12,
                "time_period": "1d",
                "spend": 0.0,
                "budget_reset_at": null
            },
            "azure": {
                "budget_limit": 100.0,
                "time_period": "1d",
                "spend": 0.0,
                "budget_reset_at": null
            },
            "anthropic": {
                "budget_limit": 100.0,
                "time_period": "10d",
                "spend": 0.0,
                "budget_reset_at": null
            },
            "vertex_ai": {
                "budget_limit": 100.0,
                "time_period": "12d",
                "spend": 0.0,
                "budget_reset_at": null
            }
        }
    }
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2756
async def get_spend_by_tags(prisma_client: PrismaClient, start_date, end_date):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_management_endpoints.py Line: 2773
async def ui_get_spend_by_tags(start_date: str, end_date: str, prisma_client: Optional[PrismaClient], tags_str: Optional[str]):
    """Should cover 2 cases:
    1. When user is getting spend for all_tags. "all_tags" in tags_list
    2. When user is getting spend for specific tags."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/config_management_endpoints/pass_through_endpoints.py Line: 21
async def create_fine_tuning_job(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/middleware/prometheus_auth_middleware.py Line: 13
class PrometheusAuthMiddleware(BaseHTTPMiddleware):
    """Middleware to authenticate requests to the metrics endpoint

    By default, auth is not run on the metrics endpoint

    Enabled by setting the following in proxy_config.yaml:

    ```yaml
    litellm_settings:
        require_auth_for_metrics_endpoint: true
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_exception_handler.py Line: 25
class UserAPIKeyAuthExceptionHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/route_checks.py Line: 18
class RouteChecks:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks_organization.py Line: 12
def organization_role_based_access_check(request_body: dict, user_object: Optional[LiteLLM_UserTable], route: str):
    """Role based access control checks only run if a user is part of an Organization

    Organization Checks:
    ONLY RUN IF user_object.organization_memberships is not None

    1. Only Proxy Admins can access /organization/new
    2. IF route is a LiteLLMRoutes.org_admin_only_routes, then check if user is an Org Admin for that organization"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks_organization.py Line: 116
def get_user_organization_info(user_object: LiteLLM_UserTable) -> Tuple[(List[str], Dict[(str, Optional[LitellmUserRoles])])]:
    """Helper function to extract user organization information.

    Args:
        user_object (LiteLLM_UserTable): The user object containing organization memberships.

    Returns:
        Tuple[List[str], Dict[str, Optional[LitellmUserRoles]]]: A tuple containing:
            - List of organization IDs the user is a member of
            - Dictionary mapping organization IDs to user roles"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks_organization.py Line: 142
def _user_is_org_admin(request_data: dict, user_object: Optional[LiteLLM_UserTable]) -> bool:
    "Helper function to check if user is an org admin for the passed organization_id"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/oauth2_check.py Line: 4
async def check_oauth2_token(token: str) -> UserAPIKeyAuth:
    """Makes a request to the token info endpoint to validate the OAuth2 token.

    Args:
    token (str): The OAuth2 token to validate.

    Returns:
    Literal[True]: If the token is valid.

    Raises:
    ValueError: If the token is invalid, the request fails, or the token info endpoint is not set."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/handle_jwt.py Line: 52
class JWTHandler:
    """- treat the sub id passed in as the user id
    - return an error if id making request doesn't exist in proxy user table
    - track spend against the user id
    - if role="litellm_proxy_user" -> allow making calls + info. Can not edit budgets"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/handle_jwt.py Line: 499
class JWTAuthManager:
    "Manages JWT authentication and authorization operations"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 64
async def common_checks(request_body: dict, team_object: Optional[LiteLLM_TeamTable], user_object: Optional[LiteLLM_UserTable], end_user_object: Optional[LiteLLM_EndUserTable], global_proxy_spend: Optional[float], general_settings: dict, route: str, llm_router: Optional[Router], proxy_logging_obj: ProxyLogging, valid_token: Optional[UserAPIKeyAuth], request: Request) -> bool:
    """Common checks across jwt + key-based auth.

    1. If team is blocked
    2. If team can call model
    3. If team is in budget
    4. If user passed in (JWT or key.user_id) - is in budget
    5. If end_user (either via JWT or 'user' passed to /chat/completions, /embeddings endpoint) is in budget
    6. [OPTIONAL] If 'enforce_end_user' enabled - did developer pass in 'user' param for openai endpoints
    7. [OPTIONAL] If 'litellm.max_budget' is set (>0), is proxy under budget
    8. [OPTIONAL] If guardrails modified - is request allowed to change this
    9. Check if request body is safe
    10. [OPTIONAL] Organization checks - is user_object.organization_id is set, run these checks"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 216
def _is_ui_route(route: str, user_obj: Optional[LiteLLM_UserTable]) -> bool:
    "- Check if the route is a UI used route"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 241
def _get_user_role(user_obj: Optional[LiteLLM_UserTable]) -> Optional[LitellmUserRoles]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 258
def _is_api_route_allowed(route: str, request: Request, request_data: dict, valid_token: Optional[UserAPIKeyAuth], user_obj: Optional[LiteLLM_UserTable]) -> bool:
    "- Route b/w api token check and normal token check"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 290
def _is_user_proxy_admin(user_obj: Optional[LiteLLM_UserTable]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 309
def _is_allowed_route(route: str, token_type: Literal[(?, ?)], request: Request, request_data: dict, valid_token: Optional[UserAPIKeyAuth], user_obj: Optional[LiteLLM_UserTable]) -> bool:
    "- Route b/w ui token check and normal token check"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 333
def _allowed_routes_check(user_route: str, allowed_routes: list) -> bool:
    """Return if a user is allowed to access route. Helper function for `allowed_routes_check`.

    Parameters:
    - user_route: str - the route the user is trying to call
    - allowed_routes: List[str|LiteLLMRoutes] - the list of allowed routes for the user."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 353
def allowed_routes_check(user_role: Literal[(?, ?, ?)], user_route: str, litellm_proxy_roles: LiteLLM_JWTAuth) -> bool:
    "Check if user -> not admin - allowed to access these routes"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 391
def allowed_route_check_inside_route(user_api_key_dict: UserAPIKeyAuth, requested_user_id: Optional[str]) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 407
def get_actual_routes(allowed_routes: list) -> list:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 423
async def get_end_user_object(end_user_id: Optional[str], prisma_client: Optional[PrismaClient], user_api_key_cache: DualCache, parent_otel_span: Optional[Span], proxy_logging_obj: Optional[ProxyLogging]) -> Optional[LiteLLM_EndUserTable]:
    """Returns end user object, if in db.

    Do a isolated check for end user in table vs. doing a combined key + team + user + end-user check, as key might come in frequently for different end-users. Larger call will slowdown query time. This way we get to cache the constant (key/team/user info) and only update based on the changing value (end-user)."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 488
def model_in_access_group(model: str, team_models: Optional[List[str]], llm_router: Optional[Router]) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 518
def _should_check_db(key: str, last_db_access_time: LimitedSizeOrderedDict, db_cache_expiry: int) -> bool:
    "Prevent calling db repeatedly for items that don't exist in the db."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 538
def _update_last_db_access_time(key: str, value: Optional[Any], last_db_access_time: LimitedSizeOrderedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 544
def _get_role_based_permissions(rbac_role: RBAC_ROLES, general_settings: dict, key: Literal[(?, ?)]) -> Optional[List[str]]:
    "Get the role based permissions from the general settings."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 566
def get_role_based_models(rbac_role: RBAC_ROLES, general_settings: dict) -> Optional[List[str]]:
    """Get the models allowed for a user role.

    Used by JWT Auth."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 583
def get_role_based_routes(rbac_role: RBAC_ROLES, general_settings: dict) -> Optional[List[str]]:
    "Get the routes allowed for a user role."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 598
async def _get_fuzzy_user_object(prisma_client: PrismaClient, sso_user_id: Optional[str], user_email: Optional[str]) -> Optional[LiteLLM_UserTable]:
    """Checks if sso user is in db.

    Called when user id match is not found in db.

    - Check if sso_user_id is user_id in db
    - Check if sso_user_id is sso_user_id in db
    - Check if user_email is user_email in db
    - If not, create new user with user_email and sso_user_id and user_id = sso_user_id"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 638
async def get_user_object(user_id: Optional[str], prisma_client: Optional[PrismaClient], user_api_key_cache: DualCache, user_id_upsert: bool, parent_otel_span: Optional[Span], proxy_logging_obj: Optional[ProxyLogging], sso_user_id: Optional[str], user_email: Optional[str], check_db_only: Optional[bool]) -> Optional[LiteLLM_UserTable]:
    """- Check if user id in proxy User Table
    - if valid, return LiteLLM_UserTable object with defined limits
    - if not, then raise an error"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 733
async def _cache_management_object(key: str, value: BaseModel, user_api_key_cache: DualCache, proxy_logging_obj: Optional[ProxyLogging]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 742
async def _cache_team_object(team_id: str, team_table: LiteLLM_TeamTableCachedObj, user_api_key_cache: DualCache, proxy_logging_obj: Optional[ProxyLogging]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 761
async def _cache_key_object(hashed_token: str, user_api_key_obj: UserAPIKeyAuth, user_api_key_cache: DualCache, proxy_logging_obj: Optional[ProxyLogging]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 780
async def _delete_cache_key_object(hashed_token: str, user_api_key_cache: DualCache, proxy_logging_obj: Optional[ProxyLogging]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 797
async def _get_team_db_check(team_id: str, prisma_client: PrismaClient, team_id_upsert: Optional[bool]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 812
async def _get_team_object_from_db(team_id: str, prisma_client: PrismaClient):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 818
async def _get_team_object_from_user_api_key_cache(team_id: str, prisma_client: PrismaClient, user_api_key_cache: DualCache, last_db_access_time: LimitedSizeOrderedDict, db_cache_expiry: int, proxy_logging_obj: Optional[ProxyLogging], key: str, team_id_upsert: Optional[bool]) -> LiteLLM_TeamTableCachedObj:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 864
async def _get_team_object_from_cache(key: str, proxy_logging_obj: Optional[ProxyLogging], user_api_key_cache: DualCache, parent_otel_span: Optional[Span]) -> Optional[LiteLLM_TeamTableCachedObj]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 895
async def get_team_object(team_id: str, prisma_client: Optional[PrismaClient], user_api_key_cache: DualCache, parent_otel_span: Optional[Span], proxy_logging_obj: Optional[ProxyLogging], check_cache_only: Optional[bool], check_db_only: Optional[bool], team_id_upsert: Optional[bool]) -> LiteLLM_TeamTableCachedObj:
    """- Check if team id in proxy Team Table
    - if valid, return LiteLLM_TeamTable object with defined limits
    - if not, then raise an error

    Raises:
        - Exception: If team doesn't exist in db or cache"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 956
async def get_key_object(hashed_token: str, prisma_client: Optional[PrismaClient], user_api_key_cache: DualCache, parent_otel_span: Optional[Span], proxy_logging_obj: Optional[ProxyLogging], check_cache_only: Optional[bool]) -> UserAPIKeyAuth:
    """- Check if team id in proxy Team Table
    - if valid, return LiteLLM_TeamTable object with defined limits
    - if not, then raise an error"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1024
async def get_org_object(org_id: str, prisma_client: Optional[PrismaClient], user_api_key_cache: DualCache, parent_otel_span: Optional[Span], proxy_logging_obj: Optional[ProxyLogging]) -> Optional[LiteLLM_OrganizationTable]:
    """- Check if org id in proxy Org Table
    - if valid, return LiteLLM_OrganizationTable object
    - if not, then raise an error"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1064
async def _can_object_call_model(model: str, llm_router: Optional[Router], models: List[str], team_model_aliases: Optional[Dict[(str, str)]], object_type: Literal[(?, ?, ?)]="user") -> Literal[?]:
    """Checks if token can call a given model

    Args:
        - model: str
        - llm_router: Optional[Router]
        - models: List[str]
        - team_model_aliases: Optional[Dict[str, str]]
        - object_type: Literal["user", "team", "key"]. We use the object type to raise the correct exception type

    Returns:
        - True: if token allowed to call model

    Raises:
        - Exception: If token not allowed to call model"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1143
def _model_in_team_aliases(model: str, team_model_aliases: Optional[Dict[(str, str)]]) -> bool:
    """Returns True if `model` being accessed is an alias of a team model

    - `model=gpt-4o`
    - `team_model_aliases={"gpt-4o": "gpt-4o-team-1"}`
        - returns True

    - `model=gp-4o`
    - `team_model_aliases={"o-3": "o3-preview"}`
        - returns False"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1163
async def can_key_call_model(model: str, llm_model_list: Optional[list], valid_token: UserAPIKeyAuth, llm_router: Optional[?]) -> Literal[?]:
    """Checks if token can call a given model

    Returns:
        - True: if token allowed to call model

    Raises:
        - Exception: If token not allowed to call model"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1187
async def can_team_access_model(model: str, team_object: Optional[LiteLLM_TeamTable], llm_router: Optional[Router], team_model_aliases: Optional[Dict[(str, str)]]) -> Literal[?]:
    "Returns True if the team can access a specific model."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1206
async def can_user_call_model(model: str, llm_router: Optional[Router], user_object: Optional[LiteLLM_UserTable]) -> Literal[?]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1230
async def is_valid_fallback_model(model: str, llm_router: Optional[Router], user_model: Optional[str]) -> Literal[?]:
    """Try to route the fallback model request.

    Validate if it can't be routed.

    Help catch invalid fallback models."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1255
async def _virtual_key_max_budget_check(valid_token: UserAPIKeyAuth, proxy_logging_obj: ProxyLogging, user_obj: Optional[LiteLLM_UserTable]):
    """Raises:
        BudgetExceededError if the token is over it's max budget.
        Triggers a budget alert if the token is over it's max budget."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1303
async def _virtual_key_soft_budget_check(valid_token: UserAPIKeyAuth, proxy_logging_obj: ProxyLogging):
    "Triggers a budget alert if the token is over it's soft budget."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1338
async def _team_max_budget_check(team_object: Optional[LiteLLM_TeamTable], valid_token: Optional[UserAPIKeyAuth], proxy_logging_obj: ProxyLogging):
    """Check if the team is over it's max budget.

    Raises:
        BudgetExceededError if the team is over it's max budget.
        Triggers a budget alert if the team is over it's max budget."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1379
def is_model_allowed_by_pattern(model: str, allowed_model_pattern: str) -> bool:
    """Check if a model matches an allowed pattern.
    Handles exact matches and wildcard patterns.

    Args:
        model (str): The model to check (e.g., "bedrock/anthropic.claude-3-5-sonnet-20240620")
        allowed_model_pattern (str): The allowed pattern (e.g., "bedrock/*", "*", "openai/*")

    Returns:
        bool: True if model matches the pattern, False otherwise"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1398
def _model_matches_any_wildcard_pattern_in_list(model: str, allowed_model_list: list) -> bool:
    """Returns True if a model matches any wildcard pattern in a list.

    eg.
    - model=`bedrock/us.amazon.nova-micro-v1:0`, allowed_models=`bedrock/*` returns True
    - model=`bedrock/us.amazon.nova-micro-v1:0`, allowed_models=`bedrock/us.*` returns True
    - model=`bedrockzzzz/us.amazon.nova-micro-v1:0`, allowed_models=`bedrock/*` returns False"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1431
def _model_custom_llm_provider_matches_wildcard_pattern(model: str, allowed_model_pattern: str) -> bool:
    """Returns True for this scenario:
    - `model=gpt-4o`
    - `allowed_model_pattern=openai/*`

    or
    - `model=claude-3-5-sonnet-20240620`
    - `allowed_model_pattern=anthropic/*`"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_checks.py Line: 1454
def _is_wildcard_pattern(allowed_model_pattern: str) -> bool:
    """Returns True if the pattern is a wildcard pattern.

    Checks if `*` is in the pattern."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/rds_iam_token.py Line: 7
def init_rds_client(aws_access_key_id: Optional[str], aws_secret_access_key: Optional[str], aws_region_name: Optional[str], aws_session_name: Optional[str], aws_profile_name: Optional[str], aws_role_name: Optional[str], aws_web_identity_token: Optional[str], timeout: Optional[Union[(float, ?)]]):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/rds_iam_token.py Line: 162
def generate_iam_auth_token(db_host, db_port, db_user, client: Optional[Any]) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/model_checks.py Line: 13
def _check_wildcard_routing(model: str) -> bool:
    """Returns True if a model is a provider wildcard.

    eg:
    - anthropic/*
    - openai/*
    - *"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/model_checks.py Line: 27
def get_provider_models(provider: str, litellm_params: Optional[LiteLLM_Params]) -> Optional[List[str]]:
    "Returns the list of known models by provider"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/model_checks.py Line: 48
def _get_models_from_access_groups(model_access_groups: Dict[(str, List[str])], all_models: List[str]) -> List[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/model_checks.py Line: 66
def get_key_models(user_api_key_dict: UserAPIKeyAuth, proxy_model_list: List[str], model_access_groups: Dict[(str, List[str])]) -> List[str]:
    """Returns:
    - List of model name strings
    - Empty list if no models set
    - If model_access_groups is provided, only return models that are in the access groups"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/model_checks.py Line: 93
def get_team_models(team_models: List[str], proxy_model_list: List[str], model_access_groups: Dict[(str, List[str])]) -> List[str]:
    """Returns:
    - List of model name strings
    - Empty list if no models set
    - If model_access_groups is provided, only return models that are in the access groups"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/model_checks.py Line: 120
def get_complete_model_list(key_models: List[str], team_models: List[str], proxy_model_list: List[str], user_model: Optional[str], infer_model_from_keys: Optional[bool], return_wildcard_routes: Optional[bool], llm_router: Optional[Router]) -> List[str]:
    "Logic for returning complete model list for a given key + team pair"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/model_checks.py Line: 161
def get_known_models_from_wildcard(wildcard_model: str, litellm_params: Optional[LiteLLM_Params]) -> List[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/model_checks.py Line: 187
def _get_wildcard_models(unique_models: Set[str], return_wildcard_routes: Optional[bool], llm_router: Optional[Router]) -> List[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/litellm_license.py Line: 16
class LicenseCheck:
    """- Check if license in env
    - Returns if license is valid"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 86
def _get_bearer_token(api_key: str):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 100
async def user_api_key_auth_websocket(websocket: WebSocket):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 146
def update_valid_token_with_end_user_params(valid_token: UserAPIKeyAuth, end_user_params: dict) -> UserAPIKeyAuth:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 156
async def get_global_proxy_spend(litellm_proxy_admin_name: str, user_api_key_cache: DualCache, prisma_client: Optional[PrismaClient], token: str, proxy_logging_obj: ProxyLogging) -> Optional[float]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 199
def get_rbac_role(jwt_handler: JWTHandler, scopes: List[str]) -> str:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 207
def get_model_from_request(request_data: dict, route: str) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 221
async def _user_api_key_auth_builder(request: Request, api_key: str, azure_api_key_header: str, anthropic_api_key_header: Optional[str], google_ai_studio_api_key_header: Optional[str], azure_apim_header: Optional[str], request_data: dict) -> UserAPIKeyAuth:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 1009
async def user_api_key_auth(request: Request, api_key: str=..., azure_api_key_header: str=..., anthropic_api_key_header: Optional[str]=..., google_ai_studio_api_key_header: Optional[str]=..., azure_apim_header: Optional[str]=...) -> UserAPIKeyAuth:
    "Parent function to authenticate user api key / jwt token."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 1047
async def _return_user_api_key_auth_obj(user_obj: Optional[LiteLLM_UserTable], api_key: str, parent_otel_span: Optional[Span], valid_token_dict: dict, route: str, start_time: datetime, user_role: Optional[LitellmUserRoles]) -> UserAPIKeyAuth:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 1094
def get_api_key_from_custom_header(request: Request, custom_litellm_key_header_name: str) -> str:
    """Get API key from custom header

    Args:
        request (Request): Request object
        custom_litellm_key_header_name (str): Custom header name

    Returns:
        Optional[str]: API key"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 1131
def _get_temp_budget_increase(valid_token: UserAPIKeyAuth):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/user_api_key_auth.py Line: 1143
def _update_key_budget_with_temp_budget_increase(valid_token: UserAPIKeyAuth) -> UserAPIKeyAuth:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 14
def _get_request_ip_address(request: Request, use_x_forwarded_for: Optional[bool]) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 28
def _check_valid_ip(allowed_ips: Optional[List[str]], request: Request, use_x_forwarded_for: Optional[bool]) -> Tuple[(bool, Optional[str])]:
    "Returns if ip is allowed or not"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 51
def check_complete_credentials(request_body: dict) -> bool:
    "if 'api_base' in request body. Check if complete credentials given. Prevent malicious attacks."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 76
def check_regex_or_str_match(request_body_value: Any, regex_str: str) -> bool:
    "Check if request_body_value matches the regex_str or is equal to param"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 85
def _is_param_allowed(param: str, request_body_value: Any, configurable_clientside_auth_params: CONFIGURABLE_CLIENTSIDE_AUTH_PARAMS) -> bool:
    "Check if param is a str or dict and if request_body_value is in the list of allowed values"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 109
def _allow_model_level_clientside_configurable_parameters(model: str, param: str, request_body_value: Any, llm_router: Optional[Router]) -> bool:
    """Check if model is allowed to use configurable client-side params
    - get matching model
    - check if 'clientside_configurable_parameters' is set for model
    -"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 142
def is_request_body_safe(request_body: dict, general_settings: dict, llm_router: Optional[Router], model: str) -> bool:
    """Check if the request body is safe.

    A malicious user can set the api_base to their own domain and invoke POST /chat/completions to intercept and steal the OpenAI API key.
    Relevant issue: https://huntr.com/bounties/4001e1a2-7b7a-4776-a3ae-e6692ec3d997"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 181
async def pre_db_read_auth_checks(request: Request, request_data: dict, route: str):
    """1. Checks if request size is under max_request_size_mb (if set)
    2. Check if request body is safe (example user has not set api_base in request body)
    3. Check if IP address is allowed (if set)
    4. Check if request route is an allowed route on the proxy (if set)

    Returns:
    - True

    Raises:
    - HTTPException if request fails initial auth checks"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 243
def route_in_additonal_public_routes(current_route: str):
    """Helper to check if the user defined public_routes on config.yaml

    Parameters:
    - current_route: str - the route the user is trying to call

    Returns:
    - bool - True if the route is defined in public_routes
    - bool - False if the route is not defined in public_routes


    In order to use this the litellm config.yaml should have the following in general_settings:

    ```yaml
    general_settings:
        master_key: sk-1234
        public_routes: ["LiteLLMRoutes.public_routes", "/spend/calculate"]
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 284
def get_request_route(request: Request) -> str:
    """Helper to get the route from the request

    remove base url from path if set e.g. `/genai/chat/completions` -> `/chat/completions"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 305
async def check_if_request_size_is_safe(request: Request) -> bool:
    """Enterprise Only:
        - Checks if the request size is within the limit

    Args:
        request (Request): The incoming request.

    Returns:
        bool: True if the request size is within the limit

    Raises:
        ProxyException: If the request size is too large"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 369
async def check_response_size_is_safe(response: Any) -> bool:
    """Enterprise Only:
        - Checks if the response size is within the limit

    Args:
        response (Any): The response to check.

    Returns:
        bool: True if the response size is within the limit

    Raises:
        ProxyException: If the response size is too large"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 409
def bytes_to_mb(bytes_value: int):
    "Helper to convert bytes to MB"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 417
def get_key_model_rpm_limit(user_api_key_dict: UserAPIKeyAuth) -> Optional[Dict[(str, int)]]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 433
def get_key_model_tpm_limit(user_api_key_dict: UserAPIKeyAuth) -> Optional[Dict[(str, int)]]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 446
def is_pass_through_provider_route(route: str) -> bool:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 459
def should_run_auth_on_pass_through_provider_route(route: str) -> bool:
    """Use this to decide if the rest of the LiteLLM Virtual Key auth checks should run on /vertex-ai/{endpoint} routes
    Use this to decide if the rest of the LiteLLM Virtual Key auth checks should run on provider pass through routes
    ex /vertex-ai/{endpoint} routes
    Run virtual key auth if the following is try:
    - User is premium_user
    - User has enabled litellm_setting.use_client_credentials_pass_through_routes"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 484
def _has_user_setup_sso():
    """Check if the user has set up single sign-on (SSO) by verifying the presence of Microsoft client ID, Google client ID or generic client ID and UI username environment variables.
    Returns a boolean indicating whether SSO has been set up."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/auth_utils.py Line: 502
def get_end_user_id_from_request_body(request_body: dict) -> Optional[str]:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/auth/oauth2_proxy_hook.py Line: 9
async def handle_oauth2_proxy_request(request: Request) -> UserAPIKeyAuth:
    "Handle request from oauth2 proxy."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/example_config_yaml/custom_callbacks.py Line: 19
def print_verbose(print_statement):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/example_config_yaml/custom_callbacks.py Line: 24
class MyCustomHandler(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/example_config_yaml/custom_callbacks1.py Line: 10
class MyCustomHandler(CustomLogger):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/example_config_yaml/custom_auth_basic.py Line: 6
async def user_api_key_auth(request: Request, api_key: str) -> UserAPIKeyAuth:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/example_config_yaml/custom_handler.py Line: 10
class MyCustomLLM(CustomLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/example_config_yaml/custom_auth.py Line: 8
async def user_api_key_auth(request: Request, api_key: str) -> UserAPIKeyAuth:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/example_config_yaml/custom_auth.py Line: 18
async def generate_key_fn(data: GenerateKeyRequest):
    """Asynchronously decides if a key should be generated or not based on the provided data.

    Args:
        data (GenerateKeyRequest): The data to be used for decision making.

    Returns:
        bool: True if a key should be generated, False otherwise."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/example_config_yaml/custom_guardrail.py Line: 11
class myCustomGuardrail(CustomGuardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 15
class IPAddress(BaseModel):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 25
async def get_allowed_ips():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 37
async def add_allowed_ip(ip_address: IPAddress):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 88
async def delete_allowed_ip(ip_address: IPAddress):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 115
async def _get_settings_with_schema(settings_key: str, settings_class: Any, config: dict) -> dict:
    """Common utility function to get settings with schema information.

    Args:
        settings_key: The key in litellm_settings to get
        settings_class: The Pydantic class to use for schema
        config: The config dictionary"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 172
async def get_sso_settings():
    """Get all SSO settings from the litellm_settings configuration.
    Returns a structured object with values and descriptions for UI display."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 194
async def get_default_team_settings():
    """Get all SSO settings from the litellm_settings configuration.
    Returns a structured object with values and descriptions for UI display."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 211
async def _update_litellm_setting(settings: Union[(DefaultInternalUserParams, DefaultTeamSSOParams)], settings_key: str, in_memory_var: Any, success_message: str):
    """Common utility function to update `litellm_settings` in both memory and config.

    Args:
        settings: The settings object to update
        settings_key: The key in litellm_settings to update
        in_memory_var: The in-memory variable to update
        success_message: Message to return on success"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 255
async def update_internal_user_settings(settings: DefaultInternalUserParams):
    """Update the default internal user parameters for SSO users.
    These settings will be applied to new users who sign in via SSO."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/ui_crud_endpoints/proxy_setting_endpoints.py Line: 273
async def update_default_team_settings(settings: DefaultTeamSSOParams):
    """Update the default team parameters for SSO users.
    These settings will be applied to new teams created from SSO."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/rerank_endpoints/endpoints.py Line: 34
async def rerank(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/credential_endpoints/endpoints.py Line: 22
class CredentialHelperUtils:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/credential_endpoints/endpoints.py Line: 38
async def create_credential(request: Request, fastapi_response: Response, credential: CreateCredentialItem, user_api_key_dict: UserAPIKeyAuth=...):
    """[BETA] endpoint. This might change unexpectedly.
    Stores credential in DB.
    Reloads credentials in memory."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/credential_endpoints/endpoints.py Line: 111
async def get_credentials(request: Request, fastapi_response: Response, user_api_key_dict: UserAPIKeyAuth=...):
    "[BETA] endpoint. This might change unexpectedly."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/credential_endpoints/endpoints.py Line: 145
async def get_credential(request: Request, fastapi_response: Response, credential_name: Optional[str], model_id: Optional[str], user_api_key_dict: UserAPIKeyAuth=...):
    "[BETA] endpoint. This might change unexpectedly."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/credential_endpoints/endpoints.py Line: 210
async def delete_credential(request: Request, fastapi_response: Response, credential_name: str, user_api_key_dict: UserAPIKeyAuth=...):
    "[BETA] endpoint. This might change unexpectedly."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/credential_endpoints/endpoints.py Line: 242
def update_db_credential(db_credential: CredentialItem, updated_patch: CredentialItem) -> CredentialItem:
    "Update a credential in the DB."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/credential_endpoints/endpoints.py Line: 285
async def update_credential(request: Request, fastapi_response: Response, credential_name: str, credential: CredentialItem, user_api_key_dict: UserAPIKeyAuth=...):
    "[BETA] endpoint. This might change unexpectedly."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 41
async def test_endpoint(request: Request):
    """[DEPRECATED] use `/health/liveliness` instead.

    A test endpoint that pings the proxy server to check if it's healthy.

    Parameters:
        request (Request): The incoming request.

    Returns:
        dict: A dictionary containing the route of the request URL."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 62
async def health_services_endpoint(user_api_key_dict: UserAPIKeyAuth=..., service: Union[(Literal[(?, ?, ?, ?, ?, ?, ?, ?)], str)]=...):
    """Use this admin-only endpoint to check if the service is healthy.

    Example:
    ```
    curl -L -X GET 'http://0.0.0.0:4000/health/services?service=datadog'     -H 'Authorization: Bearer sk-1234'
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 300
async def health_endpoint(user_api_key_dict: UserAPIKeyAuth=..., model: Optional[str]=...):
    """ USE `/health/liveliness` to health check the proxy 

    See more  https://docs.litellm.ai/docs/proxy/health


    Check the health of all the endpoints in config.yaml

    To run health checks in the background, add this to config.yaml:
    ```
    general_settings:
        # ... other settings
        background_health_checks: True
    ```
    else, the health checks will be run on models when /health is called."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 379
async def _db_health_readiness_check():

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 413
async def active_callbacks():
    """Returns a list of litellm level settings

    This is useful for debugging and ensuring the proxy server is configured correctly.

    Response schema:
    ```
    {
        "alerting": _alerting,
        "litellm.callbacks": litellm_callbacks,
        "litellm.input_callback": litellm_input_callbacks,
        "litellm.failure_callback": litellm_failure_callbacks,
        "litellm.success_callback": litellm_success_callbacks,
        "litellm._async_success_callback": litellm_async_success_callbacks,
        "litellm._async_failure_callback": litellm_async_failure_callbacks,
        "litellm._async_input_callback": litellm_async_input_callbacks,
        "all_litellm_callbacks": all_litellm_callbacks,
        "num_callbacks": len(all_litellm_callbacks),
        "num_alerting": _num_alerting,
        "litellm.request_timeout": litellm.request_timeout,
    }
    ```"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 482
def callback_name(callback):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 500
async def health_readiness():
    "Unprotected endpoint for checking if worker can receive requests"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 569
async def health_liveliness():
    "Unprotected endpoint for checking if worker is alive"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 581
async def health_readiness_options():
    "Options endpoint for health/readiness check."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 603
async def health_liveliness_options():
    "Options endpoint for health/liveliness check."

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/health_endpoints/_health_endpoints.py Line: 620
async def test_model_connection(request: Request, mode: Optional[Literal[(?, ?, ?, ?, ?, ?, ?, ?, ?)]]=..., litellm_params: Dict=..., user_api_key_dict: UserAPIKeyAuth=...):
    """Test a direct connection to a specific model.

    This endpoint allows you to verify if your proxy can successfully connect to a specific model.
    It's useful for troubleshooting model connectivity issues without going through the full proxy routing.

    Example:
    ```bash
    curl -X POST 'http://localhost:4000/health/test_connection' \
      -H 'Authorization: Bearer sk-1234' \
      -H 'Content-Type: application/json' \
      -d '{
        "litellm_params": {
            "model": "gpt-4",
            "custom_llm_provider": "azure_ai",
            "litellm_credential_name": null,
            "api_key": "6xxxxxxx",
            "api_base": "https://litellm8397336933.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-10-21",
        },
        "mode": "chat"
      }'
    ```

    Returns:
        dict: A dictionary containing the health check result with either success information or error details."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_experimental/mcp_server/tool_registry.py Line: 9
class MCPToolRegistry:
    "A registry for managing MCP tools"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_experimental/mcp_server/sse_transport.py Line: 25
class SseServerTransport:
    """SSE server transport for MCP. This class provides _two_ ASGI applications,
    suitable to be used with a framework like Starlette and a server like Hypercorn:

        1. connect_sse() is an ASGI application which receives incoming GET requests,
           and sets up a new SSE stream to send server messages to the client.
        2. handle_post_message() is an ASGI application which receives incoming POST
           requests, which should contain client messages that link to a
           previously-established SSE session."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/_experimental/mcp_server/mcp_server_manager.py Line: 21
class MCPServerManager:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/presidio.py Line: 37
class PresidioPerRequestConfig(BaseModel):
    "presdio params that can be controlled per request, api key"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/presidio.py Line: 45
class _OPTIONAL_PresidioPIIMasking(CustomGuardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/guardrails_ai.py Line: 29
class GuardrailsAIResponse(TypedDict):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/guardrails_ai.py Line: 36
class GuardrailsAI(CustomGuardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/aim.py Line: 35
class AimGuardrailMissingSecrets(Exception):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/aim.py Line: 39
class AimGuardrail(CustomGuardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/lakera_ai.py Line: 50
class lakeraAI_Moderation(CustomGuardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/aporia_ai.py Line: 42
class AporiaGuardrail(CustomGuardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/bedrock_guardrails.py Line: 48
class BedrockGuardrail(CustomGuardrail, BaseAWSLLM):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/guardrails/guardrail_hooks/custom_guardrail.py Line: 13
class myCustomGuardrail(CustomGuardrail):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_provider_handlers/vertex_passthrough_logging_handler.py Line: 31
class VertexPassthroughLoggingHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_provider_handlers/assembly_passthrough_logging_handler.py Line: 26
class AssemblyAITranscriptResponse(TypedDict, total=...):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_provider_handlers/assembly_passthrough_logging_handler.py Line: 35
class AssemblyAIPassthroughLoggingHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_provider_handlers/base_passthrough_logging_handler.py Line: 31
class BasePassthroughLoggingHandler(ABC):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_provider_handlers/cohere_passthrough_logging_handler.py Line: 16
class CoherePassthroughLoggingHandler(BasePassthroughLoggingHandler):

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/pass_through_endpoints/llm_provider_handlers/anthropic_passthrough_logging_handler.py Line: 29
class AnthropicPassthroughLoggingHandler:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/db_transaction_queue/daily_spend_update_queue.py Line: 14
class DailySpendUpdateQueue(BaseUpdateQueue):
    """In memory buffer for daily spend updates that should be committed to the database

    To add a new daily spend update transaction, use the following format:
        daily_spend_update_queue.add_update({
            "user1_date_api_key_model_custom_llm_provider": {
                "spend": 10,
                "prompt_tokens": 100,
                "completion_tokens": 100,
            }
        })

    Queue contains a list of daily spend update transactions

    eg
        queue = [
            {
                "user1_date_api_key_model_custom_llm_provider": {
                    "spend": 10,
                    "prompt_tokens": 100,
                    "completion_tokens": 100,
                    "api_requests": 100,
                    "successful_requests": 100,
                    "failed_requests": 100,
                }
            },
            {
                "user2_date_api_key_model_custom_llm_provider": {
                    "spend": 10,
                    "prompt_tokens": 100,
                    "completion_tokens": 100,
                    "api_requests": 100,
                    "successful_requests": 100,
                    "failed_requests": 100,
                }
            }
        ]"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/db_transaction_queue/pod_lock_manager.py Line: 17
class PodLockManager:
    """Manager for acquiring and releasing locks for cron jobs using Redis.

    Ensures that only one pod can run a cron job at a time."""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/db_transaction_queue/base_update_queue.py Line: 16
class BaseUpdateQueue:
    "Base class for in memory buffer for database transactions"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/db_transaction_queue/spend_update_queue.py Line: 17
class SpendUpdateQueue(BaseUpdateQueue):
    "In memory buffer for spend updates that should be committed to the database"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/db/db_transaction_queue/redis_update_buffer.py Line: 40
class RedisUpdateBuffer:
    """Handles buffering database `UPDATE` transactions in Redis before committing them to the database

    This is to prevent deadlocks and improve reliability"""

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_transformations.py Line: 12
class ScimTransformations:

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 44
async def set_scim_content_type(response: Response):
    "Sets the Content-Type header to application/scim+json"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 58
async def get_users(startIndex: int=..., count: int=..., filter: Optional[str]=...):
    "Get a list of users according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 125
async def get_user(user_id: str=...):
    "Get a single user by ID according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 164
async def create_user(user: SCIMUser=...):
    "Create a user according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 231
async def update_user(user_id: str=..., user: SCIMUser=...):
    "Update a user according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 257
async def delete_user(user_id: str=...):
    "Delete a user according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 317
async def patch_user(user_id: str=..., patch_ops: SCIMPatchOp=...):
    "Patch a user according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 359
async def get_groups(startIndex: int=..., count: int=..., filter: Optional[str]=...):
    "Get a list of groups according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 445
async def get_group(group_id: str=...):
    "Get a single group by ID according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 486
async def create_group(group: SCIMGroup=...):
    "Create a group according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 552
async def update_group(group_id: str=..., group: SCIMGroup=...):
    "Update a group according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 671
async def delete_group(group_id: str=...):
    "Delete a group according to SCIM v2 protocol"

# File: /home/andrew/.cache/uv/archive-v0/02S9efzIWI-r4cBj04gGC/lib/python3.12/site-packages/litellm/proxy/management_endpoints/scim/scim_v2.py Line: 726
async def patch_group(group_id: str=..., patch_ops: SCIMPatchOp=...):
    "Patch a group according to SCIM v2 protocol"

