"""More lingering changes - Refactored to rename columns

Revision ID: 3a3d3a70f182
Revises: 649d1365dccc # Make sure this is the correct previous revision ID
Create Date: 2025-05-05 23:31:00.475544+10:00

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql, sqlite
from sqlalchemy import Text # Required for JSONB variant

# revision identifiers, used by Alembic.
revision: str = '3a3d3a70f182'
down_revision: Union[str, None] = 'd75c672e4c87' # Assuming this reverts d75c672e4c87
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema. Reverts message_history changes from d75c672e4c87 using renames."""
    # ### commands auto generated by Alembic - please adjust! ###

    # Unrelated index operation from the original migration - kept as is
    # Note: This condition might need adjustment depending on the actual database in use.
    # Consider if this index creation belongs in this migration file.
    bind = op.get_bind()
    if bind.dialect.name == 'postgresql': # Adjusted condition based on typical usage
        op.create_index('idx_doc_embeddings_gemini_1536_hnsw_cos', 'document_embeddings', [sa.literal_column('(embedding::vector(1536)) vector_cosine_ops')], unique=False, postgresql_using='hnsw', postgresql_where=sa.text("embedding_model = 'gemini-exp-03-07'"), postgresql_with={'m': 16, 'ef_construction': 64})
    elif bind.dialect.name == 'mysql':
         # Example placeholder if MySQL support was intended - original file had 'mysql' check here
         print("Warning: Index idx_doc_embeddings_gemini_1536_hnsw_cos might need specific syntax for MySQL.")
         # op.create_index(...) # Add MySQL specific index creation if needed


    # --- Message History Reversion using Batch Mode ---
    with op.batch_alter_table('message_history', schema=None) as batch_op:
        # 1. Drop FKs or constraints referencing the PK if any exist (none apparent here)

        # 2. Drop the primary key constraint based on internal_id
        # Need to know the constraint name, assuming default 'message_history_pkey' used in d75c672e4c87
        try:
            batch_op.drop_constraint('message_history_pkey', type_='primary')
        except Exception as e:
            print(f"Info: Could not drop PK constraint 'message_history_pkey', it might not exist or have a different name. Error: {e}")
             # Attempt to find PK name dynamically if needed, or handle absence gracefully.

        # 3. Drop indexes associated with columns being renamed or dropped
        batch_op.drop_index(op.f('ix_message_history_conversation_id'))
        batch_op.drop_index(op.f('ix_message_history_interface_message_id'))
        batch_op.drop_index(op.f('ix_message_history_interface_type'))
        batch_op.drop_index(op.f('ix_message_history_thread_root_id'))
        # Check if 'ix_message_history_tool_call_id' exists before dropping (added in c6dae031c51f)
        try:
             batch_op.drop_index(op.f('ix_message_history_tool_call_id'))
        except Exception as e:
             print(f"Info: Index 'ix_message_history_tool_call_id' not found or could not be dropped. Error: {e}")
        batch_op.drop_index(op.f('ix_message_history_turn_id'))

        # 4. Rename columns and adjust types
        # Rename conversation_id -> chat_id (String -> BigInteger)
        batch_op.alter_column('conversation_id', new_column_name='chat_id',
                              existing_type=sa.String(length=255),
                              type_=sa.BigInteger(),
                              nullable=False,
                              postgresql_using='chat_id::bigint') # Explicit cast for PG

        # Rename interface_message_id -> message_id (String -> BigInteger)
        batch_op.alter_column('interface_message_id', new_column_name='message_id',
                              existing_type=sa.String(length=255),
                              type_=sa.BigInteger(),
                              nullable=True, # Keep nullable initially? Original was PK -> nullable
                              postgresql_using='message_id::bigint') # Explicit cast for PG


        # Rename tool_calls -> tool_calls_info (JSONB -> JSONB - types compatible)
        batch_op.alter_column('tool_calls', new_column_name='tool_calls_info',
                              existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=Text), 'postgresql'),
                              type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=Text), 'postgresql'), # Keep type
                              nullable=True)

        # Make message_id non-nullable before adding PK
        batch_op.alter_column('message_id', existing_type=sa.BigInteger(), nullable=False)


        # 5. Drop the columns introduced in d75c672e4c87 and c6dae031c51f
        batch_op.drop_column('internal_id')
        batch_op.drop_column('interface_type')
        batch_op.drop_column('turn_id')
        batch_op.drop_column('thread_root_id')
        batch_op.drop_column('tool_call_id') # Added in c6dae031c51f

        # 6. Recreate the old primary key constraint (chat_id, message_id)
        batch_op.create_primary_key('message_history_pkey', ['chat_id', 'message_id'])

    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema. Re-applies changes from d75c672e4c87 using renames."""
    # ### commands auto generated by Alembic - please adjust! ###

    # --- Revert Message History Changes using Batch Mode ---
    with op.batch_alter_table('message_history', schema=None) as batch_op:
        # 1. Drop the primary key constraint on (chat_id, message_id)
        try:
            batch_op.drop_constraint('message_history_pkey', type_='primary')
        except Exception as e:
            print(f"Info: Could not drop PK constraint 'message_history_pkey'. Error: {e}")

        # 2. Add columns back (nullable first)
        # Determine correct Integer type based on dialect for internal_id PK
        int_type = sa.BigInteger() if op.get_bind().dialect.name == 'postgresql' else sa.Integer()
        batch_op.add_column(sa.Column('internal_id', int_type, nullable=True)) # Nullable first
        batch_op.add_column(sa.Column('interface_type', sa.String(length=50), nullable=True))
        batch_op.add_column(sa.Column('turn_id', sa.String(length=36), nullable=True))
        batch_op.add_column(sa.Column('thread_root_id', sa.BigInteger(), nullable=True))
        batch_op.add_column(sa.Column('tool_call_id', sa.String(), nullable=True)) # Added in c6dae031c51f

        # 3. Rename columns back and adjust types
        # Rename chat_id -> conversation_id (BigInteger -> String)
        batch_op.alter_column('chat_id', new_column_name='conversation_id',
                              existing_type=sa.BigInteger(),
                              type_=sa.String(length=255),
                              nullable=False,
                              postgresql_using='conversation_id::varchar') # Explicit cast for PG

        # Rename message_id -> interface_message_id (BigInteger -> String)
        # Make nullable first as it wasn't part of the original PK
        batch_op.alter_column('message_id', existing_type=sa.BigInteger(), nullable=True)
        batch_op.alter_column('message_id', new_column_name='interface_message_id',
                              existing_type=sa.BigInteger(), # Still refers to old type before rename
                              type_=sa.String(length=255),
                              nullable=True, # Should be nullable
                              postgresql_using='interface_message_id::varchar') # Explicit cast for PG

        # Rename tool_calls_info -> tool_calls (JSON -> JSONB variant)
        batch_op.alter_column('tool_calls_info', new_column_name='tool_calls',
                              existing_type=sa.JSON().with_variant(sqlite.JSON(), 'sqlite'), # Adapt existing type if needed
                              type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=Text), 'postgresql'),
                              nullable=True)

    # 4. Populate new columns (outside batch mode) - Mimic logic from d75c672e4c87 upgrade
    bind = op.get_bind()
    if bind.dialect.name == 'postgresql':
        op.execute("""
            WITH numbered_rows AS (
                SELECT ctid, ROW_NUMBER() OVER (ORDER BY timestamp) AS rn
                FROM message_history
            )
            UPDATE message_history
            SET internal_id = numbered_rows.rn
            FROM numbered_rows
            WHERE message_history.ctid = numbered_rows.ctid AND message_history.internal_id IS NULL;
        """)
    elif bind.dialect.name == 'sqlite':
        # Ensure ROW_NUMBER() OVER() is supported or use alternative for older SQLite
        op.execute("""
             UPDATE message_history
             SET internal_id = (SELECT rn FROM (SELECT rowid, ROW_NUMBER() OVER (ORDER BY timestamp) as rn FROM message_history) AS sub WHERE sub.rowid = message_history.rowid)
             WHERE internal_id IS NULL;
         """)
        # Fallback for very old SQLite without window functions might be needed
        # print("Warning: SQLite internal_id population might need adjustment for older versions.")

    # Populate other fields based on the state before this migration's upgrade
    # Assuming default 'telegram' and copying IDs as done in d75c672e4c87 upgrade
    op.execute("UPDATE message_history SET interface_type = 'telegram' WHERE interface_type IS NULL")
    # conversation_id and interface_message_id were populated via rename + type cast
    # tool_calls was populated via rename
    # turn_id, thread_root_id, tool_call_id might need specific logic if data recovery is possible/needed.
    # Setting them to NULL is the default based on adding them as nullable.

    # 5. Final schema adjustments in batch mode
    with op.batch_alter_table('message_history', schema=None) as batch_op:
        # Make columns non-nullable where required by the state after d75c672e4c87
        batch_op.alter_column('internal_id', existing_type=int_type, nullable=False)
        batch_op.alter_column('interface_type', existing_type=sa.String(length=50), nullable=False)
        batch_op.alter_column('conversation_id', existing_type=sa.String(length=255), nullable=False)
        # interface_message_id remains nullable

        # 6. Create the primary key constraint on internal_id
        batch_op.create_primary_key('message_history_pkey', ['internal_id'])

        # 7. Recreate indexes dropped during upgrade
        batch_op.create_index(op.f('ix_message_history_conversation_id'), ['conversation_id'], unique=False)
        batch_op.create_index(op.f('ix_message_history_interface_message_id'), ['interface_message_id'], unique=False)
        batch_op.create_index(op.f('ix_message_history_interface_type'), ['interface_type'], unique=False)
        batch_op.create_index(op.f('ix_message_history_thread_root_id'), ['thread_root_id'], unique=False)
        batch_op.create_index(op.f('ix_message_history_tool_call_id'), ['tool_call_id'], unique=False) # Added in c6dae031c51f
        batch_op.create_index(op.f('ix_message_history_turn_id'), ['turn_id'], unique=False)


    # Unrelated index operation from the original migration - kept as is
    bind = op.get_bind()
    if bind.dialect.name == 'postgresql': # Adjusted condition
        op.drop_index('idx_doc_embeddings_gemini_1536_hnsw_cos', table_name='document_embeddings', postgresql_using='hnsw', postgresql_where=sa.text("embedding_model = 'gemini-exp-03-07'"))
    elif bind.dialect.name == 'mysql':
        # Example placeholder
        print("Warning: Index idx_doc_embeddings_gemini_1536_hnsw_cos might need specific syntax for dropping in MySQL.")
        # op.drop_index(...) # Add MySQL specific index drop if needed

    # ### end Alembic commands ###
