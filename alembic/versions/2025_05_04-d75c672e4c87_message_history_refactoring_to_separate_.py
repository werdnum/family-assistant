"""message history refactoring to separate ID space from telegram message ID, allow storing intermediate messages

Revision ID: d75c672e4c87
Revises: c6dae031c51f
Create Date: 2025-05-04 11:48:55.327902+10:00

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql, sqlite

# revision identifiers, used by Alembic.
# --- Custom Import ---
import pgvector.sqlalchemy # Required for VECTOR type handling in existing migration code
from sqlalchemy import Text # Required for JSONB variant

revision: str = 'd75c672e4c87'
down_revision: Union[str, None] = 'c6dae031c51f'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###

    # --- Message History Refactoring using Batch Mode for SQLite compatibility ---
    with op.batch_alter_table('message_history', schema=None) as batch_op:
        # 1. Add new columns (nullable first if they need data from old columns)
        batch_op.add_column(sa.Column('internal_id', sa.BigInteger(), nullable=True)) # Add as nullable first, will be PK later
        batch_op.add_column(sa.Column('interface_type', sa.String(length=50), nullable=True))
        batch_op.add_column(sa.Column('conversation_id', sa.String(length=255), nullable=True)) # Temp name handled by rename below
        batch_op.add_column(sa.Column('interface_message_id', sa.String(length=255), nullable=True)) # Temp name handled by rename below
        batch_op.add_column(sa.Column('turn_id', sa.String(length=36), nullable=True))
        batch_op.add_column(sa.Column('thread_root_id', sa.BigInteger(), nullable=True))
        batch_op.add_column(sa.Column('tool_calls', sa.JSON().with_variant(postgresql.JSONB(astext_type=Text), 'postgresql'), nullable=True)) # Temp name handled by rename below

    # 2. Populate new columns based on old ones (outside batch)
    #    Use op.execute for data migration logic. Casts might differ per DB.
    #    Populate internal_id - using ROW_NUMBER() for PG/SQLite or similar.
    #    This part is complex and backend-specific, especially for setting a PK sequence.
    #    A simpler approach for non-critical internal ID might be needed if this fails.
    #    For now, let's assume manual population or handle PK differently. We'll make it PK later.
    op.execute("UPDATE message_history SET interface_type = 'telegram' WHERE interface_type IS NULL") # Default existing to telegram
    op.execute("UPDATE message_history SET conversation_id = CAST(chat_id AS VARCHAR(255)) WHERE conversation_id IS NULL")
    op.execute("UPDATE message_history SET interface_message_id = CAST(message_id AS VARCHAR(255)) WHERE interface_message_id IS NULL")
    op.execute("UPDATE message_history SET tool_calls = tool_calls_info WHERE tool_calls IS NULL")

    # 3. Final schema adjustments in batch mode
    with op.batch_alter_table('message_history', schema=None) as batch_op:
        # Make columns non-nullable where required
        batch_op.alter_column('interface_type', existing_type=sa.String(length=50), nullable=False)
        batch_op.alter_column('conversation_id', existing_type=sa.String(length=255), nullable=False)

        # Drop old PK constraint (assuming name 'message_history_pkey' or based on columns)
        # This name might need adjustment based on actual DB state.
        # If dropping by name fails, try dropping by columns (less portable).
        try:
             batch_op.drop_constraint('message_history_pkey', type_='primary')
        except Exception:
             # Fallback or specific handling if constraint name is different/unknown
             # For SQLite, PK might be implicit and harder to drop directly.
             # This might require table recreation in downgrade/upgrade for SQLite.
             # For now, proceed assuming direct drop works or isn't needed for SQLite batch.
             print("Could not drop constraint 'message_history_pkey', it might not exist or name differs.")

        # Drop original columns
        batch_op.drop_column('tool_calls_info')
        batch_op.drop_column('message_id')
        batch_op.drop_column('chat_id')

        # Make internal_id the primary key (handle autoincrement if needed - dialect specific)
        # Making it non-nullable first
        batch_op.alter_column('internal_id', existing_type=sa.BigInteger(), nullable=False)
        # Create PK constraint (this might fail if internal_id wasn't populated uniquely)
        batch_op.create_primary_key('message_history_pkey', ['internal_id'])

        # Create indexes on new columns
        batch_op.create_index(op.f('ix_message_history_conversation_id'), ['conversation_id'], unique=False)
        batch_op.create_index(op.f('ix_message_history_interface_message_id'), ['interface_message_id'], unique=False)
        batch_op.create_index(op.f('ix_message_history_interface_type'), ['interface_type'], unique=False)
        batch_op.create_index(op.f('ix_message_history_thread_root_id'), ['thread_root_id'], unique=False)
        batch_op.create_index(op.f('ix_message_history_turn_id'), ['turn_id'], unique=False)

    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # Use batch mode for downgrade as well
    with op.batch_alter_table('message_history', schema=None) as batch_op:
        # Drop indexes
        batch_op.drop_index(op.f('ix_message_history_turn_id'))
        batch_op.drop_index(op.f('ix_message_history_thread_root_id'))
        batch_op.drop_index(op.f('ix_message_history_interface_type'))
        batch_op.drop_index(op.f('ix_message_history_interface_message_id'))
        batch_op.drop_index(op.f('ix_message_history_conversation_id'))

        # Drop primary key constraint on internal_id
        try:
            batch_op.drop_constraint('message_history_pkey', type_='primary')
        except Exception:
            print("Could not drop constraint 'message_history_pkey' during downgrade.")

        # Add old columns back (nullable first)
        batch_op.add_column(sa.Column('chat_id', sa.BIGINT(), nullable=True))
        batch_op.add_column(sa.Column('message_id', sa.BIGINT(), nullable=True))
        batch_op.add_column(sa.Column('tool_calls_info', sa.JSON().with_variant(sqlite.JSON(), 'sqlite'), nullable=True)) # Adjust type for sqlite if needed

    # Populate old columns from new ones (outside batch)
    op.execute("UPDATE message_history SET chat_id = CAST(conversation_id AS BIGINT) WHERE chat_id IS NULL") # Cast might fail if non-numeric data exists
    op.execute("UPDATE message_history SET message_id = CAST(interface_message_id AS BIGINT) WHERE message_id IS NULL") # Cast might fail
    op.execute("UPDATE message_history SET tool_calls_info = tool_calls WHERE tool_calls_info IS NULL")

    # Final adjustments in batch mode
    with op.batch_alter_table('message_history', schema=None) as batch_op:
        # Make old columns non-nullable
        batch_op.alter_column('chat_id', existing_type=sa.BIGINT(), nullable=False)
        batch_op.alter_column('message_id', existing_type=sa.BIGINT(), nullable=False)

        # Drop the new columns
        batch_op.drop_column('tool_calls')
        batch_op.drop_column('thread_root_id')
        batch_op.drop_column('turn_id')
        batch_op.drop_column('interface_message_id')
        batch_op.drop_column('conversation_id')
        batch_op.drop_column('interface_type')
        batch_op.drop_column('internal_id')

        # Recreate the old primary key constraint
        batch_op.create_primary_key('message_history_pkey', ['chat_id', 'message_id'])

    # ### end Alembic commands ###
