"""message history refactoring to separate ID space from telegram message ID, allow storing intermediate messages

Revision ID: d75c672e4c87
Revises: c6dae031c51f
Create Date: 2025-05-04 11:48:55.327902+10:00

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql, sqlite

# revision identifiers, used by Alembic.
# --- Custom Import ---
from sqlalchemy import Text  # Required for JSONB variant

revision: str = "d75c672e4c87"
down_revision: Union[str, None] = "c6dae031c51f"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###

    # --- Message History Refactoring using Batch Mode for SQLite compatibility ---
    with op.batch_alter_table("message_history", schema=None) as batch_op:
        # 1. Add new columns (nullable first if they need data from old columns)
        batch_op.add_column(
            sa.Column("internal_id", sa.BigInteger(), nullable=True)
        )  # Add as nullable first, will be PK later
        batch_op.add_column(
            sa.Column("interface_type", sa.String(length=50), nullable=True)
        )
        batch_op.add_column(
            sa.Column("conversation_id", sa.String(length=255), nullable=True)
        )  # Temp name handled by rename below
        batch_op.add_column(
            sa.Column("interface_message_id", sa.String(length=255), nullable=True)
        )  # Temp name handled by rename below
        batch_op.add_column(sa.Column("turn_id", sa.String(length=36), nullable=True))
        batch_op.add_column(sa.Column("thread_root_id", sa.BigInteger(), nullable=True))
        batch_op.add_column(
            sa.Column(
                "tool_calls",
                sa.JSON().with_variant(
                    postgresql.JSONB(astext_type=Text), "postgresql"
                ),
                nullable=True,
            )
        )  # Temp name handled by rename below

    # 2. Populate new columns based on old ones (outside batch mode)
    #    Use op.execute for data migration logic. Casts might differ per DB.
    #    Populate internal_id - using ROW_NUMBER() for PG/SQLite or similar.

    # Backfill internal_id sequentially based on timestamp
    bind = op.get_bind()
    if bind.dialect.name == "postgresql":
        op.execute("""
            WITH numbered_rows AS (
                SELECT ctid, ROW_NUMBER() OVER (ORDER BY timestamp) AS rn
                FROM message_history
            )
            UPDATE message_history
            SET internal_id = numbered_rows.rn
            FROM numbered_rows
            WHERE message_history.ctid = numbered_rows.ctid AND message_history.internal_id IS NULL;
        """)
    elif bind.dialect.name == "sqlite":
        op.execute("""
            UPDATE message_history
            SET internal_id = (SELECT rn FROM (SELECT rowid, ROW_NUMBER() OVER (ORDER BY timestamp) as rn FROM message_history) AS sub WHERE sub.rowid = message_history.rowid)
            WHERE internal_id IS NULL;
        """)
    # Add other dialect support if needed

    op.execute(
        "UPDATE message_history SET interface_type = 'telegram' WHERE interface_type IS NULL"
    )  # Default existing to telegram
    op.execute(
        "UPDATE message_history SET conversation_id = CAST(chat_id AS VARCHAR(255)) WHERE conversation_id IS NULL"
    )
    op.execute(
        "UPDATE message_history SET interface_message_id = CAST(message_id AS VARCHAR(255)) WHERE interface_message_id IS NULL"
    )
    op.execute(
        "UPDATE message_history SET tool_calls = tool_calls_info WHERE tool_calls IS NULL"
    )

    # 3. Final schema adjustments in batch mode
    with op.batch_alter_table("message_history", schema=None) as batch_op:
        # Make columns non-nullable where required
        batch_op.alter_column(
            "interface_type", existing_type=sa.String(length=50), nullable=False
        )
        batch_op.alter_column(
            "conversation_id", existing_type=sa.String(length=255), nullable=False
        )

        # Drop original columns
        batch_op.drop_column("tool_calls_info")
        batch_op.drop_column("message_id")
        batch_op.drop_column("chat_id")

        # Make internal_id the primary key (handle autoincrement if needed - dialect specific)
        # Making it non-nullable first
        batch_op.alter_column(
            "internal_id", existing_type=sa.BigInteger(), nullable=False
        )

        # Create PK constraint (this might fail if internal_id wasn't populated uniquely)
        # Note: Dropping the old implicit/explicit PK is handled implicitly by
        #       batch mode or by setting a new PK in some backends.
        batch_op.create_primary_key("message_history_pkey", ["internal_id"])

        # Create indexes on new columns
        batch_op.create_index(
            op.f("ix_message_history_conversation_id"),
            ["conversation_id"],
            unique=False,
        )
        batch_op.create_index(
            op.f("ix_message_history_interface_message_id"),
            ["interface_message_id"],
            unique=False,
        )
        batch_op.create_index(
            op.f("ix_message_history_interface_type"), ["interface_type"], unique=False
        )
        batch_op.create_index(
            op.f("ix_message_history_thread_root_id"), ["thread_root_id"], unique=False
        )
        batch_op.create_index(
            op.f("ix_message_history_turn_id"), ["turn_id"], unique=False
        )

    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # Use batch mode for downgrade as well
    with op.batch_alter_table("message_history", schema=None) as batch_op:
        # Drop indexes
        batch_op.drop_index(op.f("ix_message_history_turn_id"))
        batch_op.drop_index(op.f("ix_message_history_thread_root_id"))
        batch_op.drop_index(op.f("ix_message_history_interface_type"))
        batch_op.drop_index(op.f("ix_message_history_interface_message_id"))
        batch_op.drop_index(op.f("ix_message_history_conversation_id"))

        # Add old columns back (nullable first)
        batch_op.add_column(sa.Column("chat_id", sa.BIGINT(), nullable=True))
        batch_op.add_column(sa.Column("message_id", sa.BIGINT(), nullable=True))
        batch_op.add_column(
            sa.Column(
                "tool_calls_info",
                sa.JSON().with_variant(sqlite.JSON(), "sqlite"),
                nullable=True,
            )
        )  # Adjust type for sqlite if needed

    # Populate old columns from new ones (outside batch)
    op.execute(
        "UPDATE message_history SET chat_id = CAST(conversation_id AS BIGINT) WHERE chat_id IS NULL"
    )  # Cast might fail if non-numeric data exists
    op.execute(
        "UPDATE message_history SET message_id = CAST(interface_message_id AS BIGINT) WHERE message_id IS NULL"
    )  # Cast might fail
    op.execute(
        "UPDATE message_history SET tool_calls_info = tool_calls WHERE tool_calls_info IS NULL"
    )

    # Final adjustments in batch mode
    with op.batch_alter_table("message_history", schema=None) as batch_op:
        # Make old columns non-nullable
        batch_op.alter_column("chat_id", existing_type=sa.BIGINT(), nullable=False)
        batch_op.alter_column("message_id", existing_type=sa.BIGINT(), nullable=False)

        # Drop the new columns
        batch_op.drop_column("tool_calls")
        batch_op.drop_column("thread_root_id")
        batch_op.drop_column("turn_id")
        batch_op.drop_column("interface_message_id")
        batch_op.drop_column("conversation_id")
        batch_op.drop_column("interface_type")
        batch_op.drop_column("internal_id")

        # Recreate the old primary key constraint
        # Note: This assumes the old PK was ('chat_id', 'message_id')
        batch_op.create_primary_key("message_history_pkey", ["chat_id", "message_id"])

    # ### end Alembic commands ###
