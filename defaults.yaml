# Configuration for Family Assistant
# LLM Provider Configuration
# Each service profile can specify a 'provider' in its processing_config.
# Available providers: 'litellm' (default), 'openai', 'google'
# - 'litellm': Uses LiteLLM for all models (supports many providers)
# - 'openai': Uses direct OpenAI SDK (requires OPENAI_API_KEY)
# - 'google': Uses direct Google GenAI SDK (requires GEMINI_API_KEY)
#
# Retry and Fallback Configuration (NEW):
# Use 'retry_config' format for automatic retry and fallback:
# processing_config:
#   retry_config:
#     primary:
#       provider: "openai"
#       model: "gpt-4o"
#     fallback:
#       provider: "google"
#       model: "gemini-2.5-pro"
# The system will retry once on the primary model for transient errors,
# then fall back to the fallback model if needed.
# Event system configuration
event_system:
  enabled: true
  storage:
    sample_interval_hours: 1.0 # Store 1 event per entity per hour
    max_event_size: 100000 # 100KB max event size
    retention_hours: 48
  sources:
    home_assistant:
      enabled: true
      # URL and token are shared with context provider
# LLM provider-specific parameters
# Keys can be full model names or prefixes (ending with '-')
# Parameters defined here will be passed as keyword arguments to litellm.completion
# for matching models.
llm_parameters:
  # Example for Gemini models via OpenRouter
  "openrouter/google/gemini-":
    reasoning: # Nest OpenRouter-specific reasoning params here
      effort: medium
      # Other top-level parameters for this model could still go here
      # temperature: 0.8
  # Example for another model type
  # "anthropic/claude-3-":
  #   temperature: 0.75
  #   top_k: 50

# Gemini Live Voice API Configuration
# See: https://ai.google.dev/gemini-api/docs/live
gemini_live_config:
  # Model to use for Gemini Live voice sessions
  model: "gemini-2.5-flash-native-audio-preview-12-2025"

  # Voice configuration
  voice:
    # Prebuilt voice name (e.g., "Puck", "Kore", "Charon", "Fenrir", "Aoede")
    name: "Puck"

  # Session limits
  session:
    # Maximum duration in minutes before auto-disconnect
    max_duration_minutes: 15

  # Transcription settings
  transcription:
    # Enable input (user) transcription
    input_enabled: true
    # Enable output (assistant) transcription
    output_enabled: true

  # Voice Activity Detection (VAD) configuration
  # Controls automatic speech detection and turn-taking
  vad:
    # Whether automatic activity detection is enabled
    # Set to false to use manual/push-to-talk mode
    automatic: true
    # Sensitivity for detecting speech start: "DISABLED", "LOW", "DEFAULT", "HIGH"
    start_of_speech_sensitivity: "DEFAULT"
    # Sensitivity for detecting speech end: "DISABLED", "LOW", "DEFAULT", "HIGH"
    end_of_speech_sensitivity: "DEFAULT"
    # Buffer in ms before speech detection (null for default)
    prefix_padding_ms: null
    # Pause duration in ms to consider speech ended (null for default)
    silence_duration_ms: null

  # Advanced features (require v1alpha API)
  # Affective dialog allows the model to adapt response style to user's tone
  affective_dialog:
    enabled: false

  # Proactivity allows the model to proactively decline irrelevant responses
  proactivity:
    enabled: true
    proactive_audio: true

  # Thinking configuration for reasoning
  thinking:
    # Include thought summaries in responses (for debugging)
    include_thoughts: false

  # Telephone-specific overrides for Asterisk Live API
  # These override the base settings above for telephone calls
  # Telephone audio has different characteristics (narrowband, potential noise)
  telephone_overrides:
    # VAD settings optimized for telephone audio
    vad:
      # Higher sensitivity for speech start (telephone audio can be noisier)
      # Valid: "START_SENSITIVITY_HIGH", "START_SENSITIVITY_LOW", "DEFAULT" (unspecified)
      start_of_speech_sensitivity: "START_SENSITIVITY_HIGH"
      # Default sensitivity for speech end
      # Valid: "END_SENSITIVITY_HIGH", "END_SENSITIVITY_LOW", "DEFAULT" (unspecified)
      end_of_speech_sensitivity: "DEFAULT"
      # Longer silence duration to account for telephone latency
      silence_duration_ms: 1000

# Configuration for the document indexing pipeline
indexing_pipeline_config:
  # Optional global configurations for the pipeline itself, if IndexingPipeline supports them
  # global_pipeline_config:
  #   some_global_setting: value
  processors:
    - type: "TitleExtractor"
      # config: {} # No specific config for TitleExtractor in this example
    - type: "PDFTextExtractor"
      # config: {} # No specific config for PDFTextExtractor
    - type: "WebFetcher"
      config: {} # No specific config other than scraper injection for now
    - type: "LLMSummaryGenerator" # Specific config for WebFetcherProcessor
      config:
        # llm_client is injected by DocumentIndexer
        input_content_types:
          - "original_document_file"
          - "raw_body_text" # For emails, if used by this indexer
          - "extracted_markdown_content" # From PDFTextExtractor
          - "fetched_content_markdown" # From WebFetcherProcessor
        target_embedding_type: "llm_generated_summary"
        # max_content_length: 100000 # Example, if needed
    - type: "TextChunker"
      config:
        chunk_size: 1000
        chunk_overlap: 100
        embedding_type_prefix_map:
          # For content parts directly provided to DocumentIndexer
          raw_body_text: "content_chunk"
          raw_file_text: "content_chunk"
          raw_note_text: "content_chunk" # From NotesIndexer
          # For content generated by other processors
          extracted_markdown_content: "content_chunk" # From PDFTextExtractor
          fetched_content_markdown: "content_chunk" # From WebFetcherProcessor
    - type: "EmbeddingDispatch"
      config:
        embedding_types_to_dispatch:
          - "title"
          # - "summary" # If you have a manual summary type
          - "content_chunk"
          - "llm_generated_summary" # From LLMSummaryGeneratorProcessor
          # Raw content types - will be embedded if small enough, stored if too large
          - "raw_note_text"
          - "raw_body_text"
          - "raw_file_text"
          - "extracted_markdown_content" # Full PDF text
          - "fetched_content_markdown" # Full web page content
# Other future configurations can go here
# e.g., web_server_port: 8080

# --- Calendar Configuration ---
# Calendar configuration is shared across all profiles.
# Populated from environment variables (CALDAV_*, ICAL_URLS).
# Example calendar configuration:
# calendar_config:
#   caldav:
#     username: "user"
#     password: "password"
#     calendar_urls: ["url1"]
#   ical:
#     urls: ["url2"]
#   duplicate_detection:
#     enabled: true  # Enable duplicate event detection (default: true)
#     similarity_strategy: "fuzzy"  # Strategy: "fuzzy" (fast, zero dependencies) or "embedding" (requires local-embeddings extra)
#     similarity_threshold: 0.30  # Similarity threshold (0.0-1.0). Events above this are considered duplicates
#     time_window_hours: 2  # Time window in hours to search for duplicates around the event time

# --- Default Service Profile Configuration ---
# Settings for the primary, default assistant behavior.
default_profile_settings:
  processing_config:
    retry_config:
      primary:
        provider: "google"
        model: "gemini-3-pro-preview"
      fallback:
        provider: "openai"
        model: "gpt-5.2"
    home_assistant_context_template: "{# Macro to format a distance value (assumed to be in km by the distance() function) \n   into a user-friendly string with 'm' or 'km'. #}\n{%- macro get_formatted_distance_string(distance_value_km) -%}\n  {%- if distance_value_km is number and distance_value_km < 1 -%}\n    {{- (distance_value_km * 1000) | round(0) ~ \" m\" -}}\n  {%- elif distance_value_km is number -%}\n    {{- distance_value_km | round(1) ~ \" km\" -}}\n  {%- else -%}\n    {{- \"\" -}} {# Handle cases where distance might not be a number #}\n  {%- endif -%}\n{%- endmacro -%}\n\n{# Macro to find the closest zone to a person and its distance.\n   It populates the 'out_ns.result_dict' with a dictionary \n   containing {'zone_obj': <state_obj>, 'distance': <float>}, or sets it to none. #}\n{%- macro find_closest_zone_data(person_obj, out_ns) -%}\n  {%- set zones_with_distances = namespace(list=[]) -%}\n\n  {%- for zone_obj in states.zone | rejectattr('entity_id', 'equalto', 'zone.near_home') -%}\n    {%- set current_dist = distance(zone_obj.entity_id, person_obj.entity_id) -%}\n    {%- if current_dist is number -%}\n      {%- set zones_with_distances.list = zones_with_distances.list + [{'zone_obj': zone_obj, 'distance': current_dist}] -%}\n    {%- endif -%}\n  {%- endfor -%}\n\n  {%- if zones_with_distances.list -%}\n    {%- set out_ns.result_dict = (zones_with_distances.list | sort(attribute='distance') | first) -%}\n  {%- else -%}\n    {%- set out_ns.result_dict = none -%}\n  {%- endif -%}\n  {# This macro modifies 'out_ns' by side effect and does not render its primary data as output. #}\n{%- endmacro -%}\n\n{# Macro to determine and return the detailed portion of a 'not_home' status message.\n   For example: \" 1.2 km from Work\" or \" (location data unavailable)\". #}\n{%- macro get_not_home_status_details(person_obj) -%}\n  {%- if state_attr(person_obj.entity_id, 'latitude') is not none and state_attr(person_obj.entity_id, 'longitude') is not none -%}\n    {%- if states.zone | length > 0 -%}\n      {# Create a namespace to hold the output from the find_closest_zone_data macro #}\n      {%- set closest_zone_ns = namespace(result_dict=none) -%}\n      {# Call the macro; it will populate closest_zone_ns.result_dict by side effect. #}\n      {# Assigning to '_' signifies we're primarily interested in the side effect, not the macro's direct string output (which should be empty). #}\n      {%- set _ = find_closest_zone_data(person_obj, closest_zone_ns) -%} \n      \n      {%- set closest_zone_data = closest_zone_ns.result_dict -%} {# Retrieve the actual dictionary #}\n\n      {%- if closest_zone_data is not none and closest_zone_data.distance is number -%}\n        {{- \" \" ~ get_formatted_distance_string(closest_zone_data.distance) ~ \" from \" ~ closest_zone_data.zone_obj.name -}}\n      {%- elif closest_zone_data is not none -%} {# A zone was found, but distance was invalid for formatting #}\n        {{- \" (could not calculate distance to \" ~ closest_zone_data.zone_obj.name ~ \")\" -}}\n      {%- else -%}\n        {{- \" (unable to find a suitable closest zone)\" -}}\n      {%- endif -%}\n    {%- else -%}\n      {{- \" (no zones defined)\" -}}\n    {%- endif -%}\n  {%- else -%}\n    {{- \" (location data unavailable)\" -}}\n  {%- endif -%}\n{%- endmacro -%}\n\n{# Macro to format a timestamp in a friendly way #}\n{%- macro friendly_time(timestamp) -%}\n  {%- set dt = timestamp | as_datetime | as_local -%}\n  {%- set today = now().date() -%}\n  {%- set tomorrow = (now() + timedelta(days=1)).date() -%}\n  {%- set dt_date = dt.date() -%}\n  \n  {%- if dt_date == today -%}\n    {{- \"today at \" ~ (dt | as_timestamp | timestamp_custom('%-I:%M %p')) | lower -}}\n  {%- elif dt_date == tomorrow -%}\n    {{- \"tomorrow at \" ~ (dt | as_timestamp | timestamp_custom('%-I:%M %p')) | lower -}}\n  {%- elif (dt_date - today).days <= 6 and dt_date > today -%}\n    {{- (dt | as_timestamp | timestamp_custom('%A at %-I:%M %p')) | lower -}}\n  {%- else -%}\n    {{- dt | as_timestamp | timestamp_custom('%b %-d at %-I:%M %p') -}}\n  {%- endif -%}\n{%- endmacro -%}\n\n{%- for person in states.person -%}\n  {%- if not loop.first -%}{{ \"\\n\" }}{%- endif -%} {# Ensures each person's status starts on a new line. #}\n  \n  {{- person.name -}}\n  {%- if person.state == 'not_home' -%}\n    {{- \" is not home\" ~ get_not_home_status_details(person) -}}\n  {%- else -%}\n    {# If not 'not_home', the person.state is the friendly name of the zone they are in. #}\n    {{- \" is at \" ~ person.state -}}\n  {%- endif -%}\n  {%- set lat = state_attr(person.entity_id, 'latitude') -%}\n  {%- set lon = state_attr(person.entity_id, 'longitude') -%}\n  {%- if lat is not none and lon is not none -%}\n    {{- \" (\" ~ lat ~ \", \" ~ lon ~ \")\" -}}\n  {%- endif -%}\n{%- endfor -%}\n\n{# Process electricity price forecasts #}\n{%- set forecasts = state_attr(\"sensor.electricity_price_forecast\", \"forecasts\") or [] -%}\n{%- set significant_periods = [] -%}\n{%- set current = namespace(type=none, start=none, end=none, min=none, max=none) -%}\n\n{%- for forecast in forecasts | sort(attribute='start_time') -%}\n  {# Determine if this forecast is significant #}\n  {%- set price_type = none -%}\n  {%- if forecast.per_kwh < 0.15 -%}\n    {%- set price_type = \"very low\" -%} {%- elif forecast.per_kwh < 0.25 -%} {%- set price_type = \"low\" -%}\n  {%- elif forecast.descriptor in [\"high\", \"spike\"] -%}\n    {%- set price_type = \"very high\" -%}\n  {%- endif -%}\n  \n  {%- if price_type -%}\n    {# This is a significant price period #}\n    {%- if not current.type -%}\n      {# Start new period #}\n      {%- set current.type = price_type -%}\n      {%- set current.start = forecast.start_time -%}\n      {%- set current.end = forecast.end_time -%}\n      {%- set current.min = forecast.per_kwh -%}\n      {%- set current.max = forecast.per_kwh -%}\n    {%- elif current.type == price_type and current.end == forecast.start_time -%}\n      {# Extend current period (adjacent and same type) #}\n      {%- set current.end = forecast.end_time -%}\n      {%- set current.min = [current.min, forecast.per_kwh] | min -%}\n      {%- set current.max = [current.max, forecast.per_kwh] | max -%}\n    {%- else -%}\n      {# Save current period and start new one - using safe list concatenation #}\n      {%- set significant_periods = significant_periods + [{\n          \"type\": current.type,\n          \"start\": current.start,\n          \"end\": current.end,\n          \"min\": current.min,\n          \"max\": current.max\n        }] -%}\n      {%- set current.type = price_type -%}\n      {%- set current.start = forecast.start_time -%}\n      {%- set current.end = forecast.end_time -%}\n      {%- set current.min = forecast.per_kwh -%}\n      {%- set current.max = forecast.per_kwh -%}\n    {%- endif -%}\n  {%- elif current.type -%}\n    {# Save current period and reset - using safe list concatenation #}\n    {%- set significant_periods = significant_periods + [{\n        \"type\": current.type,\n        \"start\": current.start,\n        \"end\": current.end,\n        \"min\": current.min,\n        \"max\": current.max\n      }] -%}\n    {%- set current.type = none -%}\n    {%- set current.start = none -%}\n    {%- set current.end = none -%}\n    {%- set current.min = none -%}\n    {%- set current.max = none -%}\n  {%- endif -%}\n{%- endfor -%}\n\n{# Save any remaining period - using safe list concatenation #}\n{%- if current.type -%}\n  {%- set significant_periods = significant_periods + [{\n      \"type\": current.type,\n      \"start\": current.start,\n      \"end\": current.end,\n      \"min\": current.min,\n      \"max\": current.max\n    }] -%}\n{%- endif -%}\n{{ \"\\n\\n\" }}\n## Energy price\nCurrently: {{ states(\"sensor.general_price\") }} ({{ state_attr(\"sensor.general_price\", \"descriptor\") }})\n\n{%- if significant_periods -%}\n  {%- for period in significant_periods %}\nPrice is {{ period.type }} ({{ period.min | round(2) }} to {{ period.max | round(2) }} c/kWh) from {{ friendly_time(period.start) }} to {{ friendly_time(period.end) }}\n  {%- endfor -%}\n{%- else -%}\nNo significant energy price periods are forecast.\n{%- endif -%}\n\n"
    # `prompts` are loaded from prompts.yaml. This key is a placeholder for that structure.
    # prompts:
    #   system_prompt: "You are a helpful assistant. Current time is {current_time}."
    #   # ... other prompts ...

    # Default values, can be overridden by environment variables.
    timezone: "Australia/Sydney"
    max_history_messages: 10
    history_max_age_hours: 2
    # Web-specific history settings for better conversation context
    # web_max_history_messages: 100  # Uncomment to override default of 100
    # web_history_max_age_hours: 720  # Uncomment to override default of 720 (30 days)
    max_iterations: 10 # Maximum number of tool call iterations
    delegation_security_level: "confirm" # Default: "blocked", "confirm", or "unrestricted"
  tools_config:
    # `enable_local_tools` are not explicitly listed here for the default profile.
    # The application currently enables all available local tools.
    enable_local_tools:
      - "add_or_update_note"
      - "get_note"
      - "list_notes"
      - "delete_note"
      - "schedule_future_callback"
      - "schedule_recurring_task"
      - "schedule_reminder"
      - "schedule_action"
      - "schedule_recurring_action"
      - "list_pending_callbacks"
      - "modify_pending_callback"
      - "cancel_pending_callback"
      - "search_documents"
      - "get_full_document_content"
      - "get_attachment_info"
      - "get_message_history"
      - "get_user_documentation_content"
      - "ingest_document_from_url"
      - "execute_script"
      - "send_message_to_user"
      - "attach_to_response"
      - "add_calendar_event"
      - "search_calendar_events"
      - "modify_calendar_event"
      - "delete_calendar_event"
      - "delegate_to_service"
      - "query_recent_events" # Event system tool
      - "render_home_assistant_template" # Home Assistant template rendering
      - "get_camera_snapshot" # Get camera snapshots from Home Assistant
      - "download_state_history" # Download Home Assistant state history as JSON
      - "list_home_assistant_entities" # Search and list Home Assistant entities
      # Read-only automation tools (creation/editing requires automation_creation profile)
      - "list_automations" # List all automations
      - "get_automation" # Get automation details
      - "get_automation_stats" # Get automation execution statistics
      # Image generation tools
      - "generate_image" # Generate images from text using AI
      - "transform_image" # Transform existing images based on instructions
      # Video generation tools
      - "generate_video" # Generate videos from text using AI
      # Media download tools
      - "download_media" # Download video/audio from URLs using yt-dlp
      # Image manipulation tools
      - "highlight_image" # Highlight or annotate areas in images
    enable_mcp_server_ids: # Explicitly define MCP servers for the default profile
      - "time"
      - "brave"
      - "python"
      - "homeassistant"
      - "google-maps"
      # The "browser" mcp server is intentionally excluded here.
    # Tools requiring explicit user confirmation before execution for this default profile.
    # This list is overridden by the TOOLS_REQUIRING_CONFIRMATION environment variable if set.
    confirm_tools:
      - delete_calendar_event
      - modify_calendar_event
      # - tool_that_posts_online
    mcp_initialization_timeout_seconds: 60 # Default 1 minute
  # Default list of slash commands that trigger this profile.
  # Can be overridden by individual service_profiles.
  slash_commands: []
# --- Service Profiles ---
# Defines specific assistant profiles with potentially different configurations.
# Provider Configuration:
# - 'google': Direct Google GenAI SDK integration (requires GEMINI_API_KEY)
# - 'openai': Direct OpenAI SDK integration (requires OPENAI_API_KEY)
# - 'litellm': LiteLLM for models not available via direct integration (default)
service_profiles:
  - id: "default_assistant"
    description: "Main assistant using default settings, without browser tools. Suitable for general tasks, note-taking, calendar management, and information retrieval from stored documents."
    # This profile implicitly uses all settings from 'default_profile_settings'
    # as no specific 'processing_config' or 'tools_config' is defined here,
    # thus inheriting the tools_config that excludes the browser.
  - id: "reminder"
    description: "Assistant profile for handling system reminders. Its sole purpose is to formulate the reminder message to be sent to the user."
    processing_config:
      provider: "litellm"
      # Inherit default model
      prompts:
        system_prompt: "You are a reminder delivery system. Your goal is to write the text of the reminder message that will be sent to the user.\n\nInstructions:\n1. Read the reminder context provided.\n2. You may use read-only tools (calendar, notes, docs) to gather context if the reminder is vague or refers to stored information.\n3. Write a clear, friendly reminder message addressed to {user_name}.\n4. Your final response must be ONLY the message content. Do NOT describe your actions.\n5. If the reminder context contains a task, simply state the task.\n\nExample:\nContext: 'Pick up milk'\nResponse: 'Hi {user_name}, this is your reminder to pick up milk.'\n\nCurrent time: {current_time}"
      delegation_security_level: "blocked"
    tools_config:
      enable_local_tools:
        - "search_calendar_events"
        - "get_note"
        - "list_notes"
        - "search_documents"
        - "get_full_document_content"
        - "get_user_documentation_content"
      enable_mcp_server_ids: []
      confirm_tools: []
    slash_commands: []
  - id: "browser_profile"
    description: "Assistant profile with web browsing capabilities for complex browser interactions like filling forms or navigating JavaScript-heavy sites. For simple web scraping, consider using the ingest_document_from_url tool or direct MCP browser tools."
    processing_config:
      # Using simple format (no retry/fallback for browser tasks)
      provider: "google"
      llm_model: "gemini-2.5-computer-use-preview-10-2025"
      max_iterations: 50
      prompts: # MERGES with default_profile_settings.processing_config.prompts
        system_prompt: "You are an assistant with web browsing capabilities interacting with {user_name}. Every browser action you take (navigate, click, scroll, etc.) automatically captures a screenshot which you will see. By default, ALL screenshots from your actions are sent to the user. For multi-step workflows where you only want to show specific screenshots (e.g., just the final result), use attach_to_response to select which ones to send. After any tool calls you make, your final text response will be sent directly to {user_name}. Do NOT use the 'send_message_to_user' tool to respond to {user_name} - that tool is only for sending messages to OTHER users. Current time is {current_time}."
      delegation_security_level: "unrestricted" # This profile can be delegated to without forced confirmation
    tools_config: # REPLACES default_profile_settings.tools_config entirely for this profile
      enable_local_tools:
        - "click_at"
        - "type_text_at"
        - "scroll_at"
        - "open_web_browser"
        - "navigate"
        - "search"
        - "go_back"
        - "go_forward"
        - "key_combination"
        - "wait_5_seconds"
        - "hover_at"
        - "drag_and_drop"
        - "scroll_document"
        - "attach_to_response"
      enable_mcp_server_ids: []
      confirm_tools: [] # No tools to confirm
    slash_commands:
      - "/browse"
  - id: "research"
    description: "Assistant profile for deep research, utilizing the Google Gemini Deep Research agent. Ideal for comprehensive information gathering and analysis when web browsing or specific tool execution is not required."
    processing_config:
      # Updated to use Google Gemini Deep Research agent
      provider: "google"
      llm_model: "deep-research-pro-preview-12-2025"
      prompts:
        system_prompt: "You are a research assistant interacting with {user_name}. Please focus on providing comprehensive and accurate information. After any tool calls you make, your final text response will be sent directly to {user_name}. Do NOT use the 'send_message_to_user' tool to respond to {user_name} - that tool is only for sending messages to OTHER users."
      # Inherits default timezone, history settings.
      # No specific context providers are added by default for this profile.
      delegation_security_level: "unrestricted"
    tools_config: # REPLACES default_profile_settings.tools_config entirely for this profile
      enable_local_tools: [] # No local tools for this profile
      enable_mcp_server_ids: [] # No MCP tools for this profile
      confirm_tools: [] # No tools to confirm
    slash_commands:
      - "/research"
  - id: "data_visualization"
    description: "Assistant profile optimized for creating data visualizations and charts. Uses Vega/Vega-Lite to generate professional charts from user data."
    processing_config:
      max_iterations: 20
      delegation_security_level: "unrestricted"
      include_system_docs:
        - "data_visualization.md"
        - "vega_lite_reference.md"
    tools_config:
      enable_local_tools:
        - "create_vega_chart"
        - "jq_query"
        - "attach_to_response"
        - "get_attachment_info"
        - "search_documents"
        - "get_full_document_content"
        - "download_state_history"
        - "list_home_assistant_entities"
        - "render_home_assistant_template"
        - "execute_script"
      enable_mcp_server_ids:
        - "python"
        - "homeassistant"
        - "scrape"
    slash_commands:
      - "/visualize"
      - "/chart"
  - id: "event_handler"
    description: "Restricted assistant profile for automated script-event integration. Uses Gemini 2.5 Pro with read-only and non-destructive tools only."
    processing_config:
      # Using direct Google provider for consistency
      provider: "google"
      llm_model: "gemini-2.5-pro"
      # Auto-load system documentation into this profile's system prompt
      # include_system_docs:
      #   - "USER_GUIDE.md"
      #   - "scripting.md"
      prompts:
        system_prompt: "You are an automated event handler assistant. You are responding to events from scripts and automations. Provide clear, concise responses focused on the event context.\n\nCurrent time: {current_time}\n\n{aggregated_other_context}"
      max_history_messages: 1 # Minimal history for event processing
      history_max_age_hours: 0.5 # 30 minutes - events are typically immediate
      delegation_security_level: "blocked" # Cannot delegate to other services
    tools_config: # REPLACES default_profile_settings.tools_config entirely for this profile
      enable_local_tools:
        # Core data tools (read/write for notes, read-only for documents)
        - "add_or_update_note"
        - "list_notes"
        - "get_note"
        - "search_documents"
        # Communication (Telegram only to prevent email spam)
        - "send_message_to_user"
        # Calendar read-only
        - "search_calendar_events"
        # Event system query only (no listener creation/modification)
        - "query_recent_events"
      enable_mcp_server_ids:
        # Home Assistant read-only access
        - "homeassistant" # Configured to only allow get_entity_state and get_all_entities
      confirm_tools: [] # No confirmation needed for automated operations
    slash_commands: [] # Not accessible via slash commands
  - id: "automation_creation"
    description: "Specialized profile for creating and validating event-based and schedule-based automations. Includes comprehensive scripting documentation and validation tools."
    processing_config:
      provider: "google"
      llm_model: "gemini-2.5-pro"
      max_iterations: 25 # Allow more iterations for complex validation workflows
      delegation_security_level: "unrestricted" # Can delegate to other profiles if needed
      include_system_docs:
        - "scripting.md" # Include scripting documentation by default
      prompts:
        system_prompt: |
          You are a specialized automation creation assistant helping {user_name} create robust, well-tested automations.

          Current time: {current_time}

          {aggregated_other_context}

          ## Your Mission

          Your goal is to help users create reliable, validated automations (both event-based and schedule-based) that work correctly on the first try. You achieve this through systematic analysis, validation, and testing before creating the final automation.

          ## Automation Creation Procedure

          When a user asks you to create an automation, follow this systematic approach:

          ### 1. Understand the Request and Examine Existing Patterns

          - **Parse the user's intent**: What do they want to happen? When should it happen? What action should be taken?
          - **Search for similar automations**: Use `list_automations` to find existing automations that do similar things
            - Look for automations that have run successfully before (check `get_automation_stats`)
            - Identify patterns in how similar triggers are implemented
            - Note successful condition script patterns if they exist
          - **Ask clarifying questions** if the request is ambiguous

          ### 2. Identify Trigger Conditions

          Determine what should trigger the automation:

          **For Event-Based Automations:**
          - What event source? (home_assistant, indexing, webhook)
          - What specific entity or data should be watched?
          - What conditions indicate the automation should run?
          - Can conditions be expressed as simple equality checks (match_conditions)?
          - Or do they require complex logic (condition_script with Starlark)?

          **For Schedule-Based Automations:**
          - What time(s) should it run?
          - What recurrence pattern (RRULE format)?
          - Examples: "FREQ=DAILY;BYHOUR=7" for 7am daily, "FREQ=WEEKLY;BYDAY=MO,WE,FR;BYHOUR=9" for Mon/Wed/Fri at 9am

          ### 3. Determine Action Type

          Choose between two action types:

          **Use `action_type="script"` when:**
          - The action is deterministic and doesn't require judgment
          - You can write clear, testable logic for what to do
          - Fast execution is important (scripts run immediately)
          - Examples: logging data, sending notifications with known content, simple data collection

          **Use `action_type="wake_llm"` when:**
          - The action requires reasoning or context-aware judgment
          - The situation is complex and may need different responses
          - You need to analyze multiple factors before deciding what to do
          - Examples: "decide whether to send a reminder based on calendar", "summarize important emails and notify if urgent"

          ### 4. Validate All Assumptions with Tools

          **CRITICAL**: Never guess about entity IDs, event structures, or state values. Always validate:

          **Validate Event Structures:**
          ```
          # Check what events actually look like
          Use query_recent_events(source_id="home_assistant", hours=24, limit=10)
          # Examine the structure of events - what fields exist? What are the actual values?
          ```

          **Validate Entity IDs and States:**
          ```
          # For Home Assistant entities
          Use MCP homeassistant tools to:
          - Get current state: get_entity_state(entity_id="sensor.temperature")
          - List available entities: get_all_entities() then filter/search

          # Check historical state data
          Use download_state_history(entity_ids=["sensor.temperature"], hours=24)
          # Verify that assumed states/attributes actually exist in this format
          ```

          **Test Event Matching:**
          ```
          # Before creating an event automation, test if your conditions would actually match
          Use test_event_listener(
              source="home_assistant",
              match_conditions={"entity_id": "sensor.motion", "new_state.state": "on"},
              hours=24,
              limit=5
          )
          # This shows you if your conditions would have matched recent events
          ```

          ### 5. Generate and Test Candidate Scripts

          **For script-based actions:**

          - Write a candidate action script
          - Test it in isolation first using `execute_script` with test data
          - Verify it produces the expected results
          - Check for edge cases (missing data, null values, etc.)
          - For event scripts, simulate the event structure you validated in step 4

          **For condition scripts (complex event matching):**

          - Write a candidate condition script
          - Test it against actual event data from `query_recent_events`
          - Verify it returns boolean values correctly
          - Check it handles missing fields gracefully (use `.get()` with defaults)
          - Remember: Starlark has no float(), only int(). For decimal temperatures, truncate: `int(event.get('new_state', {}).get('state', '0').split('.')[0])`

          **Example Testing Pattern:**
          ```python
          # Test a temperature condition script
          test_script = '''
          temp = int(event.get("new_state", {}).get("state", "0").split('.')[0])
          old_temp = int(event.get("old_state", {}).get("state", "0").split('.')[0]) if event.get("old_state") else 0
          return temp > 25 and old_temp <= 25  # Crossing threshold
          '''

          # Execute with sample event data to verify it works
          result = execute_script(
              script_code=test_script,
              global_vars={"event": sample_event_from_query_recent_events}
          )
          ```

          ### 6. Test Actions Safely

          **Before creating the automation:**

          - For notification actions: Test the message format makes sense
          - For wake_llm actions: Verify the context will be clear to the LLM when it wakes
          - For data modification: Consider if a dry-run is possible
          - For scripts with side effects: Test with minimal impact first

          **Dry-run strategies:**
          - Add a `dry_run=True` flag to your script and only log what would happen
          - Test with a copy of data rather than production data
          - Send test notifications to yourself first

          ### 7. Validate wake_llm Context

          **For wake_llm automations:**

          - Put yourself in the LLM's position when it wakes up
          - Will the context be clear? Will it have enough information to act?
          - Is the context message specific and actionable?

          **Good wake_llm context examples:**
          - "Motion detected in garage at 2:30 AM - unusual activity"
          - "Temperature in living room reached 28°C, above comfortable range"
          - "New email from project manager with 'urgent' in subject"

          **Poor wake_llm context examples:**
          - "Event occurred" (too vague)
          - "State changed" (no actionable information)
          - "Check this" (LLM won't know what to check)

          ### 8. Create the Automation

          Once validated and tested:

          ```python
          # Create the automation with all validated parameters
          result = create_automation(
              name="Descriptive name of what this does",
              automation_type="event",  # or "schedule"
              trigger_config={
                  # For event type:
                  "event_source": "home_assistant",
                  "event_filter": {...},  # Validated filters
                  # Optional: "condition_script": "..." if complex matching needed

                  # For schedule type:
                  # "recurrence_rule": "FREQ=DAILY;BYHOUR=7"
              },
              action_type="script",  # or "wake_llm"
              action_config={
                  # For script type:
                  "script_code": "...",  # Tested script

                  # For wake_llm type:
                  "context": "Clear message explaining what happened and why LLM should care"
              },
              description="Optional longer description of purpose and behavior"
          )
          ```

          ### 9. Provide Results and Next Steps

          After creating the automation:

          - Show the automation ID from the result
          - Provide a direct link to view it in the UI: `{server_url}/automations/event/{automation_id}` or `{server_url}/automations/schedule/{automation_id}`
          - Explain what will trigger it and what it will do
          - Suggest how to verify it's working (check stats, watch for next trigger, etc.)
          - Offer to make adjustments if needed

          ## Important Guidelines

          **Always validate before creating:**
          - Use tools to verify entity IDs exist
          - Check event structures match your assumptions
          - Test scripts before embedding them in automations
          - Validate that historical data shows the patterns you expect

          **Prefer scripts over wake_llm when possible:**
          - Scripts are faster (no API calls)
          - Scripts are more deterministic
          - Scripts are cheaper (no LLM costs)
          - Only use wake_llm when genuine reasoning is needed

          **Write defensive scripts:**
          - Use `.get()` with defaults for optional fields
          - Check for None/empty values before processing
          - Remember Starlark limitations: no try/except, no while loops, no float()
          - Handle missing event fields gracefully

          **Make automations debuggable:**
          - Use descriptive names that explain what they do
          - Add meaningful descriptions
          - For scripts, include comments explaining logic
          - For wake_llm, provide context that will help future debugging

          **Consider rate limiting:**
          - Event automations have daily execution limits (default 5/day)
          - Design conditions to avoid excessive triggering
          - Use condition scripts to filter out noise

          ## Common Patterns

          **Zone entry detection:**
          ```starlark
          # Detect when someone enters home zone
          old_state = event.get('old_state', {}).get('state', '')
          new_state = event.get('new_state', {}).get('state', '')
          return old_state != 'home' and new_state == 'home'
          ```

          **Threshold crossing:**
          ```starlark
          # Alert when temperature crosses above 25°C
          temp = int(event.get("new_state", {}).get("state", "0").split('.')[0])
          old_temp = int(event.get("old_state", {}).get("state", "0").split('.')[0]) if event.get("old_state") else 0
          return temp > 25 and old_temp <= 25
          ```

          **Pattern matching:**
          ```starlark
          # Match any motion sensor
          entity_id = event.get('entity_id', '')
          new_state = event.get('new_state', {}).get('state', '')
          return entity_id.startswith('sensor.motion_') and new_state == 'on'
          ```

          ## Remember

          You have access to comprehensive scripting documentation (already loaded). Reference it when writing scripts. The scripting guide covers:
          - Starlark language features and limitations
          - Available tool APIs and how to call them
          - Time/date handling functions
          - Attachment creation and manipulation
          - Common patterns and examples

          Your success is measured by creating automations that work reliably without manual intervention. Take the time to validate, test, and verify before creating.
    tools_config:
      enable_local_tools:
        # Automation management tools
        - "create_automation"
        - "list_automations"
        - "get_automation"
        - "update_automation"
        - "enable_automation"
        - "disable_automation"
        - "delete_automation"
        - "get_automation_stats"
        # Event testing and validation tools
        - "query_recent_events"
        - "test_event_listener"
        # Script execution for testing
        - "execute_script"
        # Calendar and notes for context
        - "search_calendar_events"
        - "get_calendar_events"
        - "list_notes"
        - "get_note"
        - "search_notes"
        # Communication tools
        - "send_message_to_user"
        # Document access for understanding context
        - "search_documents"
        - "get_full_document_content"
        - "get_user_documentation_content"
        # Home Assistant state validation
        - "download_state_history"
        - "render_home_assistant_template"
        # Callback and action scheduling
        - "schedule_action"
        - "schedule_recurring_action"
        - "list_pending_callbacks"
      enable_mcp_server_ids:
        - "homeassistant" # For validating entity IDs and states
      confirm_tools: [] # No confirmation needed for automation creation workflows
    slash_commands:
      - "/automate"
  - id: "artist"
    description: "Creative assistant specialized in generating and manipulating images and videos using advanced AI models."
    processing_config:
      provider: "google"
      llm_model: "gemini-3-pro-preview"
      prompts:
        system_prompt: |
          You are an expert digital artist and video producer specialized in using advanced AI models (Gemini Nano Banana/Pro and Veo) to create stunning visuals.

          Your goal is to help users realize their creative vision by crafting highly detailed, effective prompts and using image/video generation tools with precision.

          ## Image Generation Best Practices (Gemini Nano Banana/Pro)

          When using `generate_image`, follow these prompting principles:
          1. **Be Highly Descriptive**: Start with the main subject, then describe the action/context, art style, lighting, color palette, and camera angle.
          2. **Specify Style**: Use keywords like "photorealistic", "oil painting", "cinematic", "3D render", "anime", etc.
          3. **Lighting & Atmosphere**: Describe the lighting (e.g., "soft morning light", "neon cyberpunk lighting", "dramatic shadows") and mood.
          4. **Composition**: Mention camera framing (e.g., "wide angle", "close-up", "macro", "drone shot").
          5. **Negative Constraints**: If the user wants to avoid something, describe it in the prompt or use negative prompting if available.

          Example Prompt Structure:
          "[Subject] [Action/Context], [Art Style], [Lighting], [Color Palette], [Camera Angle/Framing]"

          ## Video Generation Best Practices (Veo)

          When using `generate_video`, follow these guidelines:
          1. **Describe Motion**: Clearly state what is moving and how. Static prompts yield static videos. Use verbs like "pan", "zoom", "rotate", "track".
          2. **Visual Style**: Define the visual aesthetic (e.g., "cinematic", "grainy 8mm film", "crisp 4k").
          3. **Duration**: Default to 8 seconds for most generations unless specified otherwise.
          4. **Consistency**: When using `first_frame_image` or `images` (reference), ensure the prompt aligns with the provided visual content.

          ## Workflow
          1. **Understand**: Clarify the user's vision. What is the subject? What is the mood?
          2. **Refine**: If the user's request is vague ("make a cat"), expand it into a high-quality prompt ("A fluffy Persian cat sitting on a velvet cushion in a sunlit Victorian room, photorealistic, 4k").
          3. **Execute**: Use the appropriate tool (`generate_image`, `generate_video`, etc.).
          4. **Review**: Present the result. If using `generate_video`, note that it may take a few minutes.

          Current time: {current_time}
      delegation_security_level: "unrestricted"
    tools_config:
      enable_local_tools:
        # Image generation and manipulation
        - "generate_image"
        - "transform_image"
        - "highlight_image"
        - "generate_video"
        # Helper tools
        - "get_attachment_info"
        - "attach_to_response"
        - "execute_script"
        - "send_message_to_user"
        - "get_message_history"
        - "search_documents"
      enable_mcp_server_ids: []
      confirm_tools: []
    slash_commands:
      - "/artist"
      - "/image"
      - "/video"
  - id: "telephone"
    description: "Assistant profile for telephone interactions via Asterisk Live API."
    processing_config:
      prompts:
        system_prompt: |
          You are a helpful and friendly telephone assistant.
          Speak in a friendly, natural manner, but do not sound excessively saccharine or over-enthusiastic.
          Use a neutral accent that sits somewhere between British, American, and Australian.
          Most queries will be conversational and should not require complex processes.
          Keep your responses concise and suitable for voice interaction. Avoid using markdown formatting like bold or lists in your speech unless it translates well to spoken text.
          Current time: {current_time}
      delegation_security_level: "unrestricted"
    tools_config:
      enable_local_tools:
        - "add_or_update_note"
        - "get_note"
        - "list_notes"
        - "delete_note"
        - "schedule_future_callback"
        - "schedule_recurring_task"
        - "schedule_reminder"
        - "schedule_action"
        - "schedule_recurring_action"
        - "list_pending_callbacks"
        - "modify_pending_callback"
        - "cancel_pending_callback"
        - "search_documents"
        - "get_full_document_content"
        - "get_attachment_info"
        - "get_message_history"
        - "get_user_documentation_content"
        - "ingest_document_from_url"
        - "execute_script"
        - "send_message_to_user"
        - "add_calendar_event"
        - "search_calendar_events"
        - "modify_calendar_event"
        - "delete_calendar_event"
        - "delegate_to_service"
        - "query_recent_events"
        - "render_home_assistant_template"
        - "get_camera_snapshot"
        - "download_state_history"
        - "list_home_assistant_entities"
        - "list_automations"
        - "get_automation"
        - "get_automation_stats"
        - "generate_video"
      enable_mcp_server_ids:
        - "time"
        - "brave"
        - "python"
        - "homeassistant"
        - "google-maps"
      confirm_tools: []
    slash_commands: []
  - id: "camera_analyst"
    description: "Analyze camera footage to investigate events using binary search through thumbnails"
    processing_config:
      retry_config:
        primary:
          provider: "google"
          model: "gemini-3-flash-preview"
        fallback:
          provider: "openai"
          model: "gpt-5.2"
      max_iterations: 50
      delegation_security_level: "unrestricted"
      # Camera backend configuration for Reolink cameras
      # Can be configured here or via REOLINK_CAMERAS environment variable (JSON format)
      # Environment variable takes precedence if cameras_config is empty
      # Example environment variable:
      #   REOLINK_CAMERAS='{"coop": {"host": "192.168.1.100", "username": "admin", "password": "secret", "name": "Chicken Coop"}}'
      camera_config:
        backend: "reolink"
        # cameras_config: {}  # Uncomment and populate, or use REOLINK_CAMERAS env var
    tools_config:
      enable_local_tools:
        - "list_cameras"
        - "search_camera_events"
        - "get_camera_frame"
        - "get_camera_frames_batch"
        - "get_camera_recordings"
        - "attach_to_response"
        - "get_attachment_info"
      enable_mcp_server_ids: []
    slash_commands:
      - "/camera"
      - "/investigate"
  - id: "engineer"
    description: "Assistant profile for debugging and diagnosing application issues. Has read-only access to source code and database, and can create GitHub issues."
    processing_config:
      provider: "google"
      llm_model: "gemini-2.5-pro"
      prompts:
        system_prompt: |
          You are an engineer assistant specializing in diagnosing and debugging issues with this application.
          Your mission is to help users by investigating technical problems and reporting them clearly.

          ## Your Capabilities and Limitations

          You are operating under a restricted, read-only security profile to ensure user privacy and system integrity.

          - **You HAVE**: Read-only access to application source code, configuration, logs, and database.
          - **You DO NOT HAVE**: The ability to change any system state, modify files, or communicate externally.

          **The ONLY exception** is your ability to create a GitHub issue to report a bug.

          **CRITICAL**: You MUST NOT include any personally identifiable information (PII) or private user data in the GitHub issue title or body. Your report should focus on technical details, logs, and code references.

          ## Your Workflow

          When a user reports a problem, follow this systematic debugging process:

          1.  **Understand and Clarify**: Fully understand the reported issue. Ask clarifying questions if needed.
          2.  **Formulate a Hypothesis**: Based on the report, form a hypothesis about the root cause.
          3.  **Investigate**: Use your read-only tools (`list_source_files`, `read_file_chunk`, `search_in_file`, `database_readonly_query`, etc.) to gather data and test your hypothesis. Examine relevant code, check configurations, and look for errors.
          4.  **Synthesize Findings**: Consolidate your findings into a clear explanation of the problem.
          5.  **Report the Issue**: Use the `create_github_issue` tool to file a bug report. The report should include:
              - A clear, concise title.
              - A summary of the problem.
              - Steps to reproduce (if known).
              - Relevant logs or error messages (anonymized).
              - Pointers to the relevant source code files and line numbers.

          Your goal is to provide a high-quality bug report that will enable a human developer to quickly understand and fix the issue.
      delegation_security_level: "blocked"
    tools_config:
      enable_local_tools:
        - "list_source_files"
        - "read_file_chunk"
        - "search_in_file"
        - "create_github_issue"
        - "database_readonly_query"
        - "list_notes"
        - "get_note"
        - "search_documents"
        - "get_full_document_content"
        - "list_pending_callbacks"
        - "query_recent_events"
        - "list_automations"
        - "get_automation"
        - "get_automation_stats"
        - "get_user_documentation_content"
      enable_mcp_server_ids: []
      confirm_tools:
        - "create_github_issue"
    slash_commands:
      - "/engineer"
# Other global settings like llm_parameters, indexing_pipeline_config remain at top level.
# Attachment handling configuration
attachment_config:
  # Maximum file size for general attachments (in bytes)
  # Default: 100MB to handle large documents/images
  max_file_size: 104857600 # 100 * 1024 * 1024

  # Maximum file size for multimodal LLM processing (in bytes)
  # This is typically lower due to LLM provider limits
  # Default: 20MB (most LLM providers support up to 20MB images)
  max_multimodal_size: 20971520 # 20 * 1024 * 1024

  # Storage path for chat attachments
  # Can be overridden by chat_attachment_storage_path for backward compatibility
  storage_path: "/tmp/chat_attachments"

  # Allowed MIME types for attachments
  allowed_mime_types:
    - "image/jpeg"
    - "image/png"
    - "image/gif"
    - "image/webp"
    - "image/bmp"
    - "image/tiff"
    - "application/pdf"
    - "text/plain"
    - "application/json"
    - "text/csv"
    - "video/mp4"
    - "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    - "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    - "application/vnd.openxmlformats-officedocument.presentationml.presentation"

# AI Worker Sandbox configuration
# Enables spawning isolated AI coding agents for complex computing tasks
ai_worker_config:
  enabled: false
  backend_type: "kubernetes" # "kubernetes", "docker", "mock"
  workspace_mount_path: "/workspace"
  default_timeout_minutes: 30
  max_timeout_minutes: 120
  max_concurrent_workers: 3
  task_retention_hours: 48
  resources:
    memory_request: "512Mi"
    memory_limit: "2Gi"
    cpu_request: "500m"
    cpu_limit: "2000m"
  kubernetes:
    namespace: "ml-bot"
    ai_coder_image: "ghcr.io/werdnum/ai-coding-base:latest"
    service_account: "ai-worker"
    runtime_class: "gvisor"
    job_ttl_seconds: 3600
  docker:
    image: "ghcr.io/werdnum/ai-coding-base:latest"
    network: "bridge"

# Secrets (telegram_token, api_keys, database_url) are primarily from env.
# Model, embedding_model, server_url, storage_paths are also top-level or env.

