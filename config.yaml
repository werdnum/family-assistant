# Configuration for Family Assistant

# LLM provider-specific parameters
# Keys can be full model names or prefixes (ending with '-')
# Parameters defined here will be passed as keyword arguments to litellm.completion
# for matching models.
llm_parameters:
  # Example for Gemini models via OpenRouter
  "openrouter/google/gemini-":
    reasoning: # Nest OpenRouter-specific reasoning params here
      effort: medium
    # Other top-level parameters for this model could still go here
    # temperature: 0.8
  # Example for another model type
  # "anthropic/claude-3-":
  #   temperature: 0.75
  #   top_k: 50

# Configuration for the document indexing pipeline
indexing_pipeline_config:
  # Optional global configurations for the pipeline itself, if IndexingPipeline supports them
  # global_pipeline_config:
  #   some_global_setting: value

  processors:
    - type: "TitleExtractor"
      # config: {} # No specific config for TitleExtractor in this example
    - type: "PDFTextExtractor"
      # config: {} # No specific config for PDFTextExtractor
    - type: "WebFetcher"
      config: # Specific config for WebFetcherProcessor
        # Scraper instance is injected by DocumentIndexer, not configured here
        # Default user agent or other WebFetcherProcessor specific settings could go here
        # e.g. max_content_length: 1000000
        {} # No specific config other than scraper injection for now
    - type: "LLMSummaryGenerator"
      config:
        # llm_client is injected by DocumentIndexer
        input_content_types:
          - "original_document_file"
          - "raw_body_text" # For emails, if used by this indexer
          - "extracted_markdown_content" # From PDFTextExtractor
          - "fetched_content_markdown" # From WebFetcherProcessor
        target_embedding_type: "llm_generated_summary"
        # max_content_length: 100000 # Example, if needed
    - type: "TextChunker"
      config:
        chunk_size: 1000
        chunk_overlap: 100
        embedding_type_prefix_map:
          # For content parts directly provided to DocumentIndexer
          raw_body_text: "content_chunk"
          raw_file_text: "content_chunk"
          # For content generated by other processors
          extracted_markdown_content: "content_chunk" # From PDFTextExtractor
          fetched_content_markdown: "content_chunk" # From WebFetcherProcessor
    - type: "EmbeddingDispatch"
      config:
        embedding_types_to_dispatch:
          - "title"
          # - "summary" # If you have a manual summary type
          - "content_chunk"
          - "llm_generated_summary" # From LLMSummaryGeneratorProcessor

# Other future configurations can go here
# e.g., web_server_port: 8080

# --- Default Service Profile Configuration ---
# Settings for the primary, default assistant behavior.
default_profile_settings:
  processing_config:
    # `prompts` are loaded from prompts.yaml. This key is a placeholder for that structure.
    # prompts:
    #   system_prompt: "You are a helpful assistant. Current time is {current_time}."
    #   # ... other prompts ...

    # `calendar_config` is primarily built from environment variables (CALDAV_*, ICAL_URLS).
    # This key is a placeholder for that structure.
    # calendar_config:
    #   caldav:
    #     username: "user"
    #     password: "password"
    #     calendar_urls: ["url1"]
    #   ical:
    #     urls: ["url2"]

    # Default values, can be overridden by environment variables.
    timezone: "UTC"
    max_history_messages: 5
    history_max_age_hours: 24
    delegation_security_level: "confirm" # Default: "blocked", "confirm", or "unrestricted"

  tools_config:
    # `enable_local_tools` are not explicitly listed here for the default profile.
    # The application currently enables all available local tools.
    enable_local_tools:
      - "add_or_update_note"
      - "schedule_future_callback"
      - "schedule_recurring_task"
      - "search_documents"
      - "get_full_document_content"
      - "get_message_history"
      - "get_user_documentation_content"
      - "ingest_document_from_url"
      - "send_message_to_user"
      - "add_calendar_event"
      - "search_calendar_events"
      - "modify_calendar_event"
      - "delete_calendar_event"
      - "delegate_to_service" # Added the new tool
    enable_mcp_server_ids: # Explicitly define MCP servers for the default profile
      - "time"
      - "brave"
      - "python"
      - "homeassistant"
      - "google-maps"
      # The "browser" mcp server is intentionally excluded here.

    # Tools requiring explicit user confirmation before execution for this default profile.
    # This list is overridden by the TOOLS_REQUIRING_CONFIRMATION environment variable if set.
    confirm_tools:
      - delete_calendar_event
      - modify_calendar_event
      # - tool_that_posts_online
    mcp_initialization_timeout_seconds: 60 # Default 1 minute
  
  # Default list of slash commands that trigger this profile.
  # Can be overridden by individual service_profiles.
  slash_commands: []

# --- Service Profiles ---
# Defines specific assistant profiles with potentially different configurations.
service_profiles:
  - id: "default_assistant"
    description: "Main assistant using default settings, without browser tools."
    # This profile implicitly uses all settings from 'default_profile_settings'
    # as no specific 'processing_config' or 'tools_config' is defined here,
    # thus inheriting the tools_config that excludes the browser.

  - id: "browser_profile"
    description: "Assistant profile with web browsing capabilities."
    processing_config:
      # Optionally, use a different LLM model optimized for browsing or summarization
      # llm_model: "claude-3-sonnet-20240229"
      prompts: # MERGES with default_profile_settings.processing_config.prompts
        system_prompt: "You are an assistant with web browsing capabilities. Current time is {current_time}."
      delegation_security_level: "unrestricted" # This profile can be delegated to without forced confirmation
    tools_config: # REPLACES default_profile_settings.tools_config entirely for this profile
      # enable_local_tools: [] # Example: if you want NO local tools for this profile
      enable_mcp_server_ids:
        - "browser" # Only enable the browser MCP server
      confirm_tools: # Define tools requiring confirmation specifically for this profile
        - "browse_url" # Assuming 'browse_url' is a tool from the browser MCP server
    slash_commands:
      - "/browse"

  - id: "kubernetes_debug"
    description: "Assistant profile for Kubernetes debugging tasks."
    processing_config:
      prompts:
        system_prompt: "You are a specialized Kubernetes debugging assistant. Current time is {current_time}. Focus on diagnosing and resolving Kubernetes issues. You have access to tools for interacting with Kubernetes clusters, searching the web, and executing Python code."
      # This profile will inherit default timezone, history settings etc.
      # It will not load calendar, notes, or known_users context providers by default
      # due to the focused system prompt and lack of explicit inclusion.
    tools_config: # REPLACES default_profile_settings.tools_config entirely for this profile
      enable_local_tools:
        - "get_message_history"
      enable_mcp_server_ids:
        - "kubernetes"
        - "brave"
        - "python"
      confirm_tools:
        - "kubectl_create"
        - "kubectl_apply"
        - "kubectl_delete"
        - "kubectl_scale"
        - "kubectl_patch"
        - "kubectl_rollout"
        - "kubectl_generic" # Generic command can be destructive
        - "install_helm_chart"
        - "upgrade_helm_chart"
        - "uninstall_helm_chart"
        - "port_forward"
        - "stop_port_forward"
        - "cleanup" # Assumed tool name for cleanup operations
    slash_commands:
      - "/k8s"

# Other global settings like llm_parameters, indexing_pipeline_config remain at top level.
# Secrets (telegram_token, api_keys, database_url) are primarily from env.
# Model, embedding_model, server_url, storage_paths are also top-level or env.
