# Configuration for Family Assistant

# LLM provider-specific parameters
# Keys can be full model names or prefixes (ending with '-')
# Parameters defined here will be passed as keyword arguments to litellm.completion
# for matching models.
llm_parameters:
  # Example for Gemini models via OpenRouter
  "openrouter/google/gemini-":
    reasoning: # Nest OpenRouter-specific reasoning params here
      effort: medium
    # Other top-level parameters for this model could still go here
    # temperature: 0.8
  # Example for another model type
  # "anthropic/claude-3-":
  #   temperature: 0.75
  #   top_k: 50

# Configuration for the document indexing pipeline
indexing_pipeline_config:
  # Optional global configurations for the pipeline itself, if IndexingPipeline supports them
  # global_pipeline_config:
  #   some_global_setting: value

  processors:
    - type: "TitleExtractor"
      # config: {} # No specific config for TitleExtractor in this example
    - type: "PDFTextExtractor"
      # config: {} # No specific config for PDFTextExtractor
    - type: "WebFetcher"
      config: # Specific config for WebFetcherProcessor
        # Scraper instance is injected by DocumentIndexer, not configured here
        # Default user agent or other WebFetcherProcessor specific settings could go here
        # e.g. max_content_length: 1000000
        {} # No specific config other than scraper injection for now
    - type: "LLMSummaryGenerator"
      config:
        # llm_client is injected by DocumentIndexer
        input_content_types:
          - "original_document_file"
          - "raw_body_text" # For emails, if used by this indexer
          - "extracted_markdown_content" # From PDFTextExtractor
          - "fetched_content_markdown" # From WebFetcherProcessor
        target_embedding_type: "llm_generated_summary"
        # max_content_length: 100000 # Example, if needed
    - type: "TextChunker"
      config:
        chunk_size: 1000
        chunk_overlap: 100
        embedding_type_prefix_map:
          # For content parts directly provided to DocumentIndexer
          raw_body_text: "content_chunk"
          raw_file_text: "content_chunk"
          # For content generated by other processors
          extracted_markdown_content: "content_chunk" # From PDFTextExtractor
          fetched_content_markdown: "content_chunk" # From WebFetcherProcessor
    - type: "EmbeddingDispatch"
      config:
        embedding_types_to_dispatch:
          - "title"
          # - "summary" # If you have a manual summary type
          - "content_chunk"
          - "llm_generated_summary" # From LLMSummaryGeneratorProcessor

# Other future configurations can go here
# e.g., web_server_port: 8080

# --- Default Service Profile Configuration ---
# Settings for the primary, default assistant behavior.
default_profile_settings:
  processing_config:
    # `prompts` are loaded from prompts.yaml. This key is a placeholder for that structure.
    # prompts:
    #   system_prompt: "You are a helpful assistant. Current time is {current_time}."
    #   # ... other prompts ...

    # `calendar_config` is primarily built from environment variables (CALDAV_*, ICAL_URLS).
    # This key is a placeholder for that structure.
    # calendar_config:
    #   caldav:
    #     username: "user"
    #     password: "password"
    #     calendar_urls: ["url1"]
    #   ical:
    #     urls: ["url2"]

    # Default values, can be overridden by environment variables.
    timezone: "UTC"
    max_history_messages: 5
    history_max_age_hours: 24

  tools_config:
    # `enable_local_tools` and `enable_mcp_server_ids` are not explicitly listed here for the default profile.
    # The application currently enables all available local tools and all tools from configured MCP servers
    # for the default processing service. This structure allows for future explicit definition.
    # enable_local_tools: ["tool1", "tool2"]
    # enable_mcp_server_ids: ["mcp_server_id_1"]

    # Tools requiring explicit user confirmation before execution for this default profile.
    # This list is overridden by the TOOLS_REQUIRING_CONFIRMATION environment variable if set.
    confirm_tools:
      - delete_calendar_event
      - modify_calendar_event
      # - tool_that_posts_online
  
  # Default list of slash commands that trigger this profile.
  # Can be overridden by individual service_profiles.
  slash_commands: []

# Other global settings like llm_parameters, indexing_pipeline_config remain at top level.
# Secrets (telegram_token, api_keys, database_url) are primarily from env.
# Model, embedding_model, server_url, storage_paths are also top-level or env.
